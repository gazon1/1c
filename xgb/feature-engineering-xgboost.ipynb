{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "55f0ed2ac468f0eb59c24726ca0c9bdc8884c345"
   },
   "source": [
    "This notebook is simpified version of the final project in the [How to Win a Data Science Competition: Learn from Top Kagglers](https://www.coursera.org/learn/competitive-data-science) course. Simplified means without ensembling.\n",
    "\n",
    "#### Pipline\n",
    "* load data\n",
    "* heal data and remove outliers\n",
    "* work with shops/items/cats objects and features\n",
    "* create matrix as product of item/shop pairs within each month in the train set\n",
    "* get monthly sales for each item/shop pair in the train set and merge it to the matrix\n",
    "* clip item_cnt_month by (0,20)\n",
    "* append test to the matrix, fill 34 month nans with zeros\n",
    "* merge shops/items/cats to the matrix\n",
    "* add target lag features\n",
    "* add mean encoded features\n",
    "* add price trend features\n",
    "* add month\n",
    "* add days\n",
    "* add months since last sale/months since first sale features\n",
    "* cut first year and drop columns which can not be calculated for the test set\n",
    "* select best features\n",
    "* set validation strategy 34 test, 33 validation, less than 33 train\n",
    "* fit the model, predict and clip targets for the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f03379ee467570732ebb2b3d20062fea0584d57d"
   },
   "source": [
    "# Part 1, perfect features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sys.version_info(major=3, minor=6, micro=9, releaselevel='final', serial=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "from itertools import product\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import plot_importance\n",
    "\n",
    "def plot_features(booster, figsize):    \n",
    "    fig, ax = plt.subplots(1,1,figsize=figsize)\n",
    "    return plot_importance(booster=booster, ax=ax)\n",
    "\n",
    "import time\n",
    "import sys\n",
    "import gc\n",
    "import pickle\n",
    "sys.version_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_categories.csv\t      sales_train.csv\t     test.csv\r\n",
      "items.csv\t\t      sample_submission.csv\r\n",
      "knn_feats_minkowski_test.npy  shops.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "f0a1c729d4fb3d6609f9dfb163ebe92fa9dc654c"
   },
   "outputs": [],
   "source": [
    "items = pd.read_csv('data/items.csv')\n",
    "shops = pd.read_csv('data/shops.csv')\n",
    "cats = pd.read_csv('data/item_categories.csv')\n",
    "train = pd.read_csv('data/sales_train.csv')\n",
    "# set index to ID to avoid droping it later\n",
    "test  = pd.read_csv('data/test.csv').set_index('ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ed7a190645750a818e29a6291ba2553a91764c7c"
   },
   "source": [
    "## Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "425d8f2dc08378977b393bf80c5fdcf0fba2c992"
   },
   "source": [
    "There are items with strange prices and sales. After detailed exploration I decided to remove items with price > 100000 and sales > 1001 (1000 is ok)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "5a864412fafc3129a3e9bd5bb1f18a7cf0c62935"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f9d7525a1d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.xlim(-100, 3000)\n",
    "sns.boxplot(x=train.item_cnt_day)\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.xlim(train.item_price.min(), train.item_price.max()*1.1)\n",
    "sns.boxplot(x=train.item_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "7e621535d112603c60aeb2c2f83dbbf96d36b732"
   },
   "outputs": [],
   "source": [
    "train = train[train.item_price<100000]\n",
    "train = train[train.item_cnt_day<1001]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d2f99368478e3063b1c379537944e954d7186928"
   },
   "source": [
    "There is one item with price below zero. Fill it with median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "0fc6b90b22fe232f4240ac8f965cc52b3db5526a"
   },
   "outputs": [],
   "source": [
    "median = train[(train.shop_id==32)&(train.item_id==2973)&(train.date_block_num==4)&(train.item_price>0)].item_price.median()\n",
    "train.loc[train.item_price<0, 'item_price'] = median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7da194c285d696b5c6978148bf0143b9b2a7b0c5"
   },
   "source": [
    "Several shops are duplicates of each other (according to its name). Fix train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "00fe91e9c482ea413abd774ff903fe3d152785dd"
   },
   "outputs": [],
   "source": [
    "# Якутск Орджоникидзе, 56\n",
    "train.loc[train.shop_id == 0, 'shop_id'] = 57\n",
    "test.loc[test.shop_id == 0, 'shop_id'] = 57\n",
    "# Якутск ТЦ \"Центральный\"\n",
    "train.loc[train.shop_id == 1, 'shop_id'] = 58\n",
    "test.loc[test.shop_id == 1, 'shop_id'] = 58\n",
    "# Жуковский ул. Чкалова 39м²\n",
    "train.loc[train.shop_id == 10, 'shop_id'] = 11\n",
    "test.loc[test.shop_id == 10, 'shop_id'] = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a30f0521464e1fa20444e66d24bbdcb76b93f6de"
   },
   "source": [
    "## Shops/Cats/Items preprocessing\n",
    "Observations:\n",
    "* Each shop_name starts with the city name.\n",
    "* Each category contains type and subtype in its name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shop_name</th>\n",
       "      <th>shop_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>!Якутск Орджоникидзе, 56 фран</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>!Якутск ТЦ \"Центральный\" фран</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       shop_name  shop_id\n",
       "0  !Якутск Орджоникидзе, 56 фран        0\n",
       "1  !Якутск ТЦ \"Центральный\" фран        1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shops.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_category_name</th>\n",
       "      <th>item_category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PC - Гарнитуры/Наушники</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Аксессуары - PS2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        item_category_name  item_category_id\n",
       "0  PC - Гарнитуры/Наушники                 0\n",
       "1         Аксессуары - PS2                 1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_name</th>\n",
       "      <th>item_id</th>\n",
       "      <th>item_category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>! ВО ВЛАСТИ НАВАЖДЕНИЯ (ПЛАСТ.)         D</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>!ABBYY FineReader 12 Professional Edition Full...</td>\n",
       "      <td>1</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           item_name  item_id  \\\n",
       "0          ! ВО ВЛАСТИ НАВАЖДЕНИЯ (ПЛАСТ.)         D        0   \n",
       "1  !ABBYY FineReader 12 Professional Edition Full...        1   \n",
       "\n",
       "   item_category_id  \n",
       "0                40  \n",
       "1                76  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "12fae4c8d0c8f3e817307d1e0ffc6831e9a8d696"
   },
   "outputs": [],
   "source": [
    "shops.loc[shops.shop_name == 'Сергиев Посад ТЦ \"7Я\"', 'shop_name'] = 'СергиевПосад ТЦ \"7Я\"'\n",
    "shops['city'] = shops['shop_name'].str.split(' ').map(lambda x: x[0])\n",
    "shops.loc[shops.city == '!Якутск', 'city'] = 'Якутск'\n",
    "shops['city_code'] = LabelEncoder().fit_transform(shops['city'])\n",
    "shops = shops[['shop_id','city_code']]\n",
    "\n",
    "cats['split'] = cats['item_category_name'].str.split('-')\n",
    "cats['type'] = cats['split'].map(lambda x: x[0].strip())\n",
    "cats['type_code'] = LabelEncoder().fit_transform(cats['type'])\n",
    "# if subtype is nan then type\n",
    "cats['subtype'] = cats['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\n",
    "cats['subtype_code'] = LabelEncoder().fit_transform(cats['subtype'])\n",
    "cats = cats[['item_category_id','type_code', 'subtype_code']]\n",
    "\n",
    "items.drop(['item_name'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "62c5f83fa222595da99294f465ab28e80ce415e9"
   },
   "source": [
    "## Monthly sales\n",
    "Test set is a product of some shops and some items within 34 month. There are 5100 items * 42 shops = 214200 pairs. 363 items are new compared to the train. Hence, for the most of the items in the test set target value should be zero. \n",
    "In the other hand train set contains only pairs which were sold or returned in the past. Tha main idea is to calculate monthly sales and <b>extend it with zero sales</b> for each unique pair within the month. This way train data will be similar to test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "fb69350aef2c28cdb619e2532de1e24ab3c43899"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(363, 5100, 214200)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(set(test.item_id) - set(test.item_id).intersection(set(train.item_id)))), len(list(set(test.item_id))), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "7626c7455ea71b65894c6c866519df15080fa2ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.66 s, sys: 554 ms, total: 7.21 s\n",
      "Wall time: 7.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "matrix = []\n",
    "cols = ['date_block_num','shop_id','item_id']\n",
    "for i in range(34):\n",
    "    sales = train[train.date_block_num==i]\n",
    "    matrix.append(np.array(list(product([i], sales.shop_id.unique(), sales.item_id.unique())), dtype='int16'))\n",
    "    \n",
    "matrix = pd.DataFrame(np.vstack(matrix), columns=cols)\n",
    "matrix['date_block_num'] = matrix['date_block_num'].astype(np.int8)\n",
    "matrix['shop_id'] = matrix['shop_id'].astype(np.int8)\n",
    "matrix['item_id'] = matrix['item_id'].astype(np.int16)\n",
    "matrix.sort_values(cols,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "867e91a7570dd78b4834f4f1a166e58f80b63f93"
   },
   "source": [
    "Aggregate train set by shop/item pairs to calculate target aggreagates, then <b>clip(0,20)</b> target value. This way train target will be similar to the test predictions.\n",
    "\n",
    "<i>I use floats instead of ints for item_cnt_month to avoid downcasting it after concatination with the test set later. If it would be int16, after concatination with NaN values it becomes int64, but foat16 becomes float16 even with NaNs.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "9fef5477060be7d2e6c85dcb79d8e18e6253f7dd"
   },
   "outputs": [],
   "source": [
    "train['revenue'] = train['item_price'] *  train['item_cnt_day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "7dd27181918fc7df89676e24d72130d183929d2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.41 s, sys: 480 ms, total: 2.89 s\n",
      "Wall time: 2.89 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "group = train.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day': ['sum']})\n",
    "group.columns = ['item_cnt_month']\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "matrix = pd.merge(matrix, group, on=cols, how='left')\n",
    "matrix['item_cnt_month'] = (matrix['item_cnt_month']\n",
    "                                .fillna(0))\n",
    "#                                 .clip(0,20) # NB clip target here\n",
    "#                                 .astype(np.float16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f9d74b545f8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "# plt.xlim(train.item_price.min(), train.item_price.max()*1.1)\n",
    "sns.boxplot(x=matrix.item_cnt_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "315bc6107a93f3926a64fd09ea9244e9281ee41f"
   },
   "source": [
    "## Test set\n",
    "To use time tricks append test pairs to the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "29d02bdb4fa768577607bf735b918ca81da85d41"
   },
   "outputs": [],
   "source": [
    "test['date_block_num'] = 34\n",
    "test['date_block_num'] = test['date_block_num'].astype(np.int8)\n",
    "test['shop_id'] = test['shop_id'].astype(np.int8)\n",
    "test['item_id'] = test['item_id'].astype(np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "177fbbab94c8057d67d61357d29581248468a74d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.044194936752319336"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = time.time()\n",
    "matrix = pd.concat([matrix, test], ignore_index=True, sort=False, keys=cols)\n",
    "matrix.fillna(0, inplace=True) # 34 month\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "233e394a6cebf36ef002dc76fef8d430026a52b3"
   },
   "source": [
    "## Shops/Items/Cats features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "7dfd5df3e2bcaee4c312f3979736f52c40f2560f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5615787506103516"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = time.time()\n",
    "matrix = pd.merge(matrix, shops, on=['shop_id'], how='left')\n",
    "matrix = pd.merge(matrix, items, on=['item_id'], how='left')\n",
    "matrix = pd.merge(matrix, cats, on=['item_category_id'], how='left')\n",
    "matrix['city_code'] = matrix['city_code'].astype(np.int8)\n",
    "matrix['item_category_id'] = matrix['item_category_id'].astype(np.int8)\n",
    "matrix['type_code'] = matrix['type_code'].astype(np.int8)\n",
    "matrix['subtype_code'] = matrix['subtype_code'].astype(np.int8)\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8358b291fdc8e0e7d1b5700974803b3f104715f7"
   },
   "source": [
    "## Traget lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "9cd7bcc7643ce4545475e8e6f80d09a979aac42d"
   },
   "outputs": [],
   "source": [
    "def lag_feature(df, lags, col):\n",
    "    tmp = df[['date_block_num','shop_id','item_id',col]]\n",
    "    for i in lags:\n",
    "        shifted = tmp.copy()\n",
    "        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n",
    "        shifted['date_block_num'] += i\n",
    "        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "78bf7ece93ebc4629ad0e48cd6a9927788d8706d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.602515697479248"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = time.time()\n",
    "matrix = lag_feature(matrix, [1,2,3,6,12], 'item_cnt_month')\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>shop_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>item_cnt_month</th>\n",
       "      <th>city_code</th>\n",
       "      <th>item_category_id</th>\n",
       "      <th>type_code</th>\n",
       "      <th>subtype_code</th>\n",
       "      <th>item_cnt_month_lag_1</th>\n",
       "      <th>item_cnt_month_lag_2</th>\n",
       "      <th>item_cnt_month_lag_3</th>\n",
       "      <th>item_cnt_month_lag_6</th>\n",
       "      <th>item_cnt_month_lag_12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11127999</th>\n",
       "      <td>34</td>\n",
       "      <td>45</td>\n",
       "      <td>18454</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>55</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11128000</th>\n",
       "      <td>34</td>\n",
       "      <td>45</td>\n",
       "      <td>16188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>14</td>\n",
       "      <td>42</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11128001</th>\n",
       "      <td>34</td>\n",
       "      <td>45</td>\n",
       "      <td>15757</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>55</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11128002</th>\n",
       "      <td>34</td>\n",
       "      <td>45</td>\n",
       "      <td>19648</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>40</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11128003</th>\n",
       "      <td>34</td>\n",
       "      <td>45</td>\n",
       "      <td>969</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "      <td>37</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11128004 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          date_block_num  shop_id  item_id  item_cnt_month  city_code  \\\n",
       "0                      0        2       19             0.0          0   \n",
       "1                      0        2       27             1.0          0   \n",
       "2                      0        2       28             0.0          0   \n",
       "3                      0        2       29             0.0          0   \n",
       "4                      0        2       32             0.0          0   \n",
       "...                  ...      ...      ...             ...        ...   \n",
       "11127999              34       45    18454             0.0         20   \n",
       "11128000              34       45    16188             0.0         20   \n",
       "11128001              34       45    15757             0.0         20   \n",
       "11128002              34       45    19648             0.0         20   \n",
       "11128003              34       45      969             0.0         20   \n",
       "\n",
       "          item_category_id  type_code  subtype_code  item_cnt_month_lag_1  \\\n",
       "0                       40         11             4                   NaN   \n",
       "1                       19          5            10                   NaN   \n",
       "2                       30          8            55                   NaN   \n",
       "3                       23          5            16                   NaN   \n",
       "4                       40         11             4                   NaN   \n",
       "...                    ...        ...           ...                   ...   \n",
       "11127999                55         13             2                   1.0   \n",
       "11128000                64         14            42                   0.0   \n",
       "11128001                55         13             2                   0.0   \n",
       "11128002                40         11             4                   0.0   \n",
       "11128003                37         11             1                   0.0   \n",
       "\n",
       "          item_cnt_month_lag_2  item_cnt_month_lag_3  item_cnt_month_lag_6  \\\n",
       "0                          NaN                   NaN                   NaN   \n",
       "1                          NaN                   NaN                   NaN   \n",
       "2                          NaN                   NaN                   NaN   \n",
       "3                          NaN                   NaN                   NaN   \n",
       "4                          NaN                   NaN                   NaN   \n",
       "...                        ...                   ...                   ...   \n",
       "11127999                   0.0                   0.0                   0.0   \n",
       "11128000                   0.0                   NaN                   NaN   \n",
       "11128001                   0.0                   0.0                   0.0   \n",
       "11128002                   0.0                   0.0                   0.0   \n",
       "11128003                   0.0                   0.0                   0.0   \n",
       "\n",
       "          item_cnt_month_lag_12  \n",
       "0                           NaN  \n",
       "1                           NaN  \n",
       "2                           NaN  \n",
       "3                           NaN  \n",
       "4                           NaN  \n",
       "...                         ...  \n",
       "11127999                    NaN  \n",
       "11128000                    NaN  \n",
       "11128001                    0.0  \n",
       "11128002                    NaN  \n",
       "11128003                    0.0  \n",
       "\n",
       "[11128004 rows x 13 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c67bf4dbcef884ffe9d19c65d37bc4de1f287ef6"
   },
   "source": [
    "## Mean encoded features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "763aca242154ea10fa0a62fffadb4ef90e9532d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.4414122104644775"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = time.time()\n",
    "group = matrix.groupby(['date_block_num']).agg({'item_cnt_month': ['mean']})\n",
    "group.columns = [ 'date_avg_item_cnt' ]\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "matrix = pd.merge(matrix, group, on=['date_block_num'], how='left')\n",
    "matrix['date_avg_item_cnt'] = matrix['date_avg_item_cnt'].astype(np.float16)\n",
    "matrix = lag_feature(matrix, [1], 'date_avg_item_cnt')\n",
    "matrix.drop(['date_avg_item_cnt'], axis=1, inplace=True)\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "fc9166c4e678ebb99d03566f1751b7d4b5c690d2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.635373830795288"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = time.time()\n",
    "group = matrix.groupby(['date_block_num', 'item_id']).agg({'item_cnt_month': ['mean']})\n",
    "group.columns = [ 'date_item_avg_item_cnt' ]\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "matrix = pd.merge(matrix, group, on=['date_block_num','item_id'], how='left')\n",
    "matrix['date_item_avg_item_cnt'] = matrix['date_item_avg_item_cnt'].astype(np.float16)\n",
    "matrix = lag_feature(matrix, [1,2,3,6,12], 'date_item_avg_item_cnt')\n",
    "matrix.drop(['date_item_avg_item_cnt'], axis=1, inplace=True)\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "73f2552c403c5f67bbf07f28d69efcc015d00f32"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.36367917060852"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = time.time()\n",
    "group = matrix.groupby(['date_block_num', 'shop_id']).agg({'item_cnt_month': ['mean']})\n",
    "group.columns = [ 'date_shop_avg_item_cnt' ]\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "matrix = pd.merge(matrix, group, on=['date_block_num','shop_id'], how='left')\n",
    "matrix['date_shop_avg_item_cnt'] = matrix['date_shop_avg_item_cnt'].astype(np.float16)\n",
    "matrix = lag_feature(matrix, [1,2,3,6,12], 'date_shop_avg_item_cnt')\n",
    "matrix.drop(['date_shop_avg_item_cnt'], axis=1, inplace=True)\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "c3948a9b206bc480b31385c29a713aa49747de19"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.299272060394287"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = time.time()\n",
    "group = matrix.groupby(['date_block_num', 'item_category_id']).agg({'item_cnt_month': ['mean']})\n",
    "group.columns = [ 'date_cat_avg_item_cnt' ]\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "matrix = pd.merge(matrix, group, on=['date_block_num','item_category_id'], how='left')\n",
    "matrix['date_cat_avg_item_cnt'] = matrix['date_cat_avg_item_cnt'].astype(np.float16)\n",
    "matrix = lag_feature(matrix, [1], 'date_cat_avg_item_cnt')\n",
    "matrix.drop(['date_cat_avg_item_cnt'], axis=1, inplace=True)\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "bf98335755692f0d7666eeac2db1961692f09a16"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.587519407272339"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = time.time()\n",
    "group = matrix.groupby(['date_block_num', 'shop_id', 'item_category_id']).agg({'item_cnt_month': ['mean']})\n",
    "group.columns = ['date_shop_cat_avg_item_cnt']\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "matrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'item_category_id'], how='left')\n",
    "matrix['date_shop_cat_avg_item_cnt'] = matrix['date_shop_cat_avg_item_cnt'].astype(np.float16)\n",
    "matrix = lag_feature(matrix, [1], 'date_shop_cat_avg_item_cnt')\n",
    "matrix.drop(['date_shop_cat_avg_item_cnt'], axis=1, inplace=True)\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "3959603ea684eb3cbfd17d557399caa6e9da88e4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.663727283477783"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = time.time()\n",
    "group = matrix.groupby(['date_block_num', 'shop_id', 'type_code']).agg({'item_cnt_month': ['mean']})\n",
    "group.columns = ['date_shop_type_avg_item_cnt']\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "matrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'type_code'], how='left')\n",
    "matrix['date_shop_type_avg_item_cnt'] = matrix['date_shop_type_avg_item_cnt'].astype(np.float16)\n",
    "matrix = lag_feature(matrix, [1], 'date_shop_type_avg_item_cnt')\n",
    "matrix.drop(['date_shop_type_avg_item_cnt'], axis=1, inplace=True)\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "39f66d2e30f691237aa5d41ff9fc3a0eb7e9a788"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.83553409576416"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = time.time()\n",
    "group = matrix.groupby(['date_block_num', 'shop_id', 'subtype_code']).agg({'item_cnt_month': ['mean']})\n",
    "group.columns = ['date_shop_subtype_avg_item_cnt']\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "matrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'subtype_code'], how='left')\n",
    "matrix['date_shop_subtype_avg_item_cnt'] = matrix['date_shop_subtype_avg_item_cnt'].astype(np.float16)\n",
    "matrix = lag_feature(matrix, [1], 'date_shop_subtype_avg_item_cnt')\n",
    "matrix.drop(['date_shop_subtype_avg_item_cnt'], axis=1, inplace=True)\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "87d57d01beb0830138dabae79b4022d4c6a9cc12"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.676076412200928"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = time.time()\n",
    "group = matrix.groupby(['date_block_num', 'city_code']).agg({'item_cnt_month': ['mean']})\n",
    "group.columns = [ 'date_city_avg_item_cnt' ]\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "matrix = pd.merge(matrix, group, on=['date_block_num', 'city_code'], how='left')\n",
    "matrix['date_city_avg_item_cnt'] = matrix['date_city_avg_item_cnt'].astype(np.float16)\n",
    "matrix = lag_feature(matrix, [1], 'date_city_avg_item_cnt')\n",
    "matrix.drop(['date_city_avg_item_cnt'], axis=1, inplace=True)\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "db1f0170ec4a6fd9894bc53b36f3166d4b26abcf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.87193751335144"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = time.time()\n",
    "group = matrix.groupby(['date_block_num', 'item_id', 'city_code']).agg({'item_cnt_month': ['mean']})\n",
    "group.columns = [ 'date_item_city_avg_item_cnt' ]\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "matrix = pd.merge(matrix, group, on=['date_block_num', 'item_id', 'city_code'], how='left')\n",
    "matrix['date_item_city_avg_item_cnt'] = matrix['date_item_city_avg_item_cnt'].astype(np.float16)\n",
    "matrix = lag_feature(matrix, [1], 'date_item_city_avg_item_cnt')\n",
    "matrix.drop(['date_item_city_avg_item_cnt'], axis=1, inplace=True)\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "3cd5232ad63357dacebe9d223cc93dd669132bb7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.728000640869141"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = time.time()\n",
    "group = matrix.groupby(['date_block_num', 'type_code']).agg({'item_cnt_month': ['mean']})\n",
    "group.columns = [ 'date_type_avg_item_cnt' ]\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "matrix = pd.merge(matrix, group, on=['date_block_num', 'type_code'], how='left')\n",
    "matrix['date_type_avg_item_cnt'] = matrix['date_type_avg_item_cnt'].astype(np.float16)\n",
    "matrix = lag_feature(matrix, [1], 'date_type_avg_item_cnt')\n",
    "matrix.drop(['date_type_avg_item_cnt'], axis=1, inplace=True)\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "00394f3694ae9c7093176eadac7abeaa79ff5467"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.817093372344971"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = time.time()\n",
    "group = matrix.groupby(['date_block_num', 'subtype_code']).agg({'item_cnt_month': ['mean']})\n",
    "group.columns = [ 'date_subtype_avg_item_cnt' ]\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "matrix = pd.merge(matrix, group, on=['date_block_num', 'subtype_code'], how='left')\n",
    "matrix['date_subtype_avg_item_cnt'] = matrix['date_subtype_avg_item_cnt'].astype(np.float16)\n",
    "matrix = lag_feature(matrix, [1], 'date_subtype_avg_item_cnt')\n",
    "matrix.drop(['date_subtype_avg_item_cnt'], axis=1, inplace=True)\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix.to_pickle('data_new.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6bcea31d93ab035ca3fa1ed7c0afddbf602c414a"
   },
   "source": [
    "## Trend features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0504e9613087237c255914d9ebd165fac4e88cd0"
   },
   "source": [
    "Price trend for the last six months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0da2ded8502e273137991fd2bebbadaf19c19622"
   },
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "group = train.groupby(['item_id']).agg({'item_price': ['mean']})\n",
    "group.columns = ['item_avg_item_price']\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "matrix = pd.merge(matrix, group, on=['item_id'], how='left')\n",
    "matrix['item_avg_item_price'] = matrix['item_avg_item_price'].astype(np.float16)\n",
    "\n",
    "group = train.groupby(['date_block_num','item_id']).agg({'item_price': ['mean']})\n",
    "group.columns = ['date_item_avg_item_price']\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "matrix = pd.merge(matrix, group, on=['date_block_num','item_id'], how='left')\n",
    "matrix['date_item_avg_item_price'] = matrix['date_item_avg_item_price'].astype(np.float16)\n",
    "\n",
    "lags = [1,2,3,4,5,6]\n",
    "matrix = lag_feature(matrix, lags, 'date_item_avg_item_price')\n",
    "\n",
    "for i in lags:\n",
    "    matrix['delta_price_lag_'+str(i)] = \\\n",
    "        (matrix['date_item_avg_item_price_lag_'+str(i)] - matrix['item_avg_item_price']) / matrix['item_avg_item_price']\n",
    "\n",
    "def select_trend(row):\n",
    "    for i in lags:\n",
    "        if row['delta_price_lag_'+str(i)]:\n",
    "            return row['delta_price_lag_'+str(i)]\n",
    "    return 0\n",
    "    \n",
    "matrix['delta_price_lag'] = matrix.apply(select_trend, axis=1)\n",
    "matrix['delta_price_lag'] = matrix['delta_price_lag'].astype(np.float16)\n",
    "matrix['delta_price_lag'].fillna(0, inplace=True)\n",
    "\n",
    "# https://stackoverflow.com/questions/31828240/first-non-null-value-per-row-from-a-list-of-pandas-columns/31828559\n",
    "# matrix['price_trend'] = matrix[['delta_price_lag_1','delta_price_lag_2','delta_price_lag_3']].bfill(axis=1).iloc[:, 0]\n",
    "# Invalid dtype for backfill_2d [float16]\n",
    "\n",
    "fetures_to_drop = ['item_avg_item_price', 'date_item_avg_item_price']\n",
    "for i in lags:\n",
    "    fetures_to_drop += ['date_item_avg_item_price_lag_'+str(i)]\n",
    "    fetures_to_drop += ['delta_price_lag_'+str(i)]\n",
    "\n",
    "matrix.drop(fetures_to_drop, axis=1, inplace=True)\n",
    "\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "17765ddb48f52abd88847a42c0a3ffe974e5b121"
   },
   "source": [
    "Last month shop revenue trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e633be47f1a22b41487866ce67fb874bd296339e"
   },
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "group = train.groupby(['date_block_num','shop_id']).agg({'revenue': ['sum']})\n",
    "group.columns = ['date_shop_revenue']\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "matrix = pd.merge(matrix, group, on=['date_block_num','shop_id'], how='left')\n",
    "matrix['date_shop_revenue'] = matrix['date_shop_revenue'].astype(np.float32)\n",
    "\n",
    "group = group.groupby(['shop_id']).agg({'date_shop_revenue': ['mean']})\n",
    "group.columns = ['shop_avg_revenue']\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "matrix = pd.merge(matrix, group, on=['shop_id'], how='left')\n",
    "matrix['shop_avg_revenue'] = matrix['shop_avg_revenue'].astype(np.float32)\n",
    "\n",
    "matrix['delta_revenue'] = (matrix['date_shop_revenue'] - matrix['shop_avg_revenue']) / matrix['shop_avg_revenue']\n",
    "matrix['delta_revenue'] = matrix['delta_revenue'].astype(np.float16)\n",
    "\n",
    "matrix = lag_feature(matrix, [1], 'delta_revenue')\n",
    "\n",
    "matrix.drop(['date_shop_revenue','shop_avg_revenue','delta_revenue'], axis=1, inplace=True)\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "47e06af411b7d26cd93dad3d6735e48e5fbdee50"
   },
   "source": [
    "## Special features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bb521e1f33d4124a3b90b47447bdb29150770b6e"
   },
   "outputs": [],
   "source": [
    "matrix['month'] = matrix['date_block_num'] % 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b4dc4d2ff86483989c4b74fc02a0d01ca68a5c75"
   },
   "source": [
    "Number of days in a month. There are no leap years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e23f0201056b73368e3b70d4c36c6bb9e4a55291"
   },
   "outputs": [],
   "source": [
    "days = pd.Series([31,28,31,30,31,30,31,31,30,31,30,31])\n",
    "matrix['days'] = matrix['month'].map(days).astype(np.int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7c096e86eb0043c0f6eeb899de24e28ca4c4e044"
   },
   "source": [
    "Months since the last sale for each shop/item pair and for item only. I use programing approach.\n",
    "\n",
    "<i>Create HashTable with key equals to {shop_id,item_id} and value equals to date_block_num. Iterate data from the top. Foreach row if {row.shop_id,row.item_id} is not present in the table, then add it to the table and set its value to row.date_block_num. if HashTable contains key, then calculate the difference beteween cached value and row.date_block_num.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3458a7056c963167760921417d1f863f074f2b39"
   },
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "cache = {}\n",
    "matrix['item_shop_last_sale'] = -1\n",
    "matrix['item_shop_last_sale'] = matrix['item_shop_last_sale'].astype(np.int8)\n",
    "for idx, row in matrix.iterrows():    \n",
    "    key = str(row.item_id)+' '+str(row.shop_id)\n",
    "    if key not in cache:\n",
    "        if row.item_cnt_month!=0:\n",
    "            cache[key] = row.date_block_num\n",
    "    else:\n",
    "        last_date_block_num = cache[key]\n",
    "        matrix.at[idx, 'item_shop_last_sale'] = row.date_block_num - last_date_block_num\n",
    "        cache[key] = row.date_block_num         \n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "28b29fae3906d870b4dc3064a7f359b6d3abf623"
   },
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "cache = {}\n",
    "matrix['item_last_sale'] = -1\n",
    "matrix['item_last_sale'] = matrix['item_last_sale'].astype(np.int8)\n",
    "for idx, row in matrix.iterrows():    \n",
    "    key = row.item_id\n",
    "    if key not in cache:\n",
    "        if row.item_cnt_month!=0:\n",
    "            cache[key] = row.date_block_num\n",
    "    else:\n",
    "        last_date_block_num = cache[key]\n",
    "        if row.date_block_num>last_date_block_num:\n",
    "            matrix.at[idx, 'item_last_sale'] = row.date_block_num - last_date_block_num\n",
    "            cache[key] = row.date_block_num         \n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "61987e6adc1bec2ea897eec837c0253f7f73fdb5"
   },
   "source": [
    "Months since the first sale for each shop/item pair and for item only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ad0869709bbada35726d5ca41dd913d817249f8e"
   },
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "matrix['item_shop_first_sale'] = matrix['date_block_num'] - matrix.groupby(['item_id','shop_id'])['date_block_num'].transform('min')\n",
    "matrix['item_first_sale'] = matrix['date_block_num'] - matrix.groupby('item_id')['date_block_num'].transform('min')\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "966cb34ccfe849fbb3707d93270691cb8eef7a89"
   },
   "source": [
    "## Final preparations\n",
    "Because of the using 12 as lag value drop first 12 months. Also drop all the columns with this month calculated values (other words which can not be calcucated for the test set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "04df1bc4240f409a5d4521c6f70c2ced44f7c3d4"
   },
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "matrix = matrix[matrix.date_block_num > 11]\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "48a14784050901f878b40f093e4bc34e07ecce05"
   },
   "source": [
    "Producing lags brings a lot of nulls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8e5d8cb5cea9be28af4a0486cc1bf797e5b5c7ee"
   },
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "def fill_na(df):\n",
    "    for col in df.columns:\n",
    "        if ('_lag_' in col) & (df[col].isnull().any()):\n",
    "            if ('item_cnt' in col):\n",
    "                df[col].fillna(0, inplace=True)         \n",
    "    return df\n",
    "\n",
    "matrix = fill_na(matrix)\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "00bf3fffc1b143d0555d03b9d79b5fd00d9d0dc9"
   },
   "outputs": [],
   "source": [
    "matrix.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5f4e4c5c552daf8d4da6999ae4b63f13459b2887"
   },
   "outputs": [],
   "source": [
    "matrix.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB clip target here\n",
    "matrix['item_cnt_month'] = (matrix['item_cnt_month']\n",
    "                                .clip(0,20) \n",
    "                                .astype(np.float16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5d9988f8da8876f74092fbf827ceb6c61dd09d5e"
   },
   "outputs": [],
   "source": [
    "matrix.to_pickle('data_new.pkl')\n",
    "del matrix\n",
    "del cache\n",
    "del group\n",
    "del items\n",
    "del shops\n",
    "del cats\n",
    "del train\n",
    "# leave test for submission\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b69932efb440af8f6435f3cd802fbcd15682af71",
    "collapsed": true
   },
   "source": [
    "# Part 2, xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "a54364495b1818e9f069efa0c53500bf9e21d5f9"
   },
   "outputs": [],
   "source": [
    "data = pd.read_pickle('data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e5742775554b9e48e4d5c19784184069ad3eb9fb"
   },
   "source": [
    "Select perfect features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "bfc928a916bb8b285b2fe90fb1a311cf2fbbf2e3"
   },
   "outputs": [],
   "source": [
    "data = data[[\n",
    "    'date_block_num',\n",
    "    'shop_id',\n",
    "    'item_id',\n",
    "    'item_cnt_month',\n",
    "    'city_code',\n",
    "    'item_category_id',\n",
    "    'type_code',\n",
    "    'subtype_code',\n",
    "    'item_cnt_month_lag_1',\n",
    "    'item_cnt_month_lag_2',\n",
    "    'item_cnt_month_lag_3',\n",
    "    'item_cnt_month_lag_6',\n",
    "    'item_cnt_month_lag_12',\n",
    "    'date_avg_item_cnt_lag_1',\n",
    "    'date_item_avg_item_cnt_lag_1',\n",
    "    'date_item_avg_item_cnt_lag_2',\n",
    "    'date_item_avg_item_cnt_lag_3',\n",
    "    'date_item_avg_item_cnt_lag_6',\n",
    "    'date_item_avg_item_cnt_lag_12',\n",
    "    'date_shop_avg_item_cnt_lag_1',\n",
    "    'date_shop_avg_item_cnt_lag_2',\n",
    "    'date_shop_avg_item_cnt_lag_3',\n",
    "    'date_shop_avg_item_cnt_lag_6',\n",
    "    'date_shop_avg_item_cnt_lag_12',\n",
    "    'date_cat_avg_item_cnt_lag_1',\n",
    "    'date_shop_cat_avg_item_cnt_lag_1',\n",
    "    #'date_shop_type_avg_item_cnt_lag_1',\n",
    "    #'date_shop_subtype_avg_item_cnt_lag_1',\n",
    "    'date_city_avg_item_cnt_lag_1',\n",
    "    'date_item_city_avg_item_cnt_lag_1',\n",
    "    #'date_type_avg_item_cnt_lag_1',\n",
    "    #'date_subtype_avg_item_cnt_lag_1',\n",
    "    'delta_price_lag',\n",
    "    'month',\n",
    "    'days',\n",
    "    'item_shop_last_sale',\n",
    "    'item_last_sale',\n",
    "    'item_shop_first_sale',\n",
    "    'item_first_sale',\n",
    "]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "11eb4f2f5ada18aa8993ec55e8c63e80758fc19e"
   },
   "source": [
    "Validation strategy is 34 month for the test set, 33 month for the validation set and 13-33 months for the train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "9af76d7b80064573a453e5e10c35b76fc31c47a4"
   },
   "outputs": [],
   "source": [
    "X_train = data[data.date_block_num < 33].drop(['item_cnt_month'], axis=1)\n",
    "Y_train = data[data.date_block_num < 33]['item_cnt_month']\n",
    "X_valid = data[data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\n",
    "Y_valid = data[data.date_block_num == 33]['item_cnt_month']\n",
    "X_test = data[data.date_block_num == 34].drop(['item_cnt_month'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "6ea5437e8949db6d3e54e68b7b0c18cd0befe38c"
   },
   "outputs": [],
   "source": [
    "del data\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "acef75c36501f808d45f81fc69f9708fc3283bc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:17:55] WARNING: /workspace/src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for faster speed. To use old behavior (exact greedy algorithm on single machine), set tree_method to 'exact'.\n",
      "[0]\tvalidation_0-rmse:1.15143\tvalidation_1-rmse:1.11694\n",
      "Multiple eval metrics have been passed: 'validation_1-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-rmse hasn't improved in 10 rounds.\n",
      "[1]\tvalidation_0-rmse:1.10762\tvalidation_1-rmse:1.08392\n",
      "[2]\tvalidation_0-rmse:1.06571\tvalidation_1-rmse:1.05302\n",
      "[3]\tvalidation_0-rmse:1.03242\tvalidation_1-rmse:1.02733\n",
      "[4]\tvalidation_0-rmse:1.00470\tvalidation_1-rmse:1.00850\n",
      "[5]\tvalidation_0-rmse:0.97947\tvalidation_1-rmse:0.99005\n",
      "[6]\tvalidation_0-rmse:0.95821\tvalidation_1-rmse:0.97509\n",
      "[7]\tvalidation_0-rmse:0.93972\tvalidation_1-rmse:0.96128\n",
      "[8]\tvalidation_0-rmse:0.92408\tvalidation_1-rmse:0.95111\n",
      "[9]\tvalidation_0-rmse:0.91121\tvalidation_1-rmse:0.94314\n",
      "[10]\tvalidation_0-rmse:0.90040\tvalidation_1-rmse:0.93633\n",
      "[11]\tvalidation_0-rmse:0.89129\tvalidation_1-rmse:0.93084\n",
      "[12]\tvalidation_0-rmse:0.88348\tvalidation_1-rmse:0.92656\n",
      "[13]\tvalidation_0-rmse:0.87708\tvalidation_1-rmse:0.92328\n",
      "[14]\tvalidation_0-rmse:0.87005\tvalidation_1-rmse:0.92086\n",
      "[15]\tvalidation_0-rmse:0.86483\tvalidation_1-rmse:0.91820\n",
      "[16]\tvalidation_0-rmse:0.86005\tvalidation_1-rmse:0.91601\n",
      "[17]\tvalidation_0-rmse:0.85591\tvalidation_1-rmse:0.91449\n",
      "[18]\tvalidation_0-rmse:0.85199\tvalidation_1-rmse:0.91319\n",
      "[19]\tvalidation_0-rmse:0.84887\tvalidation_1-rmse:0.91219\n",
      "[20]\tvalidation_0-rmse:0.84624\tvalidation_1-rmse:0.91118\n",
      "[21]\tvalidation_0-rmse:0.84364\tvalidation_1-rmse:0.91025\n",
      "[22]\tvalidation_0-rmse:0.84109\tvalidation_1-rmse:0.90925\n",
      "[23]\tvalidation_0-rmse:0.83917\tvalidation_1-rmse:0.90845\n",
      "[24]\tvalidation_0-rmse:0.83707\tvalidation_1-rmse:0.90785\n",
      "[25]\tvalidation_0-rmse:0.83506\tvalidation_1-rmse:0.90688\n",
      "[26]\tvalidation_0-rmse:0.83364\tvalidation_1-rmse:0.90694\n",
      "[27]\tvalidation_0-rmse:0.83189\tvalidation_1-rmse:0.90651\n",
      "[28]\tvalidation_0-rmse:0.83042\tvalidation_1-rmse:0.90697\n",
      "[29]\tvalidation_0-rmse:0.82895\tvalidation_1-rmse:0.90688\n",
      "[30]\tvalidation_0-rmse:0.82787\tvalidation_1-rmse:0.90692\n",
      "[31]\tvalidation_0-rmse:0.82653\tvalidation_1-rmse:0.90621\n",
      "[32]\tvalidation_0-rmse:0.82565\tvalidation_1-rmse:0.90597\n",
      "[33]\tvalidation_0-rmse:0.82491\tvalidation_1-rmse:0.90556\n",
      "[34]\tvalidation_0-rmse:0.82398\tvalidation_1-rmse:0.90601\n",
      "[35]\tvalidation_0-rmse:0.82326\tvalidation_1-rmse:0.90608\n",
      "[36]\tvalidation_0-rmse:0.82261\tvalidation_1-rmse:0.90608\n",
      "[37]\tvalidation_0-rmse:0.82179\tvalidation_1-rmse:0.90631\n",
      "[38]\tvalidation_0-rmse:0.82106\tvalidation_1-rmse:0.90627\n",
      "[39]\tvalidation_0-rmse:0.82046\tvalidation_1-rmse:0.90667\n",
      "[40]\tvalidation_0-rmse:0.81983\tvalidation_1-rmse:0.90673\n",
      "[41]\tvalidation_0-rmse:0.81931\tvalidation_1-rmse:0.90634\n",
      "[42]\tvalidation_0-rmse:0.81873\tvalidation_1-rmse:0.90612\n",
      "[43]\tvalidation_0-rmse:0.81838\tvalidation_1-rmse:0.90630\n",
      "Stopping. Best iteration:\n",
      "[33]\tvalidation_0-rmse:0.82491\tvalidation_1-rmse:0.90556\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "304.514164686203"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = time.time()\n",
    "\n",
    "model = XGBRegressor(\n",
    "    max_depth=8,\n",
    "    n_estimators=1000,\n",
    "    min_child_weight=300, \n",
    "    colsample_bytree=0.8, \n",
    "    subsample=0.8, \n",
    "    eta=0.1,    \n",
    "    seed=42)\n",
    "\n",
    "model.fit(\n",
    "    X_train, \n",
    "    Y_train, \n",
    "    eval_metric=\"rmse\", \n",
    "    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n",
    "    verbose=True, \n",
    "    early_stopping_rounds = 10)\n",
    "\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting hyperopt\n",
      "  Downloading hyperopt-0.2.4-py2.py3-none-any.whl (964 kB)\n",
      "\u001b[K     |████████████████████████████████| 964 kB 815 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /home/max/.local/lib/python3.6/site-packages (from hyperopt) (4.40.2)\n",
      "Requirement already satisfied: six in /home/max/.local/lib/python3.6/site-packages (from hyperopt) (1.14.0)\n",
      "Collecting cloudpickle\n",
      "  Downloading cloudpickle-1.5.0-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: scipy in /home/max/.local/lib/python3.6/site-packages (from hyperopt) (1.4.1)\n",
      "Requirement already satisfied: future in /home/max/.local/lib/python3.6/site-packages (from hyperopt) (0.18.2)\n",
      "Requirement already satisfied: numpy in /home/max/.local/lib/python3.6/site-packages (from hyperopt) (1.17.4)\n",
      "Requirement already satisfied: networkx>=2.2 in /home/max/.local/lib/python3.6/site-packages (from hyperopt) (2.3)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/max/.local/lib/python3.6/site-packages (from networkx>=2.2->hyperopt) (4.4.1)\n",
      "Installing collected packages: cloudpickle, hyperopt\n",
      "Successfully installed cloudpickle-1.5.0 hyperopt-0.2.4\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting lightgbm\n",
      "  Downloading lightgbm-2.3.1-py2.py3-none-manylinux1_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 397 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn in /home/max/.local/lib/python3.6/site-packages (from lightgbm) (0.22.2.post1)\n",
      "Requirement already satisfied: numpy in /home/max/.local/lib/python3.6/site-packages (from lightgbm) (1.17.4)\n",
      "Requirement already satisfied: scipy in /home/max/.local/lib/python3.6/site-packages (from lightgbm) (1.4.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/max/.local/lib/python3.6/site-packages (from scikit-learn->lightgbm) (0.14.1)\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-2.3.1\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm\n",
    "from functools import partial\n",
    "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK, STATUS_RUNNING\n",
    "# Define searched space\n",
    "hyper_space = {'objective': 'regression',\n",
    "               'metric':'rmse',\n",
    "               'boosting':'gbdt',\n",
    "#                'n_estimators': hp.choice('n_estimators', [25, 40, 50, 75, 100, 250, 500]),\n",
    "               'max_depth':  hp.choice('max_depth', [3, 5, 8, 10, 12, 15]),\n",
    "               'num_leaves': hp.choice('num_leaves', [25, 50, 75, 100, 125, 150, 225, 250, 350, 400, 500]),\n",
    "               'subsample': hp.choice('subsample', [.3, .5, .7, .8, .9, 1]),\n",
    "               'colsample_bytree': hp.choice('colsample_bytree', [.5, .6, .7, .8, .9, 1]),\n",
    "               'learning_rate': hp.choice('learning_rate', [.01, .001, .05, .1, .2]),\n",
    "               'reg_alpha': hp.choice('reg_alpha', [.1, .2, .3, .4, .5, .6, .7]),\n",
    "               'reg_lambda':  hp.choice('reg_lambda', [.1, .2, .3, .4, .5, .6]), \n",
    "                # 'bagging_fraction': hp.choice('bagging_fraction', [.5, .6, .7, .8, .9, 1]),\n",
    "               'feature_fraction':  hp.choice('feature_fraction', [.6, .7, .8, .9, 1]), \n",
    "               'bagging_frequency':  hp.choice('bagging_frequency', [.3, .4, .5, .6, .7, .8, .9]),                  \n",
    "               'min_child_samples': hp.choice('min_child_samples', [10, 20, 30, 40])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_pred, y):\n",
    "    return np.sqrt(np.mean(np.square(y - y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgtrain = lightgbm.Dataset(X_train, label=Y_train)\n",
    "lgval = lightgbm.Dataset(X_valid, label=Y_valid)\n",
    "\n",
    "def evaluate_metric(params):\n",
    "    \n",
    "    model_lgb = lightgbm.train(params, lgtrain, 1000, \n",
    "                          valid_sets=[lgtrain, lgval], early_stopping_rounds=100, \n",
    "                          verbose_eval=300)\n",
    "\n",
    "    pred = model_lgb.predict(X_valid, num_iteration=1000)\n",
    "\n",
    "    score = rmse(pred, Y_valid)\n",
    "    \n",
    "    print(score, params)\n",
    " \n",
    "    return {\n",
    "        'loss': score,\n",
    "        'status': STATUS_OK,\n",
    "        'stats_running': STATUS_RUNNING\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/200 [00:00<?, ?trial/s, best loss=?]\u001b[A\n",
      "\u001b[A                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      "  0%|          | 0/200 [00:03<?, ?trial/s, best loss=?]\u001b[A\n",
      "\u001b[A                                                    Early stopping, best iteration is:\n",
      "[21]\ttraining's rmse: 0.778524\tvalid_1's rmse: 0.917374\n",
      "\n",
      "  0%|          | 0/200 [00:27<?, ?trial/s, best loss=?]\u001b[A\n",
      "\u001b[A                                                    0.9173742285440434\n",
      "\n",
      "  0%|          | 0/200 [00:27<?, ?trial/s, best loss=?]\u001b[A\n",
      "\u001b[A                                                    {'bagging_frequency': 0.9, 'boosting': 'gbdt', 'colsample_bytree': 0.6, 'feature_fraction': 1, 'learning_rate': 0.2, 'max_depth': 12, 'metric': 'rmse', 'min_child_samples': 20, 'num_leaves': 150, 'objective': 'regression', 'reg_alpha': 0.1, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n",
      "  0%|          | 0/200 [00:27<?, ?trial/s, best loss=?]\u001b[A\n",
      "  0%|          | 1/200 [00:27<1:31:12, 27.50s/trial, best loss: 0.9173742285440434]\u001b[A\n",
      "\u001b[A                                                                                Training until validation scores don't improve for 100 rounds\n",
      "\n",
      "  0%|          | 1/200 [00:27<1:31:12, 27.50s/trial, best loss: 0.9173742285440434]\u001b[A\n",
      "\u001b[A                                                                                Early stopping, best iteration is:\n",
      "[21]\ttraining's rmse: 0.778524\tvalid_1's rmse: 0.917374\n",
      "\n",
      "  0%|          | 1/200 [00:53<1:31:12, 27.50s/trial, best loss: 0.9173742285440434]\u001b[A\n",
      "\u001b[A                                                                                0.9173742285440434\n",
      "\n",
      "  0%|          | 1/200 [00:53<1:31:12, 27.50s/trial, best loss: 0.9173742285440434]\u001b[A\n",
      "\u001b[A                                                                                {'bagging_frequency': 0.9, 'boosting': 'gbdt', 'colsample_bytree': 0.6, 'feature_fraction': 1, 'learning_rate': 0.2, 'max_depth': 12, 'metric': 'rmse', 'min_child_samples': 20, 'num_leaves': 150, 'objective': 'regression', 'reg_alpha': 0.1, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n",
      "  0%|          | 1/200 [00:53<1:31:12, 27.50s/trial, best loss: 0.9173742285440434]\u001b[A\n",
      "  1%|          | 2/200 [00:53<1:29:31, 27.13s/trial, best loss: 0.9173742285440434]\u001b[A\n",
      "\u001b[A                                                                                Training until validation scores don't improve for 100 rounds\n",
      "\n",
      "  1%|          | 2/200 [00:54<1:29:31, 27.13s/trial, best loss: 0.9173742285440434]\u001b[A\n",
      "\u001b[A                                                                                Early stopping, best iteration is:\n",
      "[24]\ttraining's rmse: 0.825452\tvalid_1's rmse: 0.909048\n",
      "\n",
      "  1%|          | 2/200 [01:27<1:29:31, 27.13s/trial, best loss: 0.9173742285440434]\u001b[A\n",
      "\u001b[A                                                                                0.9090476749530313\n",
      "\n",
      "  1%|          | 2/200 [01:27<1:29:31, 27.13s/trial, best loss: 0.9173742285440434]\u001b[A\n",
      "\u001b[A                                                                                {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.8, 'feature_fraction': 0.9, 'learning_rate': 0.1, 'max_depth': 8, 'metric': 'rmse', 'min_child_samples': 20, 'num_leaves': 250, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n",
      "  1%|          | 2/200 [01:27<1:29:31, 27.13s/trial, best loss: 0.9173742285440434]\u001b[A\n",
      "  2%|▏         | 3/200 [01:27<1:35:38, 29.13s/trial, best loss: 0.9090476749530313]\u001b[A\n",
      "\u001b[A                                                                                Training until validation scores don't improve for 100 rounds\n",
      "\n",
      "  2%|▏         | 3/200 [01:28<1:35:38, 29.13s/trial, best loss: 0.9090476749530313]\u001b[A\n",
      "\u001b[A                                                                                Early stopping, best iteration is:\n",
      "[30]\ttraining's rmse: 0.813395\tvalid_1's rmse: 0.906779\n",
      "\n",
      "  2%|▏         | 3/200 [02:03<1:35:38, 29.13s/trial, best loss: 0.9090476749530313]\u001b[A\n",
      "\u001b[A                                                                                0.9067793484419511\n",
      "\n",
      "  2%|▏         | 3/200 [02:03<1:35:38, 29.13s/trial, best loss: 0.9090476749530313]\u001b[A\n",
      "\u001b[A                                                                                {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.8, 'feature_fraction': 0.9, 'learning_rate': 0.1, 'max_depth': 8, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 250, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.4, 'subsample': 0.5}\n",
      "\n",
      "  2%|▏         | 3/200 [02:03<1:35:38, 29.13s/trial, best loss: 0.9090476749530313]\u001b[A\n",
      "  2%|▏         | 4/200 [02:03<1:41:47, 31.16s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                Training until validation scores don't improve for 100 rounds\n",
      "\n",
      "  2%|▏         | 4/200 [02:04<1:41:47, 31.16s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                [300]\ttraining's rmse: 1.04368\tvalid_1's rmse: 1.0379\n",
      "\n",
      "  2%|▏         | 4/200 [04:12<1:41:47, 31.16s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                [600]\ttraining's rmse: 0.949026\tvalid_1's rmse: 0.984999\n",
      "\n",
      "  2%|▏         | 4/200 [06:11<1:41:47, 31.16s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                [900]\ttraining's rmse: 0.887069\tvalid_1's rmse: 0.9587\n",
      "\n",
      "  2%|▏         | 4/200 [08:04<1:41:47, 31.16s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 0.871445\tvalid_1's rmse: 0.953864\n",
      "\n",
      "  2%|▏         | 4/200 [08:41<1:41:47, 31.16s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                0.9538640446943186\n",
      "\n",
      "  2%|▏         | 4/200 [08:46<1:41:47, 31.16s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.8, 'learning_rate': 0.001, 'max_depth': 15, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 350, 'objective': 'regression', 'reg_alpha': 0.6, 'reg_lambda': 0.4, 'subsample': 0.5}\n",
      "\n",
      "  2%|▏         | 4/200 [08:46<1:41:47, 31.16s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "  2%|▎         | 5/200 [08:46<7:43:46, 142.70s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                 Training until validation scores don't improve for 100 rounds\n",
      "\n",
      "  2%|▎         | 5/200 [08:46<7:43:46, 142.70s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                 Early stopping, best iteration is:\n",
      "[84]\ttraining's rmse: 0.797815\tvalid_1's rmse: 0.909246\n",
      "\n",
      "  2%|▎         | 5/200 [09:32<7:43:46, 142.70s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                 0.9092464410188473\n",
      "\n",
      "  2%|▎         | 5/200 [09:32<7:43:46, 142.70s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                 {'bagging_frequency': 0.3, 'boosting': 'gbdt', 'colsample_bytree': 0.8, 'feature_fraction': 0.6, 'learning_rate': 0.05, 'max_depth': 8, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 250, 'objective': 'regression', 'reg_alpha': 0.2, 'reg_lambda': 0.4, 'subsample': 0.5}\n",
      "\n",
      "  2%|▎         | 5/200 [09:32<7:43:46, 142.70s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "  3%|▎         | 6/200 [09:32<6:08:07, 113.86s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                 Training until validation scores don't improve for 100 rounds\n",
      "\n",
      "  3%|▎         | 6/200 [09:33<6:08:07, 113.86s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                 [300]\ttraining's rmse: 0.847386\tvalid_1's rmse: 0.924617\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 6/200 [10:09<6:08:07, 113.86s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                 Early stopping, best iteration is:\n",
      "[406]\ttraining's rmse: 0.840911\tvalid_1's rmse: 0.921408\n",
      "\n",
      "  3%|▎         | 6/200 [10:33<6:08:07, 113.86s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                 0.9214080626766782\n",
      "\n",
      "  3%|▎         | 6/200 [10:34<6:08:07, 113.86s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                 {'bagging_frequency': 0.6, 'boosting': 'gbdt', 'colsample_bytree': 0.5, 'feature_fraction': 0.9, 'learning_rate': 0.1, 'max_depth': 3, 'metric': 'rmse', 'min_child_samples': 30, 'num_leaves': 500, 'objective': 'regression', 'reg_alpha': 0.3, 'reg_lambda': 0.5, 'subsample': 0.8}\n",
      "\n",
      "  3%|▎         | 6/200 [10:34<6:08:07, 113.86s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "  4%|▎         | 7/200 [10:34<5:15:41, 98.14s/trial, best loss: 0.9067793484419511] \u001b[A\n",
      "\u001b[A                                                                                Training until validation scores don't improve for 100 rounds\n",
      "\n",
      "  4%|▎         | 7/200 [10:34<5:15:41, 98.14s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                [300]\ttraining's rmse: 0.780026\tvalid_1's rmse: 0.909437\n",
      "\n",
      "  4%|▎         | 7/200 [12:21<5:15:41, 98.14s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                Early stopping, best iteration is:\n",
      "[261]\ttraining's rmse: 0.789407\tvalid_1's rmse: 0.908606\n",
      "\n",
      "  4%|▎         | 7/200 [12:40<5:15:41, 98.14s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                0.9086058934807201\n",
      "\n",
      "  4%|▎         | 7/200 [12:41<5:15:41, 98.14s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                {'bagging_frequency': 0.5, 'boosting': 'gbdt', 'colsample_bytree': 1, 'feature_fraction': 0.7, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 40, 'num_leaves': 400, 'objective': 'regression', 'reg_alpha': 0.7, 'reg_lambda': 0.3, 'subsample': 0.7}\n",
      "\n",
      "  4%|▎         | 7/200 [12:41<5:15:41, 98.14s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "  4%|▍         | 8/200 [12:41<5:42:04, 106.90s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                 Training until validation scores don't improve for 100 rounds\n",
      "\n",
      "  4%|▍         | 8/200 [12:42<5:42:04, 106.90s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                 Early stopping, best iteration is:\n",
      "[115]\ttraining's rmse: 0.828595\tvalid_1's rmse: 0.917986\n",
      "\n",
      "  4%|▍         | 8/200 [13:13<5:42:04, 106.90s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                 0.9179856170727786\n",
      "\n",
      "  4%|▍         | 8/200 [13:13<5:42:04, 106.90s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                 {'bagging_frequency': 0.8, 'boosting': 'gbdt', 'colsample_bytree': 0.9, 'feature_fraction': 0.9, 'learning_rate': 0.1, 'max_depth': 5, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 25, 'objective': 'regression', 'reg_alpha': 0.4, 'reg_lambda': 0.6, 'subsample': 0.9}\n",
      "\n",
      "  4%|▍         | 8/200 [13:13<5:42:04, 106.90s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "  4%|▍         | 9/200 [13:13<4:28:49, 84.45s/trial, best loss: 0.9067793484419511] \u001b[A\n",
      "\u001b[A                                                                                Training until validation scores don't improve for 100 rounds\n",
      "\n",
      "  4%|▍         | 9/200 [13:14<4:28:49, 84.45s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                Early stopping, best iteration is:\n",
      "[34]\ttraining's rmse: 0.816543\tvalid_1's rmse: 0.910636\n",
      "\n",
      "  4%|▍         | 9/200 [13:44<4:28:49, 84.45s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                0.9106357737218191\n",
      "\n",
      "  4%|▍         | 9/200 [13:44<4:28:49, 84.45s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                {'bagging_frequency': 0.7, 'boosting': 'gbdt', 'colsample_bytree': 0.8, 'feature_fraction': 0.9, 'learning_rate': 0.1, 'max_depth': 8, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 125, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.1, 'subsample': 0.3}\n",
      "\n",
      "  4%|▍         | 9/200 [13:44<4:28:49, 84.45s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "  5%|▌         | 10/200 [13:44<3:36:26, 68.35s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                 Training until validation scores don't improve for 100 rounds\n",
      "\n",
      "  5%|▌         | 10/200 [13:44<3:36:26, 68.35s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                 [300]\ttraining's rmse: 0.832421\tvalid_1's rmse: 0.911857\n",
      "\n",
      "  5%|▌         | 10/200 [14:55<3:36:26, 68.35s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                 Early stopping, best iteration is:\n",
      "[335]\ttraining's rmse: 0.827778\tvalid_1's rmse: 0.911326\n",
      "\n",
      "  5%|▌         | 10/200 [15:22<3:36:26, 68.35s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                 0.9113256162773701\n",
      "\n",
      "  5%|▌         | 10/200 [15:23<3:36:26, 68.35s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                 {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.8, 'feature_fraction': 0.8, 'learning_rate': 0.01, 'max_depth': 8, 'metric': 'rmse', 'min_child_samples': 40, 'num_leaves': 75, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.4, 'subsample': 0.5}\n",
      "\n",
      "  5%|▌         | 10/200 [15:23<3:36:26, 68.35s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "  6%|▌         | 11/200 [15:23<4:04:17, 77.55s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                 Training until validation scores don't improve for 100 rounds\n",
      "\n",
      "  6%|▌         | 11/200 [15:23<4:04:17, 77.55s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                 [300]\ttraining's rmse: 1.0752\tvalid_1's rmse: 1.05741\n",
      "\n",
      "  6%|▌         | 11/200 [16:12<4:04:17, 77.55s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                 [600]\ttraining's rmse: 1.00254\tvalid_1's rmse: 1.00899\n",
      "\n",
      "  6%|▌         | 11/200 [17:03<4:04:17, 77.55s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                 [900]\ttraining's rmse: 0.956404\tvalid_1's rmse: 0.979598\n",
      "\n",
      "  6%|▌         | 11/200 [17:52<4:04:17, 77.55s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                 Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 0.945053\tvalid_1's rmse: 0.972619\n",
      "\n",
      "  6%|▌         | 11/200 [18:08<4:04:17, 77.55s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                 0.9726188328190164\n",
      "\n",
      "  6%|▌         | 11/200 [18:10<4:04:17, 77.55s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                 {'bagging_frequency': 0.3, 'boosting': 'gbdt', 'colsample_bytree': 0.9, 'feature_fraction': 0.7, 'learning_rate': 0.001, 'max_depth': 5, 'metric': 'rmse', 'min_child_samples': 30, 'num_leaves': 100, 'objective': 'regression', 'reg_alpha': 0.2, 'reg_lambda': 0.4, 'subsample': 0.5}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 11/200 [18:10<4:04:17, 77.55s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "  6%|▌         | 12/200 [18:10<5:26:41, 104.26s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                  Training until validation scores don't improve for 100 rounds\n",
      "\n",
      "  6%|▌         | 12/200 [18:10<5:26:41, 104.26s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                  Early stopping, best iteration is:\n",
      "[61]\ttraining's rmse: 0.79328\tvalid_1's rmse: 0.911591\n",
      "\n",
      "  6%|▌         | 12/200 [18:50<5:26:41, 104.26s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                  0.9115909078783013\n",
      "\n",
      "  6%|▌         | 12/200 [18:50<5:26:41, 104.26s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                  {'bagging_frequency': 0.6, 'boosting': 'gbdt', 'colsample_bytree': 1, 'feature_fraction': 0.6, 'learning_rate': 0.05, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 225, 'objective': 'regression', 'reg_alpha': 0.4, 'reg_lambda': 0.3, 'subsample': 0.7}\n",
      "\n",
      "  6%|▌         | 12/200 [18:50<5:26:41, 104.26s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "  6%|▋         | 13/200 [18:50<4:25:18, 85.13s/trial, best loss: 0.9067793484419511] \u001b[A\n",
      "\u001b[A                                                                                 Training until validation scores don't improve for 100 rounds\n",
      "\n",
      "  6%|▋         | 13/200 [18:50<4:25:18, 85.13s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                 [300]\ttraining's rmse: 0.845717\tvalid_1's rmse: 0.925582\n",
      "\n",
      "  6%|▋         | 13/200 [19:26<4:25:18, 85.13s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                 [600]\ttraining's rmse: 0.831691\tvalid_1's rmse: 0.924339\n",
      "\n",
      "  6%|▋         | 13/200 [20:03<4:25:18, 85.13s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                 Early stopping, best iteration is:\n",
      "[523]\ttraining's rmse: 0.834291\tvalid_1's rmse: 0.923236\n",
      "\n",
      "  6%|▋         | 13/200 [20:06<4:25:18, 85.13s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                 0.9232356465194839\n",
      "\n",
      "  6%|▋         | 13/200 [20:07<4:25:18, 85.13s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                 {'bagging_frequency': 0.5, 'boosting': 'gbdt', 'colsample_bytree': 0.5, 'feature_fraction': 0.9, 'learning_rate': 0.1, 'max_depth': 3, 'metric': 'rmse', 'min_child_samples': 40, 'num_leaves': 50, 'objective': 'regression', 'reg_alpha': 0.6, 'reg_lambda': 0.1, 'subsample': 0.9}\n",
      "\n",
      "  6%|▋         | 13/200 [20:07<4:25:18, 85.13s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "  7%|▋         | 14/200 [20:07<4:15:44, 82.50s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                 Training until validation scores don't improve for 100 rounds\n",
      "\n",
      "  7%|▋         | 14/200 [20:07<4:15:44, 82.50s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                 [300]\ttraining's rmse: 1.04882\tvalid_1's rmse: 1.03957\n",
      "\n",
      "  7%|▋         | 14/200 [21:55<4:15:44, 82.50s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                 [600]\ttraining's rmse: 0.957395\tvalid_1's rmse: 0.984725\n",
      "\n",
      "  7%|▋         | 14/200 [23:40<4:15:44, 82.50s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                 [900]\ttraining's rmse: 0.897397\tvalid_1's rmse: 0.956561\n",
      "\n",
      "  7%|▋         | 14/200 [25:20<4:15:44, 82.50s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                 Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 0.882326\tvalid_1's rmse: 0.950796\n",
      "\n",
      "  7%|▋         | 14/200 [25:53<4:15:44, 82.50s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                 0.9507962457084804\n",
      "\n",
      "  7%|▋         | 14/200 [25:57<4:15:44, 82.50s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                 {'bagging_frequency': 0.7, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.7, 'learning_rate': 0.001, 'max_depth': 15, 'metric': 'rmse', 'min_child_samples': 30, 'num_leaves': 250, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.6, 'subsample': 0.8}\n",
      "\n",
      "  7%|▋         | 14/200 [25:57<4:15:44, 82.50s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "  8%|▊         | 15/200 [25:57<8:22:25, 162.95s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                  Training until validation scores don't improve for 100 rounds\n",
      "\n",
      "  8%|▊         | 15/200 [25:58<8:22:25, 162.95s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                  Early stopping, best iteration is:\n",
      "[81]\ttraining's rmse: 0.80898\tvalid_1's rmse: 0.910818\n",
      "\n",
      "  8%|▊         | 15/200 [26:37<8:22:25, 162.95s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                  0.9108181260451362\n",
      "\n",
      "  8%|▊         | 15/200 [26:37<8:22:25, 162.95s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                  {'bagging_frequency': 0.8, 'boosting': 'gbdt', 'colsample_bytree': 0.8, 'feature_fraction': 0.6, 'learning_rate': 0.05, 'max_depth': 8, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 125, 'objective': 'regression', 'reg_alpha': 0.3, 'reg_lambda': 0.5, 'subsample': 0.3}\n",
      "\n",
      "  8%|▊         | 15/200 [26:37<8:22:25, 162.95s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "  8%|▊         | 16/200 [26:37<6:26:21, 125.99s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                  Training until validation scores don't improve for 100 rounds\n",
      "\n",
      "  8%|▊         | 16/200 [26:38<6:26:21, 125.99s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                  Early stopping, best iteration is:\n",
      "[10]\ttraining's rmse: 0.775471\tvalid_1's rmse: 0.922065\n",
      "\n",
      "  8%|▊         | 16/200 [27:11<6:26:21, 125.99s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                  0.9220645934769739\n",
      "\n",
      "  8%|▊         | 16/200 [27:12<6:26:21, 125.99s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                  {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.6, 'feature_fraction': 0.8, 'learning_rate': 0.2, 'max_depth': 15, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 500, 'objective': 'regression', 'reg_alpha': 0.7, 'reg_lambda': 0.4, 'subsample': 0.5}\n",
      "\n",
      "  8%|▊         | 16/200 [27:12<6:26:21, 125.99s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "  8%|▊         | 17/200 [27:12<5:00:37, 98.56s/trial, best loss: 0.9067793484419511] \u001b[A\n",
      "\u001b[A                                                                                 Training until validation scores don't improve for 100 rounds\n",
      "\n",
      "  8%|▊         | 17/200 [27:12<5:00:37, 98.56s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                 [300]\ttraining's rmse: 0.780026\tvalid_1's rmse: 0.909437\n",
      "\n",
      "  8%|▊         | 17/200 [29:04<5:00:37, 98.56s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                 Early stopping, best iteration is:\n",
      "[261]\ttraining's rmse: 0.789407\tvalid_1's rmse: 0.908606\n",
      "\n",
      "  8%|▊         | 17/200 [29:24<5:00:37, 98.56s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                 0.9086058934807201\n",
      "\n",
      "  8%|▊         | 17/200 [29:25<5:00:37, 98.56s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                 {'bagging_frequency': 0.5, 'boosting': 'gbdt', 'colsample_bytree': 1, 'feature_fraction': 0.7, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 40, 'num_leaves': 400, 'objective': 'regression', 'reg_alpha': 0.7, 'reg_lambda': 0.3, 'subsample': 0.7}\n",
      "\n",
      "  8%|▊         | 17/200 [29:25<5:00:37, 98.56s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "  9%|▉         | 18/200 [29:25<5:30:46, 109.04s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                  Training until validation scores don't improve for 100 rounds\n",
      "\n",
      "  9%|▉         | 18/200 [29:26<5:30:46, 109.04s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                  [300]\ttraining's rmse: 0.780026\tvalid_1's rmse: 0.909437\n",
      "\n",
      "  9%|▉         | 18/200 [31:17<5:30:46, 109.04s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                  Early stopping, best iteration is:\n",
      "[261]\ttraining's rmse: 0.789407\tvalid_1's rmse: 0.908606\n",
      "\n",
      "  9%|▉         | 18/200 [31:37<5:30:46, 109.04s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                  0.9086058934807201\n",
      "\n",
      "  9%|▉         | 18/200 [31:38<5:30:46, 109.04s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                  {'bagging_frequency': 0.5, 'boosting': 'gbdt', 'colsample_bytree': 1, 'feature_fraction': 0.7, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 40, 'num_leaves': 400, 'objective': 'regression', 'reg_alpha': 0.7, 'reg_lambda': 0.3, 'subsample': 0.7}\n",
      "\n",
      "  9%|▉         | 18/200 [31:38<5:30:46, 109.04s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      " 10%|▉         | 19/200 [31:38<5:50:57, 116.34s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                  Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 10%|▉         | 19/200 [31:39<5:50:57, 116.34s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                  [300]\ttraining's rmse: 0.780026\tvalid_1's rmse: 0.909437\n",
      "\n",
      " 10%|▉         | 19/200 [33:28<5:50:57, 116.34s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                  Early stopping, best iteration is:\n",
      "[261]\ttraining's rmse: 0.789407\tvalid_1's rmse: 0.908606\n",
      "\n",
      " 10%|▉         | 19/200 [33:47<5:50:57, 116.34s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                  0.9086058934807201\n",
      "\n",
      " 10%|▉         | 19/200 [33:48<5:50:57, 116.34s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                  {'bagging_frequency': 0.5, 'boosting': 'gbdt', 'colsample_bytree': 1, 'feature_fraction': 0.7, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 40, 'num_leaves': 400, 'objective': 'regression', 'reg_alpha': 0.7, 'reg_lambda': 0.3, 'subsample': 0.7}\n",
      "\n",
      " 10%|▉         | 19/200 [33:48<5:50:57, 116.34s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      " 10%|█         | 20/200 [33:48<6:01:03, 120.35s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                  Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 10%|█         | 20/200 [33:48<6:01:03, 120.35s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                  [300]\ttraining's rmse: 0.834472\tvalid_1's rmse: 0.918536\n",
      "\n",
      " 10%|█         | 20/200 [34:47<6:01:03, 120.35s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                  [600]\ttraining's rmse: 0.807319\tvalid_1's rmse: 0.917148\n",
      "\n",
      " 10%|█         | 20/200 [35:34<6:01:03, 120.35s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                  Early stopping, best iteration is:\n",
      "[516]\ttraining's rmse: 0.81295\tvalid_1's rmse: 0.916807\n",
      "\n",
      " 10%|█         | 20/200 [35:37<6:01:03, 120.35s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                  0.9168071599797161\n",
      "\n",
      " 10%|█         | 20/200 [35:38<6:01:03, 120.35s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                  {'bagging_frequency': 0.5, 'boosting': 'gbdt', 'colsample_bytree': 1, 'feature_fraction': 0.7, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 40, 'num_leaves': 50, 'objective': 'regression', 'reg_alpha': 0.7, 'reg_lambda': 0.3, 'subsample': 0.7}\n",
      "\n",
      " 10%|█         | 20/200 [35:38<6:01:03, 120.35s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      " 10%|█         | 21/200 [35:38<5:49:29, 117.15s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                  Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 10%|█         | 21/200 [35:38<5:49:29, 117.15s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                  [300]\ttraining's rmse: 0.783745\tvalid_1's rmse: 0.905312\n",
      "\n",
      " 10%|█         | 21/200 [37:35<5:49:29, 117.15s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                  Early stopping, best iteration is:\n",
      "[263]\ttraining's rmse: 0.792133\tvalid_1's rmse: 0.904819\n",
      "\n",
      " 10%|█         | 21/200 [37:56<5:49:29, 117.15s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                  0.9048186227889994\n",
      "\n",
      " 10%|█         | 21/200 [37:57<5:49:29, 117.15s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      "\u001b[A                                                                                  {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.5, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 40, 'num_leaves': 350, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.6, 'subsample': 0.8}\n",
      "\n",
      " 10%|█         | 21/200 [37:57<5:49:29, 117.15s/trial, best loss: 0.9067793484419511]\u001b[A\n",
      " 11%|█         | 22/200 [37:57<6:07:36, 123.91s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                  Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 11%|█         | 22/200 [37:58<6:07:36, 123.91s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                  Early stopping, best iteration is:\n",
      "[146]\ttraining's rmse: 0.819483\tvalid_1's rmse: 0.914704\n",
      "\n",
      " 11%|█         | 22/200 [38:36<6:07:36, 123.91s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                  0.9147036076698064\n",
      "\n",
      " 11%|█         | 22/200 [38:36<6:07:36, 123.91s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                  {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.5, 'feature_fraction': 0.9, 'learning_rate': 0.1, 'max_depth': 5, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 350, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.6, 'subsample': 0.8}\n",
      "\n",
      " 11%|█         | 22/200 [38:36<6:07:36, 123.91s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      " 12%|█▏        | 23/200 [38:36<4:50:19, 98.41s/trial, best loss: 0.9048186227889994] \u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[ATraining until validation scores don't improve for 100 rounds                    \n",
      "\n",
      " 12%|█▏        | 23/200 [38:37<4:50:19, 98.41s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 [300]\ttraining's rmse: 0.904619\tvalid_1's rmse: 0.956682\n",
      "\n",
      " 12%|█▏        | 23/200 [39:13<4:50:19, 98.41s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 [600]\ttraining's rmse: 0.885562\tvalid_1's rmse: 0.944896\n",
      "\n",
      " 12%|█▏        | 23/200 [39:50<4:50:19, 98.41s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 [900]\ttraining's rmse: 0.87481\tvalid_1's rmse: 0.937027\n",
      "\n",
      " 12%|█▏        | 23/200 [40:26<4:50:19, 98.41s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 0.870848\tvalid_1's rmse: 0.93397\n",
      "\n",
      " 12%|█▏        | 23/200 [40:38<4:50:19, 98.41s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 0.9339701903666001\n",
      "\n",
      " 12%|█▏        | 23/200 [40:39<4:50:19, 98.41s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.5, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 3, 'metric': 'rmse', 'min_child_samples': 20, 'num_leaves': 350, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.6, 'subsample': 0.8}\n",
      "\n",
      " 12%|█▏        | 23/200 [40:39<4:50:19, 98.41s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      " 12%|█▏        | 24/200 [40:39<5:10:23, 105.81s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                  Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 12%|█▏        | 24/200 [40:40<5:10:23, 105.81s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                  Early stopping, best iteration is:\n",
      "[109]\ttraining's rmse: 0.814405\tvalid_1's rmse: 0.912625\n",
      "\n",
      " 12%|█▏        | 24/200 [41:08<5:10:23, 105.81s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                  0.9126253102750023\n",
      "\n",
      " 12%|█▏        | 24/200 [41:09<5:10:23, 105.81s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                  {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.5, 'feature_fraction': 0.9, 'learning_rate': 0.1, 'max_depth': 8, 'metric': 'rmse', 'min_child_samples': 30, 'num_leaves': 25, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.6, 'subsample': 0.8}\n",
      "\n",
      " 12%|█▏        | 24/200 [41:09<5:10:23, 105.81s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      " 12%|█▎        | 25/200 [41:09<4:01:28, 82.79s/trial, best loss: 0.9048186227889994] \u001b[A\n",
      "\u001b[A                                                                                 Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 12%|█▎        | 25/200 [41:09<4:01:28, 82.79s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 Early stopping, best iteration is:\n",
      "[17]\ttraining's rmse: 0.800699\tvalid_1's rmse: 0.923085\n",
      "\n",
      " 12%|█▎        | 25/200 [41:30<4:01:28, 82.79s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 0.923084753847989\n",
      "\n",
      " 12%|█▎        | 25/200 [41:30<4:01:28, 82.79s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.5, 'feature_fraction': 1, 'learning_rate': 0.2, 'max_depth': 12, 'metric': 'rmse', 'min_child_samples': 40, 'num_leaves': 100, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'subsample': 0.8}\n",
      "\n",
      " 12%|█▎        | 25/200 [41:30<4:01:28, 82.79s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      " 13%|█▎        | 26/200 [41:30<3:06:50, 64.43s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 13%|█▎        | 26/200 [41:31<3:06:50, 64.43s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 Early stopping, best iteration is:\n",
      "[29]\ttraining's rmse: 0.78399\tvalid_1's rmse: 0.907631\n",
      "\n",
      " 13%|█▎        | 26/200 [42:11<3:06:50, 64.43s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 0.907630611273654\n",
      "\n",
      " 13%|█▎        | 26/200 [42:11<3:06:50, 64.43s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 {'bagging_frequency': 0.8, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.9, 'learning_rate': 0.1, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 40, 'num_leaves': 350, 'objective': 'regression', 'reg_alpha': 0.1, 'reg_lambda': 0.6, 'subsample': 0.3}\n",
      "\n",
      " 13%|█▎        | 26/200 [42:11<3:06:50, 64.43s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      " 14%|█▎        | 27/200 [42:11<2:45:16, 57.32s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 14%|█▎        | 27/200 [42:11<2:45:16, 57.32s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 [300]\ttraining's rmse: 0.813165\tvalid_1's rmse: 0.908726\n",
      "\n",
      " 14%|█▎        | 27/200 [43:45<2:45:16, 57.32s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 Early stopping, best iteration is:\n",
      "[335]\ttraining's rmse: 0.807481\tvalid_1's rmse: 0.908468\n",
      "\n",
      " 14%|█▎        | 27/200 [44:21<2:45:16, 57.32s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 0.9084682224945141\n",
      "\n",
      " 14%|█▎        | 27/200 [44:22<2:45:16, 57.32s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 {'bagging_frequency': 0.9, 'boosting': 'gbdt', 'colsample_bytree': 0.9, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 8, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 225, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.1, 'subsample': 0.9}\n",
      "\n",
      " 14%|█▎        | 27/200 [44:22<2:45:16, 57.32s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      " 14%|█▍        | 28/200 [44:22<3:48:11, 79.60s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 14%|█▍        | 28/200 [44:23<3:48:11, 79.60s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 Early stopping, best iteration is:\n",
      "[42]\ttraining's rmse: 0.802826\tvalid_1's rmse: 0.925485\n",
      "\n",
      " 14%|█▍        | 28/200 [45:06<3:48:11, 79.60s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 0.925484671694565\n",
      "\n",
      " 14%|█▍        | 28/200 [45:06<3:48:11, 79.60s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.8, 'feature_fraction': 1, 'learning_rate': 0.05, 'max_depth': 12, 'metric': 'rmse', 'min_child_samples': 20, 'num_leaves': 250, 'objective': 'regression', 'reg_alpha': 0.3, 'reg_lambda': 0.4, 'subsample': 0.5}\n",
      "\n",
      " 14%|█▍        | 28/200 [45:06<3:48:11, 79.60s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      " 14%|█▍        | 29/200 [45:06<3:16:13, 68.85s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 14%|█▍        | 29/200 [45:06<3:16:13, 68.85s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 [300]\ttraining's rmse: 1.07312\tvalid_1's rmse: 1.05597\n",
      "\n",
      " 14%|█▍        | 29/200 [46:01<3:16:13, 68.85s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 [600]\ttraining's rmse: 1.00084\tvalid_1's rmse: 1.00803\n",
      "\n",
      " 14%|█▍        | 29/200 [46:57<3:16:13, 68.85s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 [900]\ttraining's rmse: 0.95535\tvalid_1's rmse: 0.978958\n",
      "\n",
      " 14%|█▍        | 29/200 [47:52<3:16:13, 68.85s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 0.944202\tvalid_1's rmse: 0.972002\n",
      "\n",
      " 14%|█▍        | 29/200 [48:10<3:16:13, 68.85s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 0.9720018680624024\n",
      "\n",
      " 14%|█▍        | 29/200 [48:12<3:16:13, 68.85s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 {'bagging_frequency': 0.7, 'boosting': 'gbdt', 'colsample_bytree': 0.6, 'feature_fraction': 0.9, 'learning_rate': 0.001, 'max_depth': 5, 'metric': 'rmse', 'min_child_samples': 30, 'num_leaves': 75, 'objective': 'regression', 'reg_alpha': 0.6, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n",
      " 14%|█▍        | 29/200 [48:12<3:16:13, 68.85s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      " 15%|█▌        | 30/200 [48:12<4:54:09, 103.82s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                  Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 15%|█▌        | 30/200 [48:12<4:54:09, 103.82s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                  Early stopping, best iteration is:\n",
      "[12]\ttraining's rmse: 0.805472\tvalid_1's rmse: 0.929416\n",
      "\n",
      " 15%|█▌        | 30/200 [48:35<4:54:09, 103.82s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                  0.9294158964633117\n",
      "\n",
      " 15%|█▌        | 30/200 [48:35<4:54:09, 103.82s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                  {'bagging_frequency': 0.9, 'boosting': 'gbdt', 'colsample_bytree': 0.8, 'feature_fraction': 1, 'learning_rate': 0.2, 'max_depth': 12, 'metric': 'rmse', 'min_child_samples': 20, 'num_leaves': 150, 'objective': 'regression', 'reg_alpha': 0.1, 'reg_lambda': 0.6, 'subsample': 0.8}\n",
      "\n",
      " 15%|█▌        | 30/200 [48:35<4:54:09, 103.82s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      " 16%|█▌        | 31/200 [48:35<3:44:07, 79.57s/trial, best loss: 0.9048186227889994] \u001b[A\n",
      "\u001b[A                                                                                 Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 16%|█▌        | 31/200 [48:35<3:44:07, 79.57s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 Early stopping, best iteration is:\n",
      "[30]\ttraining's rmse: 0.815442\tvalid_1's rmse: 0.905082\n",
      "\n",
      " 16%|█▌        | 31/200 [49:08<3:44:07, 79.57s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 0.9050822444180289\n",
      "\n",
      " 16%|█▌        | 31/200 [49:08<3:44:07, 79.57s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 {'bagging_frequency': 0.3, 'boosting': 'gbdt', 'colsample_bytree': 0.6, 'feature_fraction': 0.9, 'learning_rate': 0.1, 'max_depth': 8, 'metric': 'rmse', 'min_child_samples': 40, 'num_leaves': 250, 'objective': 'regression', 'reg_alpha': 0.2, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n",
      " 16%|█▌        | 31/200 [49:08<3:44:07, 79.57s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      " 16%|█▌        | 32/200 [49:08<3:04:18, 65.82s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 16%|█▌        | 32/200 [49:09<3:04:18, 65.82s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 Early stopping, best iteration is:\n",
      "[17]\ttraining's rmse: 0.805877\tvalid_1's rmse: 0.913504\n",
      "\n",
      " 16%|█▌        | 32/200 [49:31<3:04:18, 65.82s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 0.913503930597604\n",
      "\n",
      " 16%|█▌        | 32/200 [49:31<3:04:18, 65.82s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 {'bagging_frequency': 0.3, 'boosting': 'gbdt', 'colsample_bytree': 0.6, 'feature_fraction': 0.6, 'learning_rate': 0.2, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 40, 'num_leaves': 150, 'objective': 'regression', 'reg_alpha': 0.2, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n",
      " 16%|█▌        | 32/200 [49:31<3:04:18, 65.82s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      " 16%|█▋        | 33/200 [49:31<2:27:02, 52.83s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 16%|█▋        | 33/200 [49:31<2:27:02, 52.83s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 [300]\ttraining's rmse: 0.755157\tvalid_1's rmse: 0.942259\n",
      "\n",
      " 16%|█▋        | 33/200 [51:13<2:27:02, 52.83s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 Early stopping, best iteration is:\n",
      "[215]\ttraining's rmse: 0.78222\tvalid_1's rmse: 0.937153\n",
      "\n",
      " 16%|█▋        | 33/200 [51:17<2:27:02, 52.83s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 0.9371529580694028\n",
      "\n",
      " 16%|█▋        | 33/200 [51:18<2:27:02, 52.83s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 {'bagging_frequency': 0.3, 'boosting': 'gbdt', 'colsample_bytree': 0.6, 'feature_fraction': 0.8, 'learning_rate': 0.01, 'max_depth': 15, 'metric': 'rmse', 'min_child_samples': 40, 'num_leaves': 350, 'objective': 'regression', 'reg_alpha': 0.2, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n",
      " 16%|█▋        | 33/200 [51:18<2:27:02, 52.83s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      " 17%|█▋        | 34/200 [51:18<3:11:24, 69.19s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 17%|█▋        | 34/200 [51:19<3:11:24, 69.19s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 Early stopping, best iteration is:\n",
      "[30]\ttraining's rmse: 0.815442\tvalid_1's rmse: 0.905082\n",
      "\n",
      " 17%|█▋        | 34/200 [51:52<3:11:24, 69.19s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 0.9050822444180289\n",
      "\n",
      " 17%|█▋        | 34/200 [51:52<3:11:24, 69.19s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 {'bagging_frequency': 0.3, 'boosting': 'gbdt', 'colsample_bytree': 0.6, 'feature_fraction': 0.9, 'learning_rate': 0.1, 'max_depth': 8, 'metric': 'rmse', 'min_child_samples': 40, 'num_leaves': 250, 'objective': 'regression', 'reg_alpha': 0.2, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 34/200 [51:52<3:11:24, 69.19s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      " 18%|█▊        | 35/200 [51:52<2:40:54, 58.51s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 18%|█▊        | 35/200 [51:52<2:40:54, 58.51s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 [300]\ttraining's rmse: 1.08857\tvalid_1's rmse: 1.06871\n",
      "\n",
      " 18%|█▊        | 35/200 [52:29<2:40:54, 58.51s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 [600]\ttraining's rmse: 1.02503\tvalid_1's rmse: 1.02729\n",
      "\n",
      " 18%|█▊        | 35/200 [53:05<2:40:54, 58.51s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 [900]\ttraining's rmse: 0.98432\tvalid_1's rmse: 1.00183\n",
      "\n",
      " 18%|█▊        | 35/200 [53:42<2:40:54, 58.51s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 0.974066\tvalid_1's rmse: 0.995475\n",
      "\n",
      " 18%|█▊        | 35/200 [53:55<2:40:54, 58.51s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 0.9954746950223626\n",
      "\n",
      " 18%|█▊        | 35/200 [53:56<2:40:54, 58.51s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 {'bagging_frequency': 0.3, 'boosting': 'gbdt', 'colsample_bytree': 0.6, 'feature_fraction': 1, 'learning_rate': 0.001, 'max_depth': 3, 'metric': 'rmse', 'min_child_samples': 40, 'num_leaves': 250, 'objective': 'regression', 'reg_alpha': 0.2, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n",
      " 18%|█▊        | 35/200 [53:56<2:40:54, 58.51s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      " 18%|█▊        | 36/200 [53:56<3:34:06, 78.33s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 18%|█▊        | 36/200 [53:57<3:34:06, 78.33s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 Early stopping, best iteration is:\n",
      "[48]\ttraining's rmse: 0.826019\tvalid_1's rmse: 0.908838\n",
      "\n",
      " 18%|█▊        | 36/200 [54:39<3:34:06, 78.33s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 0.9088378690010251\n",
      "\n",
      " 18%|█▊        | 36/200 [54:39<3:34:06, 78.33s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 {'bagging_frequency': 0.6, 'boosting': 'gbdt', 'colsample_bytree': 0.5, 'feature_fraction': 0.9, 'learning_rate': 0.05, 'max_depth': 8, 'metric': 'rmse', 'min_child_samples': 20, 'num_leaves': 500, 'objective': 'regression', 'reg_alpha': 0.4, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n",
      " 18%|█▊        | 36/200 [54:39<3:34:06, 78.33s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      " 18%|█▊        | 37/200 [54:39<3:03:43, 67.63s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 18%|█▊        | 37/200 [54:40<3:03:43, 67.63s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 Early stopping, best iteration is:\n",
      "[21]\ttraining's rmse: 0.791641\tvalid_1's rmse: 0.929572\n",
      "\n",
      " 18%|█▊        | 37/200 [55:20<3:03:43, 67.63s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 0.929572245494799\n",
      "\n",
      " 18%|█▊        | 37/200 [55:21<3:03:43, 67.63s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 {'bagging_frequency': 0.9, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.9, 'learning_rate': 0.1, 'max_depth': 12, 'metric': 'rmse', 'min_child_samples': 40, 'num_leaves': 350, 'objective': 'regression', 'reg_alpha': 0.1, 'reg_lambda': 0.5, 'subsample': 1}\n",
      "\n",
      " 18%|█▊        | 37/200 [55:21<3:03:43, 67.63s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      " 19%|█▉        | 38/200 [55:21<2:41:22, 59.77s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 19%|█▉        | 38/200 [55:21<2:41:22, 59.77s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 [300]\ttraining's rmse: 0.854854\tvalid_1's rmse: 0.921116\n",
      "\n",
      " 19%|█▉        | 38/200 [56:27<2:41:22, 59.77s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 Early stopping, best iteration is:\n",
      "[483]\ttraining's rmse: 0.838442\tvalid_1's rmse: 0.917636\n",
      "\n",
      " 19%|█▉        | 38/200 [57:15<2:41:22, 59.77s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 0.917636029089089\n",
      "\n",
      " 19%|█▉        | 38/200 [57:16<2:41:22, 59.77s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 {'bagging_frequency': 0.3, 'boosting': 'gbdt', 'colsample_bytree': 0.6, 'feature_fraction': 0.8, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 40, 'num_leaves': 25, 'objective': 'regression', 'reg_alpha': 0.2, 'reg_lambda': 0.2, 'subsample': 0.8}\n",
      "\n",
      " 19%|█▉        | 38/200 [57:16<2:41:22, 59.77s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      " 20%|█▉        | 39/200 [57:16<3:25:29, 76.58s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 20%|█▉        | 39/200 [57:17<3:25:29, 76.58s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 Early stopping, best iteration is:\n",
      "[34]\ttraining's rmse: 0.817139\tvalid_1's rmse: 0.906796\n",
      "\n",
      " 20%|█▉        | 39/200 [57:44<3:25:29, 76.58s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 0.9067962532332873\n",
      "\n",
      " 20%|█▉        | 39/200 [57:44<3:25:29, 76.58s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 {'bagging_frequency': 0.6, 'boosting': 'gbdt', 'colsample_bytree': 0.9, 'feature_fraction': 0.6, 'learning_rate': 0.1, 'max_depth': 8, 'metric': 'rmse', 'min_child_samples': 40, 'num_leaves': 125, 'objective': 'regression', 'reg_alpha': 0.6, 'reg_lambda': 0.1, 'subsample': 0.9}\n",
      "\n",
      " 20%|█▉        | 39/200 [57:44<3:25:29, 76.58s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      " 20%|██        | 40/200 [57:44<2:45:22, 62.01s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 20%|██        | 40/200 [57:45<2:45:22, 62.01s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 Early stopping, best iteration is:\n",
      "[9]\ttraining's rmse: 0.845758\tvalid_1's rmse: 0.936811\n",
      "\n",
      " 20%|██        | 40/200 [58:03<2:45:22, 62.01s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 0.9368113031055517\n",
      "\n",
      " 20%|██        | 40/200 [58:03<2:45:22, 62.01s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 {'bagging_frequency': 0.8, 'boosting': 'gbdt', 'colsample_bytree': 0.5, 'feature_fraction': 0.9, 'learning_rate': 0.2, 'max_depth': 15, 'metric': 'rmse', 'min_child_samples': 20, 'num_leaves': 75, 'objective': 'regression', 'reg_alpha': 0.3, 'reg_lambda': 0.6, 'subsample': 0.3}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20%|██        | 40/200 [58:03<2:45:22, 62.01s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      " 20%|██        | 41/200 [58:03<2:10:09, 49.11s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 20%|██        | 41/200 [58:04<2:10:09, 49.11s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 [300]\ttraining's rmse: 0.847221\tvalid_1's rmse: 0.925238\n",
      "\n",
      " 20%|██        | 41/200 [58:41<2:10:09, 49.11s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 Early stopping, best iteration is:\n",
      "[288]\ttraining's rmse: 0.847934\tvalid_1's rmse: 0.924094\n",
      "\n",
      " 20%|██        | 41/200 [58:52<2:10:09, 49.11s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 0.9240937388656257\n",
      "\n",
      " 20%|██        | 41/200 [58:52<2:10:09, 49.11s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 {'bagging_frequency': 0.3, 'boosting': 'gbdt', 'colsample_bytree': 0.6, 'feature_fraction': 0.9, 'learning_rate': 0.1, 'max_depth': 3, 'metric': 'rmse', 'min_child_samples': 30, 'num_leaves': 100, 'objective': 'regression', 'reg_alpha': 0.4, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n",
      " 20%|██        | 41/200 [58:52<2:10:09, 49.11s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      " 21%|██        | 42/200 [58:52<2:08:55, 48.96s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 21%|██        | 42/200 [58:52<2:08:55, 48.96s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 [300]\ttraining's rmse: 1.07314\tvalid_1's rmse: 1.05598\n",
      "\n",
      " 21%|██        | 42/200 [59:50<2:08:55, 48.96s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                 [600]\ttraining's rmse: 1.00085\tvalid_1's rmse: 1.00802\n",
      "\n",
      " 21%|██        | 42/200 [1:00:47<2:08:55, 48.96s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                   [900]\ttraining's rmse: 0.95538\tvalid_1's rmse: 0.978967\n",
      "\n",
      " 21%|██        | 42/200 [1:01:43<2:08:55, 48.96s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                   Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 0.944216\tvalid_1's rmse: 0.97202\n",
      "\n",
      " 21%|██        | 42/200 [1:02:02<2:08:55, 48.96s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                   0.9720201322649821\n",
      "\n",
      " 21%|██        | 42/200 [1:02:04<2:08:55, 48.96s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                   {'bagging_frequency': 0.7, 'boosting': 'gbdt', 'colsample_bytree': 0.5, 'feature_fraction': 0.9, 'learning_rate': 0.001, 'max_depth': 5, 'metric': 'rmse', 'min_child_samples': 40, 'num_leaves': 225, 'objective': 'regression', 'reg_alpha': 0.2, 'reg_lambda': 0.6, 'subsample': 0.8}\n",
      "\n",
      " 21%|██        | 42/200 [1:02:04<2:08:55, 48.96s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      " 22%|██▏       | 43/200 [1:02:04<4:00:35, 91.95s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                   Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 22%|██▏       | 43/200 [1:02:05<4:00:35, 91.95s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                   [300]\ttraining's rmse: 0.814896\tvalid_1's rmse: 0.907801\n",
      "\n",
      " 22%|██▏       | 43/200 [1:03:42<4:00:35, 91.95s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                   Early stopping, best iteration is:\n",
      "[322]\ttraining's rmse: 0.811494\tvalid_1's rmse: 0.907659\n",
      "\n",
      " 22%|██▏       | 43/200 [1:04:16<4:00:35, 91.95s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                   0.9076592650994826\n",
      "\n",
      " 22%|██▏       | 43/200 [1:04:17<4:00:35, 91.95s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                   {'bagging_frequency': 0.3, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.8, 'learning_rate': 0.01, 'max_depth': 8, 'metric': 'rmse', 'min_child_samples': 40, 'num_leaves': 250, 'objective': 'regression', 'reg_alpha': 0.2, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "\n",
      " 22%|██▏       | 43/200 [1:04:17<4:00:35, 91.95s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      " 22%|██▏       | 44/200 [1:04:17<4:31:15, 104.33s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 22%|██▏       | 44/200 [1:04:18<4:31:15, 104.33s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                    Early stopping, best iteration is:\n",
      "[55]\ttraining's rmse: 0.812276\tvalid_1's rmse: 0.913878\n",
      "\n",
      " 22%|██▏       | 44/200 [1:04:59<4:31:15, 104.33s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                    0.9138777240162748\n",
      "\n",
      " 22%|██▏       | 44/200 [1:04:59<4:31:15, 104.33s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.9, 'boosting': 'gbdt', 'colsample_bytree': 0.9, 'feature_fraction': 0.6, 'learning_rate': 0.05, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 30, 'num_leaves': 150, 'objective': 'regression', 'reg_alpha': 0.6, 'reg_lambda': 0.1, 'subsample': 0.3}\n",
      "\n",
      " 22%|██▏       | 44/200 [1:04:59<4:31:15, 104.33s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      " 22%|██▎       | 45/200 [1:04:59<3:41:10, 85.61s/trial, best loss: 0.9048186227889994] \u001b[A\n",
      "\u001b[A                                                                                   Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 22%|██▎       | 45/200 [1:05:00<3:41:10, 85.61s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                   Early stopping, best iteration is:\n",
      "[36]\ttraining's rmse: 0.834189\tvalid_1's rmse: 0.914742\n",
      "\n",
      " 22%|██▎       | 45/200 [1:05:26<3:41:10, 85.61s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                   0.9147424255158486\n",
      "\n",
      " 22%|██▎       | 45/200 [1:05:27<3:41:10, 85.61s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                   {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.6, 'feature_fraction': 1, 'learning_rate': 0.1, 'max_depth': 8, 'metric': 'rmse', 'min_child_samples': 40, 'num_leaves': 50, 'objective': 'regression', 'reg_alpha': 0.1, 'reg_lambda': 0.2, 'subsample': 0.8}\n",
      "\n",
      " 22%|██▎       | 45/200 [1:05:27<3:41:10, 85.61s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      " 23%|██▎       | 46/200 [1:05:27<2:54:42, 68.07s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                   Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 23%|██▎       | 46/200 [1:05:27<2:54:42, 68.07s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                   [300]\ttraining's rmse: 0.755903\tvalid_1's rmse: 0.941976\n",
      "\n",
      " 23%|██▎       | 46/200 [1:07:48<2:54:42, 68.07s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                   Early stopping, best iteration is:\n",
      "[284]\ttraining's rmse: 0.759983\tvalid_1's rmse: 0.941688\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 46/200 [1:08:18<2:54:42, 68.07s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                   0.9416881942406787\n",
      "\n",
      " 23%|██▎       | 46/200 [1:08:19<2:54:42, 68.07s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                   {'bagging_frequency': 0.6, 'boosting': 'gbdt', 'colsample_bytree': 0.5, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 15, 'metric': 'rmse', 'min_child_samples': 40, 'num_leaves': 350, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.6, 'subsample': 1}\n",
      "\n",
      " 23%|██▎       | 46/200 [1:08:19<2:54:42, 68.07s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      " 24%|██▎       | 47/200 [1:08:19<4:13:49, 99.54s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                   Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 24%|██▎       | 47/200 [1:08:20<4:13:49, 99.54s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                   [300]\ttraining's rmse: 1.05011\tvalid_1's rmse: 1.03651\n",
      "\n",
      " 24%|██▎       | 47/200 [1:10:50<4:13:49, 99.54s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                   [600]\ttraining's rmse: 0.961978\tvalid_1's rmse: 0.978823\n",
      "\n",
      " 24%|██▎       | 47/200 [1:13:14<4:13:49, 99.54s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                   [900]\ttraining's rmse: 0.905972\tvalid_1's rmse: 0.94553\n",
      "\n",
      " 24%|██▎       | 47/200 [1:15:31<4:13:49, 99.54s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                   Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 0.892099\tvalid_1's rmse: 0.938016\n",
      "\n",
      " 24%|██▎       | 47/200 [1:16:18<4:13:49, 99.54s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                   0.9380162937072687\n",
      "\n",
      " 24%|██▎       | 47/200 [1:16:23<4:13:49, 99.54s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                   {'bagging_frequency': 0.8, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.9, 'learning_rate': 0.001, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 20, 'num_leaves': 250, 'objective': 'regression', 'reg_alpha': 0.4, 'reg_lambda': 0.4, 'subsample': 0.5}\n",
      "\n",
      " 24%|██▎       | 47/200 [1:16:23<4:13:49, 99.54s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      " 24%|██▍       | 48/200 [1:16:23<9:03:54, 214.70s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 24%|██▍       | 48/200 [1:16:23<9:03:54, 214.70s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                    [300]\ttraining's rmse: 0.847069\tvalid_1's rmse: 0.926884\n",
      "\n",
      " 24%|██▍       | 48/200 [1:17:04<9:03:54, 214.70s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                    [600]\ttraining's rmse: 0.831422\tvalid_1's rmse: 0.922246\n",
      "\n",
      " 24%|██▍       | 48/200 [1:17:43<9:03:54, 214.70s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                    Early stopping, best iteration is:\n",
      "[552]\ttraining's rmse: 0.833184\tvalid_1's rmse: 0.920757\n",
      "\n",
      " 24%|██▍       | 48/200 [1:17:50<9:03:54, 214.70s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                    0.9207574394551957\n",
      "\n",
      " 24%|██▍       | 48/200 [1:17:51<9:03:54, 214.70s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.7, 'boosting': 'gbdt', 'colsample_bytree': 0.6, 'feature_fraction': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'metric': 'rmse', 'min_child_samples': 40, 'num_leaves': 500, 'objective': 'regression', 'reg_alpha': 0.3, 'reg_lambda': 0.6, 'subsample': 0.8}\n",
      "\n",
      " 24%|██▍       | 48/200 [1:17:51<9:03:54, 214.70s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      " 24%|██▍       | 49/200 [1:17:51<7:24:37, 176.67s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 24%|██▍       | 49/200 [1:17:51<7:24:37, 176.67s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                    [300]\ttraining's rmse: 0.820453\tvalid_1's rmse: 0.917918\n",
      "\n",
      " 24%|██▍       | 49/200 [1:18:40<7:24:37, 176.67s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                    Early stopping, best iteration is:\n",
      "[240]\ttraining's rmse: 0.826231\tvalid_1's rmse: 0.915988\n",
      "\n",
      " 24%|██▍       | 49/200 [1:18:46<7:24:37, 176.67s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                    0.9159884338781967\n",
      "\n",
      " 24%|██▍       | 49/200 [1:18:46<7:24:37, 176.67s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.9, 'feature_fraction': 0.6, 'learning_rate': 0.05, 'max_depth': 5, 'metric': 'rmse', 'min_child_samples': 30, 'num_leaves': 125, 'objective': 'regression', 'reg_alpha': 0.2, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "\n",
      " 24%|██▍       | 49/200 [1:18:46<7:24:37, 176.67s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      " 25%|██▌       | 50/200 [1:18:46<5:50:36, 140.24s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 25%|██▌       | 50/200 [1:18:46<5:50:36, 140.24s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                    [300]\ttraining's rmse: 0.831715\tvalid_1's rmse: 0.913366\n",
      "\n",
      " 25%|██▌       | 50/200 [1:20:22<5:50:36, 140.24s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                    Early stopping, best iteration is:\n",
      "[327]\ttraining's rmse: 0.828007\tvalid_1's rmse: 0.912977\n",
      "\n",
      " 25%|██▌       | 50/200 [1:20:52<5:50:36, 140.24s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                    0.9129773372451013\n",
      "\n",
      " 25%|██▌       | 50/200 [1:20:53<5:50:36, 140.24s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.3, 'boosting': 'gbdt', 'colsample_bytree': 0.5, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 8, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 75, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.1, 'subsample': 1}\n",
      "\n",
      " 25%|██▌       | 50/200 [1:20:53<5:50:36, 140.24s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      " 26%|██▌       | 51/200 [1:20:53<5:38:07, 136.15s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 26%|██▌       | 51/200 [1:20:53<5:38:07, 136.15s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                    Early stopping, best iteration is:\n",
      "[37]\ttraining's rmse: 0.828615\tvalid_1's rmse: 0.917396\n",
      "\n",
      " 26%|██▌       | 51/200 [1:21:15<5:38:07, 136.15s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                    0.9173960527697707\n",
      "\n",
      " 26%|██▌       | 51/200 [1:21:15<5:38:07, 136.15s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.8, 'feature_fraction': 0.7, 'learning_rate': 0.2, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 40, 'num_leaves': 25, 'objective': 'regression', 'reg_alpha': 0.6, 'reg_lambda': 0.3, 'subsample': 0.3}\n",
      "\n",
      " 26%|██▌       | 51/200 [1:21:15<5:38:07, 136.15s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      " 26%|██▌       | 52/200 [1:21:15<4:11:22, 101.91s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 26%|██▌       | 52/200 [1:21:15<4:11:22, 101.91s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                    Early stopping, best iteration is:\n",
      "[37]\ttraining's rmse: 0.820436\tvalid_1's rmse: 0.928013\n",
      "\n",
      " 26%|██▌       | 52/200 [1:21:44<4:11:22, 101.91s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                    0.9280132735532087\n",
      "\n",
      " 26%|██▌       | 52/200 [1:21:44<4:11:22, 101.91s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.3, 'boosting': 'gbdt', 'colsample_bytree': 1, 'feature_fraction': 0.9, 'learning_rate': 0.1, 'max_depth': 12, 'metric': 'rmse', 'min_child_samples': 40, 'num_leaves': 50, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.2, 'subsample': 0.7}\n",
      "\n",
      " 26%|██▌       | 52/200 [1:21:44<4:11:22, 101.91s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      " 26%|██▋       | 53/200 [1:21:44<3:16:28, 80.19s/trial, best loss: 0.9048186227889994] \u001b[A\n",
      "\u001b[A                                                                                   Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 26%|██▋       | 53/200 [1:21:45<3:16:28, 80.19s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                   [300]\ttraining's rmse: 0.814109\tvalid_1's rmse: 0.912958\n",
      "\n",
      " 26%|██▋       | 53/200 [1:23:38<3:16:28, 80.19s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                   Early stopping, best iteration is:\n",
      "[313]\ttraining's rmse: 0.811905\tvalid_1's rmse: 0.912767\n",
      "\n",
      " 26%|██▋       | 53/200 [1:24:13<3:16:28, 80.19s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                   0.9127669960684448\n",
      "\n",
      " 26%|██▋       | 53/200 [1:24:14<3:16:28, 80.19s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                   {'bagging_frequency': 0.6, 'boosting': 'gbdt', 'colsample_bytree': 0.5, 'feature_fraction': 1, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 100, 'objective': 'regression', 'reg_alpha': 0.4, 'reg_lambda': 0.4, 'subsample': 0.5}\n",
      "\n",
      " 26%|██▋       | 53/200 [1:24:14<3:16:28, 80.19s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      " 27%|██▋       | 54/200 [1:24:14<4:05:44, 100.99s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 27%|██▋       | 54/200 [1:24:14<4:05:44, 100.99s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                    [300]\ttraining's rmse: 1.05596\tvalid_1's rmse: 1.03916\n",
      "\n",
      " 27%|██▋       | 54/200 [1:26:30<4:05:44, 100.99s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                    [600]\ttraining's rmse: 0.972065\tvalid_1's rmse: 0.981724\n",
      "\n",
      " 27%|██▋       | 54/200 [1:28:18<4:05:44, 100.99s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                    [900]\ttraining's rmse: 0.9192\tvalid_1's rmse: 0.948711\n",
      "\n",
      " 27%|██▋       | 54/200 [1:30:02<4:05:44, 100.99s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                    Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 0.906295\tvalid_1's rmse: 0.941261\n",
      "\n",
      " 27%|██▋       | 54/200 [1:30:35<4:05:44, 100.99s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                    0.9412609551030137\n",
      "\n",
      " 27%|██▋       | 54/200 [1:30:39<4:05:44, 100.99s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.5, 'boosting': 'gbdt', 'colsample_bytree': 0.6, 'feature_fraction': 0.9, 'learning_rate': 0.001, 'max_depth': 8, 'metric': 'rmse', 'min_child_samples': 40, 'num_leaves': 350, 'objective': 'regression', 'reg_alpha': 0.7, 'reg_lambda': 0.6, 'subsample': 0.8}\n",
      "\n",
      " 27%|██▋       | 54/200 [1:30:39<4:05:44, 100.99s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      " 28%|██▊       | 55/200 [1:30:39<7:30:06, 186.25s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 28%|██▊       | 55/200 [1:30:39<7:30:06, 186.25s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                    Early stopping, best iteration is:\n",
      "[15]\ttraining's rmse: 0.812717\tvalid_1's rmse: 0.928844\n",
      "\n",
      " 28%|██▊       | 55/200 [1:31:14<7:30:06, 186.25s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                    0.9288440684063206\n",
      "\n",
      " 28%|██▊       | 55/200 [1:31:14<7:30:06, 186.25s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.8, 'boosting': 'gbdt', 'colsample_bytree': 1, 'feature_fraction': 0.7, 'learning_rate': 0.1, 'max_depth': 15, 'metric': 'rmse', 'min_child_samples': 20, 'num_leaves': 400, 'objective': 'regression', 'reg_alpha': 0.3, 'reg_lambda': 0.3, 'subsample': 0.7}\n",
      "\n",
      " 28%|██▊       | 55/200 [1:31:14<7:30:06, 186.25s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      " 28%|██▊       | 56/200 [1:31:14<5:38:02, 140.85s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 28%|██▊       | 56/200 [1:31:14<5:38:02, 140.85s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                    [300]\ttraining's rmse: 0.869752\tvalid_1's rmse: 0.932861\n",
      "\n",
      " 28%|██▊       | 56/200 [1:32:06<5:38:02, 140.85s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                    [600]\ttraining's rmse: 0.850053\tvalid_1's rmse: 0.927605\n",
      "\n",
      " 28%|██▊       | 56/200 [1:32:55<5:38:02, 140.85s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                    [900]\ttraining's rmse: 0.835879\tvalid_1's rmse: 0.919975\n",
      "\n",
      " 28%|██▊       | 56/200 [1:33:44<5:38:02, 140.85s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                    Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 0.832061\tvalid_1's rmse: 0.918125\n",
      "\n",
      " 28%|██▊       | 56/200 [1:33:59<5:38:02, 140.85s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                    0.9181250494872739\n",
      "\n",
      " 28%|██▊       | 56/200 [1:34:01<5:38:02, 140.85s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.5, 'feature_fraction': 0.8, 'learning_rate': 0.01, 'max_depth': 5, 'metric': 'rmse', 'min_child_samples': 30, 'num_leaves': 225, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.6, 'subsample': 0.8}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 56/200 [1:34:01<5:38:02, 140.85s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      " 28%|██▊       | 57/200 [1:34:01<5:54:35, 148.78s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 28%|██▊       | 57/200 [1:34:01<5:54:35, 148.78s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                    [300]\ttraining's rmse: 0.864966\tvalid_1's rmse: 0.932579\n",
      "\n",
      " 28%|██▊       | 57/200 [1:34:32<5:54:35, 148.78s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                    [600]\ttraining's rmse: 0.849939\tvalid_1's rmse: 0.92691\n",
      "\n",
      " 28%|██▊       | 57/200 [1:35:02<5:54:35, 148.78s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                    [900]\ttraining's rmse: 0.841451\tvalid_1's rmse: 0.926025\n",
      "\n",
      " 28%|██▊       | 57/200 [1:35:32<5:54:35, 148.78s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                    Early stopping, best iteration is:\n",
      "[809]\ttraining's rmse: 0.843656\tvalid_1's rmse: 0.924477\n",
      "\n",
      " 28%|██▊       | 57/200 [1:35:33<5:54:35, 148.78s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                    0.9244766547788519\n",
      "\n",
      " 28%|██▊       | 57/200 [1:35:34<5:54:35, 148.78s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.7, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.6, 'learning_rate': 0.05, 'max_depth': 3, 'metric': 'rmse', 'min_child_samples': 40, 'num_leaves': 250, 'objective': 'regression', 'reg_alpha': 0.2, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n",
      " 28%|██▊       | 57/200 [1:35:34<5:54:35, 148.78s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      " 29%|██▉       | 58/200 [1:35:34<5:12:12, 131.92s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 29%|██▉       | 58/200 [1:35:34<5:12:12, 131.92s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                    [300]\ttraining's rmse: 0.78029\tvalid_1's rmse: 0.905099\n",
      "\n",
      " 29%|██▉       | 58/200 [1:37:34<5:12:12, 131.92s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                    Early stopping, best iteration is:\n",
      "[314]\ttraining's rmse: 0.777314\tvalid_1's rmse: 0.904705\n",
      "\n",
      " 29%|██▉       | 58/200 [1:38:12<5:12:12, 131.92s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                    0.9047051993416964\n",
      "\n",
      " 29%|██▉       | 58/200 [1:38:14<5:12:12, 131.92s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.9, 'boosting': 'gbdt', 'colsample_bytree': 0.8, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 350, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "\n",
      " 29%|██▉       | 58/200 [1:38:14<5:12:12, 131.92s/trial, best loss: 0.9048186227889994]\u001b[A\n",
      " 30%|██▉       | 59/200 [1:38:14<5:29:41, 140.29s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 30%|██▉       | 59/200 [1:38:14<5:29:41, 140.29s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[A                                                                                    [300]\ttraining's rmse: 0.78029\tvalid_1's rmse: 0.905099\n",
      "\n",
      " 30%|██▉       | 59/200 [1:40:14<5:29:41, 140.29s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[A                                                                                    Early stopping, best iteration is:\n",
      "[314]\ttraining's rmse: 0.777314\tvalid_1's rmse: 0.904705\n",
      "\n",
      " 30%|██▉       | 59/200 [1:40:52<5:29:41, 140.29s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[A                                                                                    0.9047051993416964\n",
      "\n",
      " 30%|██▉       | 59/200 [1:40:53<5:29:41, 140.29s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.9, 'boosting': 'gbdt', 'colsample_bytree': 0.8, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 350, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "\n",
      " 30%|██▉       | 59/200 [1:40:53<5:29:41, 140.29s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      " 30%|███       | 60/200 [1:40:53<5:40:52, 146.09s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 30%|███       | 60/200 [1:40:54<5:40:52, 146.09s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[A                                                                                    [300]\ttraining's rmse: 0.779911\tvalid_1's rmse: 0.911772\n",
      "\n",
      " 30%|███       | 60/200 [1:42:39<5:40:52, 146.09s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[A                                                                                    Early stopping, best iteration is:\n",
      "[257]\ttraining's rmse: 0.790763\tvalid_1's rmse: 0.910989\n",
      "\n",
      " 30%|███       | 60/200 [1:42:56<5:40:52, 146.09s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[A                                                                                    0.9109889407807243\n",
      "\n",
      " 30%|███       | 60/200 [1:42:57<5:40:52, 146.09s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.9, 'boosting': 'gbdt', 'colsample_bytree': 0.8, 'feature_fraction': 0.7, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 350, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "\n",
      " 30%|███       | 60/200 [1:42:57<5:40:52, 146.09s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      " 30%|███       | 61/200 [1:42:57<5:22:56, 139.40s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 30%|███       | 61/200 [1:42:57<5:22:56, 139.40s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[A                                                                                    [300]\ttraining's rmse: 0.78029\tvalid_1's rmse: 0.905099\n",
      "\n",
      " 30%|███       | 61/200 [1:44:57<5:22:56, 139.40s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[A                                                                                    Early stopping, best iteration is:\n",
      "[314]\ttraining's rmse: 0.777314\tvalid_1's rmse: 0.904705\n",
      "\n",
      " 30%|███       | 61/200 [1:45:36<5:22:56, 139.40s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[A                                                                                    0.9047051993416964\n",
      "\n",
      " 30%|███       | 61/200 [1:45:37<5:22:56, 139.40s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.9, 'boosting': 'gbdt', 'colsample_bytree': 0.8, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 350, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "\n",
      " 30%|███       | 61/200 [1:45:37<5:22:56, 139.40s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      " 31%|███       | 62/200 [1:45:37<5:35:06, 145.70s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[ATraining until validation scores don't improve for 100 rounds                       \n",
      "\n",
      " 31%|███       | 62/200 [1:45:38<5:35:06, 145.70s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[A                                                                                    [300]\ttraining's rmse: 0.780265\tvalid_1's rmse: 0.907113\n",
      "\n",
      " 31%|███       | 62/200 [1:47:41<5:35:06, 145.70s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[A                                                                                    Early stopping, best iteration is:\n",
      "[260]\ttraining's rmse: 0.79003\tvalid_1's rmse: 0.905903\n",
      "\n",
      " 31%|███       | 62/200 [1:48:03<5:35:06, 145.70s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[A                                                                                    0.9059030257708\n",
      "\n",
      " 31%|███       | 62/200 [1:48:04<5:35:06, 145.70s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.9, 'boosting': 'gbdt', 'colsample_bytree': 0.8, 'feature_fraction': 1, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 350, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "\n",
      " 31%|███       | 62/200 [1:48:04<5:35:06, 145.70s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      " 32%|███▏      | 63/200 [1:48:04<5:33:10, 145.92s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 32%|███▏      | 63/200 [1:48:04<5:33:10, 145.92s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[A                                                                                    [300]\ttraining's rmse: 0.78029\tvalid_1's rmse: 0.905099\n",
      "\n",
      " 32%|███▏      | 63/200 [1:50:04<5:33:10, 145.92s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[A                                                                                    Early stopping, best iteration is:\n",
      "[314]\ttraining's rmse: 0.777314\tvalid_1's rmse: 0.904705\n",
      "\n",
      " 32%|███▏      | 63/200 [1:50:43<5:33:10, 145.92s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[A                                                                                    0.9047051993416964\n",
      "\n",
      " 32%|███▏      | 63/200 [1:50:44<5:33:10, 145.92s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.9, 'boosting': 'gbdt', 'colsample_bytree': 0.8, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 350, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "\n",
      " 32%|███▏      | 63/200 [1:50:44<5:33:10, 145.92s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      " 32%|███▏      | 64/200 [1:50:44<5:40:28, 150.21s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 32%|███▏      | 64/200 [1:50:44<5:40:28, 150.21s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[A                                                                                    [300]\ttraining's rmse: 0.77046\tvalid_1's rmse: 0.908664\n",
      "\n",
      " 32%|███▏      | 64/200 [1:52:48<5:40:28, 150.21s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[A                                                                                    Early stopping, best iteration is:\n",
      "[257]\ttraining's rmse: 0.781507\tvalid_1's rmse: 0.9073\n",
      "\n",
      " 32%|███▏      | 64/200 [1:53:09<5:40:28, 150.21s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[A                                                                                    0.9072996429849067\n",
      "\n",
      " 32%|███▏      | 64/200 [1:53:10<5:40:28, 150.21s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.9, 'boosting': 'gbdt', 'colsample_bytree': 0.8, 'feature_fraction': 0.8, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 500, 'objective': 'regression', 'reg_alpha': 0.1, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "\n",
      " 32%|███▏      | 64/200 [1:53:10<5:40:28, 150.21s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      " 32%|███▎      | 65/200 [1:53:10<5:35:16, 149.01s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 32%|███▎      | 65/200 [1:53:11<5:35:16, 149.01s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[A                                                                                    [300]\ttraining's rmse: 0.78029\tvalid_1's rmse: 0.905099\n",
      "\n",
      " 32%|███▎      | 65/200 [1:55:05<5:35:16, 149.01s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[A                                                                                    Early stopping, best iteration is:\n",
      "[314]\ttraining's rmse: 0.777314\tvalid_1's rmse: 0.904705\n",
      "\n",
      " 32%|███▎      | 65/200 [1:55:42<5:35:16, 149.01s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[A                                                                                    0.9047051993416964\n",
      "\n",
      " 32%|███▎      | 65/200 [1:55:43<5:35:16, 149.01s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.9, 'boosting': 'gbdt', 'colsample_bytree': 0.8, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 350, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "\n",
      " 32%|███▎      | 65/200 [1:55:43<5:35:16, 149.01s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      " 33%|███▎      | 66/200 [1:55:43<5:35:30, 150.23s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 33%|███▎      | 66/200 [1:55:44<5:35:30, 150.23s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[A                                                                                    [300]\ttraining's rmse: 0.78029\tvalid_1's rmse: 0.905099\n",
      "\n",
      " 33%|███▎      | 66/200 [1:57:38<5:35:30, 150.23s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[A                                                                                    Early stopping, best iteration is:\n",
      "[314]\ttraining's rmse: 0.777314\tvalid_1's rmse: 0.904705\n",
      "\n",
      " 33%|███▎      | 66/200 [1:58:15<5:35:30, 150.23s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[A                                                                                    0.9047051993416964\n",
      "\n",
      " 33%|███▎      | 66/200 [1:58:17<5:35:30, 150.23s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.9, 'boosting': 'gbdt', 'colsample_bytree': 0.8, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 350, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "\n",
      " 33%|███▎      | 66/200 [1:58:17<5:35:30, 150.23s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      " 34%|███▎      | 67/200 [1:58:17<5:35:07, 151.18s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 34%|███▎      | 67/200 [1:58:17<5:35:07, 151.18s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[A                                                                                    [300]\ttraining's rmse: 0.78029\tvalid_1's rmse: 0.905099\n",
      "\n",
      " 34%|███▎      | 67/200 [2:00:14<5:35:07, 151.18s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[A                                                                                    Early stopping, best iteration is:\n",
      "[314]\ttraining's rmse: 0.777314\tvalid_1's rmse: 0.904705\n",
      "\n",
      " 34%|███▎      | 67/200 [2:00:51<5:35:07, 151.18s/trial, best loss: 0.9047051993416964]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[A0.9047051993416964                                                                  \n",
      "\n",
      " 34%|███▎      | 67/200 [2:00:53<5:35:07, 151.18s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.9, 'boosting': 'gbdt', 'colsample_bytree': 0.8, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 350, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "\n",
      " 34%|███▎      | 67/200 [2:00:53<5:35:07, 151.18s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      " 34%|███▍      | 68/200 [2:00:53<5:35:49, 152.64s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 34%|███▍      | 68/200 [2:00:53<5:35:49, 152.64s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[A                                                                                    [300]\ttraining's rmse: 0.78029\tvalid_1's rmse: 0.905099\n",
      "\n",
      " 34%|███▍      | 68/200 [2:02:50<5:35:49, 152.64s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[A                                                                                    Early stopping, best iteration is:\n",
      "[314]\ttraining's rmse: 0.777314\tvalid_1's rmse: 0.904705\n",
      "\n",
      " 34%|███▍      | 68/200 [2:03:27<5:35:49, 152.64s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[A                                                                                    0.9047051993416964\n",
      "\n",
      " 34%|███▍      | 68/200 [2:03:28<5:35:49, 152.64s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.9, 'boosting': 'gbdt', 'colsample_bytree': 0.8, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 350, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "\n",
      " 34%|███▍      | 68/200 [2:03:28<5:35:49, 152.64s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      " 34%|███▍      | 69/200 [2:03:28<5:35:02, 153.45s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 34%|███▍      | 69/200 [2:03:29<5:35:02, 153.45s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[A                                                                                    [300]\ttraining's rmse: 0.78029\tvalid_1's rmse: 0.905099\n",
      "\n",
      " 34%|███▍      | 69/200 [2:05:24<5:35:02, 153.45s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[A                                                                                    Early stopping, best iteration is:\n",
      "[314]\ttraining's rmse: 0.777314\tvalid_1's rmse: 0.904705\n",
      "\n",
      " 34%|███▍      | 69/200 [2:06:01<5:35:02, 153.45s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[A                                                                                    0.9047051993416964\n",
      "\n",
      " 34%|███▍      | 69/200 [2:06:02<5:35:02, 153.45s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.9, 'boosting': 'gbdt', 'colsample_bytree': 0.8, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 350, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "\n",
      " 34%|███▍      | 69/200 [2:06:02<5:35:02, 153.45s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      " 35%|███▌      | 70/200 [2:06:02<5:33:02, 153.71s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 35%|███▌      | 70/200 [2:06:03<5:33:02, 153.71s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[A                                                                                    [300]\ttraining's rmse: 0.777069\tvalid_1's rmse: 0.905572\n",
      "\n",
      " 35%|███▌      | 70/200 [2:08:02<5:33:02, 153.71s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[A                                                                                    Early stopping, best iteration is:\n",
      "[270]\ttraining's rmse: 0.784137\tvalid_1's rmse: 0.904634\n",
      "\n",
      " 35%|███▌      | 70/200 [2:08:26<5:33:02, 153.71s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[A                                                                                    0.9046337956631954\n",
      "\n",
      " 35%|███▌      | 70/200 [2:08:28<5:33:02, 153.71s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.9, 'boosting': 'gbdt', 'colsample_bytree': 0.8, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 400, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "\n",
      " 35%|███▌      | 70/200 [2:08:28<5:33:02, 153.71s/trial, best loss: 0.9047051993416964]\u001b[A\n",
      " 36%|███▌      | 71/200 [2:08:28<5:24:58, 151.15s/trial, best loss: 0.9046337956631954]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 36%|███▌      | 71/200 [2:08:28<5:24:58, 151.15s/trial, best loss: 0.9046337956631954]\u001b[A\n",
      "\u001b[A                                                                                    [300]\ttraining's rmse: 0.777123\tvalid_1's rmse: 0.904133\n",
      "\n",
      " 36%|███▌      | 71/200 [2:10:28<5:24:58, 151.15s/trial, best loss: 0.9046337956631954]\u001b[A\n",
      "\u001b[A                                                                                    Early stopping, best iteration is:\n",
      "[267]\ttraining's rmse: 0.784919\tvalid_1's rmse: 0.903904\n",
      "\n",
      " 36%|███▌      | 71/200 [2:10:50<5:24:58, 151.15s/trial, best loss: 0.9046337956631954]\u001b[A\n",
      "\u001b[A                                                                                    0.9039042903249872\n",
      "\n",
      " 36%|███▌      | 71/200 [2:10:52<5:24:58, 151.15s/trial, best loss: 0.9046337956631954]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.9, 'boosting': 'gbdt', 'colsample_bytree': 0.8, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 400, 'objective': 'regression', 'reg_alpha': 0.7, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "\n",
      " 36%|███▌      | 71/200 [2:10:52<5:24:58, 151.15s/trial, best loss: 0.9046337956631954]\u001b[A\n",
      " 36%|███▌      | 72/200 [2:10:52<5:17:57, 149.04s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 36%|███▌      | 72/200 [2:10:52<5:17:57, 149.04s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Early stopping, best iteration is:\n",
      "[8]\ttraining's rmse: 0.81217\tvalid_1's rmse: 0.916601\n",
      "\n",
      " 36%|███▌      | 72/200 [2:11:24<5:17:57, 149.04s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    0.9166011785139092\n",
      "\n",
      " 36%|███▌      | 72/200 [2:11:24<5:17:57, 149.04s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.9, 'boosting': 'gbdt', 'colsample_bytree': 0.8, 'feature_fraction': 0.9, 'learning_rate': 0.2, 'max_depth': 12, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 400, 'objective': 'regression', 'reg_alpha': 0.7, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "\n",
      " 36%|███▌      | 72/200 [2:11:24<5:17:57, 149.04s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 36%|███▋      | 73/200 [2:11:24<4:01:19, 114.02s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 36%|███▋      | 73/200 [2:11:24<4:01:19, 114.02s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    [300]\ttraining's rmse: 0.779434\tvalid_1's rmse: 0.909145\n",
      "\n",
      " 36%|███▋      | 73/200 [2:13:05<4:01:19, 114.02s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Early stopping, best iteration is:\n",
      "[245]\ttraining's rmse: 0.794602\tvalid_1's rmse: 0.907886\n",
      "\n",
      " 36%|███▋      | 73/200 [2:13:18<4:01:19, 114.02s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    0.9078856547642731\n",
      "\n",
      " 36%|███▋      | 73/200 [2:13:19<4:01:19, 114.02s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.9, 'boosting': 'gbdt', 'colsample_bytree': 0.8, 'feature_fraction': 0.6, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 400, 'objective': 'regression', 'reg_alpha': 0.7, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "\n",
      " 36%|███▋      | 73/200 [2:13:19<4:01:19, 114.02s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 37%|███▋      | 74/200 [2:13:19<4:00:03, 114.31s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 37%|███▋      | 74/200 [2:13:19<4:00:03, 114.31s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Early stopping, best iteration is:\n",
      "[177]\ttraining's rmse: 0.793683\tvalid_1's rmse: 0.941863\n",
      "\n",
      " 37%|███▋      | 74/200 [2:15:06<4:00:03, 114.31s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    0.9418634038879472\n",
      "\n",
      " 37%|███▋      | 74/200 [2:15:07<4:00:03, 114.31s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.5, 'boosting': 'gbdt', 'colsample_bytree': 0.8, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 15, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 400, 'objective': 'regression', 'reg_alpha': 0.7, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "\n",
      " 37%|███▋      | 74/200 [2:15:07<4:00:03, 114.31s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 38%|███▊      | 75/200 [2:15:07<3:54:20, 112.49s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 38%|███▊      | 75/200 [2:15:08<3:54:20, 112.49s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    [300]\ttraining's rmse: 1.05054\tvalid_1's rmse: 1.03699\n",
      "\n",
      " 38%|███▊      | 75/200 [2:17:09<3:54:20, 112.49s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    [600]\ttraining's rmse: 0.960568\tvalid_1's rmse: 0.977991\n",
      "\n",
      " 38%|███▊      | 75/200 [2:19:11<3:54:20, 112.49s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    [900]\ttraining's rmse: 0.90175\tvalid_1's rmse: 0.944191\n",
      "\n",
      " 38%|███▊      | 75/200 [2:21:12<3:54:20, 112.49s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 0.887008\tvalid_1's rmse: 0.936849\n",
      "\n",
      " 38%|███▊      | 75/200 [2:21:52<3:54:20, 112.49s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    0.9368494936692726\n",
      "\n",
      " 38%|███▊      | 75/200 [2:21:58<3:54:20, 112.49s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.9, 'boosting': 'gbdt', 'colsample_bytree': 0.8, 'feature_fraction': 0.7, 'learning_rate': 0.001, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 400, 'objective': 'regression', 'reg_alpha': 0.7, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "\n",
      " 38%|███▊      | 75/200 [2:21:58<3:54:20, 112.49s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 38%|███▊      | 76/200 [2:21:58<6:57:36, 202.07s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 38%|███▊      | 76/200 [2:21:59<6:57:36, 202.07s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    [300]\ttraining's rmse: 0.872362\tvalid_1's rmse: 0.935756\n",
      "\n",
      " 38%|███▊      | 76/200 [2:22:55<6:57:36, 202.07s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    [600]\ttraining's rmse: 0.851309\tvalid_1's rmse: 0.929837\n",
      "\n",
      " 38%|███▊      | 76/200 [2:23:45<6:57:36, 202.07s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    [900]\ttraining's rmse: 0.836182\tvalid_1's rmse: 0.921242\n",
      "\n",
      " 38%|███▊      | 76/200 [2:24:35<6:57:36, 202.07s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 0.832678\tvalid_1's rmse: 0.919829\n",
      "\n",
      " 38%|███▊      | 76/200 [2:24:51<6:57:36, 202.07s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    0.9198286551815901\n",
      "\n",
      " 38%|███▊      | 76/200 [2:24:53<6:57:36, 202.07s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.9, 'boosting': 'gbdt', 'colsample_bytree': 0.9, 'feature_fraction': 1, 'learning_rate': 0.01, 'max_depth': 5, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 400, 'objective': 'regression', 'reg_alpha': 0.7, 'reg_lambda': 0.4, 'subsample': 0.5}\n",
      "\n",
      " 38%|███▊      | 76/200 [2:24:53<6:57:36, 202.07s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 38%|███▊      | 77/200 [2:24:53<6:37:10, 193.75s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 38%|███▊      | 77/200 [2:24:53<6:37:10, 193.75s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    [300]\ttraining's rmse: 0.860917\tvalid_1's rmse: 0.928566\n",
      "\n",
      " 38%|███▊      | 77/200 [2:25:30<6:37:10, 193.75s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    [600]\ttraining's rmse: 0.846977\tvalid_1's rmse: 0.923356\n",
      "\n",
      " 38%|███▊      | 77/200 [2:26:05<6:37:10, 193.75s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Early stopping, best iteration is:\n",
      "[596]\ttraining's rmse: 0.847092\tvalid_1's rmse: 0.92326\n",
      "\n",
      " 38%|███▊      | 77/200 [2:26:16<6:37:10, 193.75s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    0.9232595435850689\n",
      "\n",
      " 38%|███▊      | 77/200 [2:26:17<6:37:10, 193.75s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.9, 'boosting': 'gbdt', 'colsample_bytree': 0.8, 'feature_fraction': 0.9, 'learning_rate': 0.05, 'max_depth': 3, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 400, 'objective': 'regression', 'reg_alpha': 0.7, 'reg_lambda': 0.1, 'subsample': 0.3}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 77/200 [2:26:17<6:37:10, 193.75s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 39%|███▉      | 78/200 [2:26:17<5:27:22, 161.00s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 39%|███▉      | 78/200 [2:26:18<5:27:22, 161.00s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Early stopping, best iteration is:\n",
      "[12]\ttraining's rmse: 0.776102\tvalid_1's rmse: 0.916273\n",
      "\n",
      " 39%|███▉      | 78/200 [2:26:50<5:27:22, 161.00s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    0.9162731916016408\n",
      "\n",
      " 39%|███▉      | 78/200 [2:26:50<5:27:22, 161.00s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.8, 'boosting': 'gbdt', 'colsample_bytree': 1, 'feature_fraction': 0.9, 'learning_rate': 0.2, 'max_depth': 12, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 400, 'objective': 'regression', 'reg_alpha': 0.6, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "\n",
      " 39%|███▉      | 78/200 [2:26:50<5:27:22, 161.00s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 40%|███▉      | 79/200 [2:26:50<4:07:17, 122.62s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 40%|███▉      | 79/200 [2:26:51<4:07:17, 122.62s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    [300]\ttraining's rmse: 0.802312\tvalid_1's rmse: 0.909372\n",
      "\n",
      " 40%|███▉      | 79/200 [2:28:20<4:07:17, 122.62s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Early stopping, best iteration is:\n",
      "[310]\ttraining's rmse: 0.80046\tvalid_1's rmse: 0.909163\n",
      "\n",
      " 40%|███▉      | 79/200 [2:28:49<4:07:17, 122.62s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    0.9091629742698674\n",
      "\n",
      " 40%|███▉      | 79/200 [2:28:50<4:07:17, 122.62s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.6, 'boosting': 'gbdt', 'colsample_bytree': 0.8, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 150, 'objective': 'regression', 'reg_alpha': 0.3, 'reg_lambda': 0.3, 'subsample': 0.7}\n",
      "\n",
      " 40%|███▉      | 79/200 [2:28:50<4:07:17, 122.62s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 40%|████      | 80/200 [2:28:50<4:03:20, 121.67s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 40%|████      | 80/200 [2:28:50<4:03:20, 121.67s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    [300]\ttraining's rmse: 0.807462\tvalid_1's rmse: 0.91191\n",
      "\n",
      " 40%|████      | 80/200 [2:30:10<4:03:20, 121.67s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Early stopping, best iteration is:\n",
      "[316]\ttraining's rmse: 0.804796\tvalid_1's rmse: 0.911822\n",
      "\n",
      " 40%|████      | 80/200 [2:30:36<4:03:20, 121.67s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    0.9118223059647358\n",
      "\n",
      " 40%|████      | 80/200 [2:30:37<4:03:20, 121.67s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.7, 'boosting': 'gbdt', 'colsample_bytree': 0.8, 'feature_fraction': 0.8, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 125, 'objective': 'regression', 'reg_alpha': 0.1, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "\n",
      " 40%|████      | 80/200 [2:30:37<4:03:20, 121.67s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 40%|████      | 81/200 [2:30:37<3:52:56, 117.45s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 40%|████      | 81/200 [2:30:38<3:52:56, 117.45s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    [300]\ttraining's rmse: 1.07476\tvalid_1's rmse: 1.05222\n",
      "\n",
      " 40%|████      | 81/200 [2:31:30<3:52:56, 117.45s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    [600]\ttraining's rmse: 1.00049\tvalid_1's rmse: 1.00072\n",
      "\n",
      " 40%|████      | 81/200 [2:32:24<3:52:56, 117.45s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    [900]\ttraining's rmse: 0.952276\tvalid_1's rmse: 0.969775\n",
      "\n",
      " 40%|████      | 81/200 [2:33:14<3:52:56, 117.45s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 0.940133\tvalid_1's rmse: 0.962505\n",
      "\n",
      " 40%|████      | 81/200 [2:33:31<3:52:56, 117.45s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    0.9625051001095754\n",
      "\n",
      " 40%|████      | 81/200 [2:33:33<3:52:56, 117.45s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.5, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.6, 'learning_rate': 0.001, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 20, 'num_leaves': 25, 'objective': 'regression', 'reg_alpha': 0.4, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "\n",
      " 40%|████      | 81/200 [2:33:33<3:52:56, 117.45s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 41%|████      | 82/200 [2:33:33<4:25:24, 134.95s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 41%|████      | 82/200 [2:33:34<4:25:24, 134.95s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    [300]\ttraining's rmse: 0.749338\tvalid_1's rmse: 0.942052\n",
      "\n",
      " 41%|████      | 82/200 [2:35:28<4:25:24, 134.95s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Early stopping, best iteration is:\n",
      "[211]\ttraining's rmse: 0.778278\tvalid_1's rmse: 0.941506\n",
      "\n",
      " 41%|████      | 82/200 [2:35:31<4:25:24, 134.95s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    0.9415057068612902\n",
      "\n",
      " 41%|████      | 82/200 [2:35:32<4:25:24, 134.95s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.9, 'boosting': 'gbdt', 'colsample_bytree': 0.9, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 15, 'metric': 'rmse', 'min_child_samples': 30, 'num_leaves': 400, 'objective': 'regression', 'reg_alpha': 0.7, 'reg_lambda': 0.4, 'subsample': 0.5}\n",
      "\n",
      " 41%|████      | 82/200 [2:35:32<4:25:24, 134.95s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 42%|████▏     | 83/200 [2:35:32<4:13:50, 130.18s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 42%|████▏     | 83/200 [2:35:32<4:13:50, 130.18s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A[300]\ttraining's rmse: 0.818636\tvalid_1's rmse: 0.915377                            \n",
      "\n",
      " 42%|████▏     | 83/200 [2:36:15<4:13:50, 130.18s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Early stopping, best iteration is:\n",
      "[243]\ttraining's rmse: 0.825183\tvalid_1's rmse: 0.913975\n",
      "\n",
      " 42%|████▏     | 83/200 [2:36:21<4:13:50, 130.18s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    0.9139745075519402\n",
      "\n",
      " 42%|████▏     | 83/200 [2:36:22<4:13:50, 130.18s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.9, 'boosting': 'gbdt', 'colsample_bytree': 0.8, 'feature_fraction': 0.7, 'learning_rate': 0.05, 'max_depth': 5, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 75, 'objective': 'regression', 'reg_alpha': 0.6, 'reg_lambda': 0.1, 'subsample': 0.3}\n",
      "\n",
      " 42%|████▏     | 83/200 [2:36:22<4:13:50, 130.18s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 42%|████▏     | 84/200 [2:36:22<3:24:51, 105.96s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 42%|████▏     | 84/200 [2:36:22<3:24:51, 105.96s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    [300]\ttraining's rmse: 0.831699\tvalid_1's rmse: 0.92319\n",
      "\n",
      " 42%|████▏     | 84/200 [2:36:58<3:24:51, 105.96s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Early stopping, best iteration is:\n",
      "[217]\ttraining's rmse: 0.838171\tvalid_1's rmse: 0.922516\n",
      "\n",
      " 42%|████▏     | 84/200 [2:37:00<3:24:51, 105.96s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    0.9225155176320647\n",
      "\n",
      " 42%|████▏     | 84/200 [2:37:00<3:24:51, 105.96s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.9, 'boosting': 'gbdt', 'colsample_bytree': 0.8, 'feature_fraction': 1, 'learning_rate': 0.2, 'max_depth': 3, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 100, 'objective': 'regression', 'reg_alpha': 0.7, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "\n",
      " 42%|████▏     | 84/200 [2:37:00<3:24:51, 105.96s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 42%|████▎     | 85/200 [2:37:00<2:44:26, 85.79s/trial, best loss: 0.9039042903249872] \u001b[A\n",
      "\u001b[A                                                                                   Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 42%|████▎     | 85/200 [2:37:01<2:44:26, 85.79s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                   [300]\ttraining's rmse: 0.792125\tvalid_1's rmse: 0.906639\n",
      "\n",
      " 42%|████▎     | 85/200 [2:38:41<2:44:26, 85.79s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                   Early stopping, best iteration is:\n",
      "[283]\ttraining's rmse: 0.795676\tvalid_1's rmse: 0.906129\n",
      "\n",
      " 42%|████▎     | 85/200 [2:39:05<2:44:26, 85.79s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                   0.9061289087881554\n",
      "\n",
      " 42%|████▎     | 85/200 [2:39:06<2:44:26, 85.79s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                   {'bagging_frequency': 0.8, 'boosting': 'gbdt', 'colsample_bytree': 1, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 20, 'num_leaves': 225, 'objective': 'regression', 'reg_alpha': 0.3, 'reg_lambda': 0.3, 'subsample': 0.9}\n",
      "\n",
      " 42%|████▎     | 85/200 [2:39:06<2:44:26, 85.79s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 43%|████▎     | 86/200 [2:39:06<3:05:33, 97.66s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                   Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 43%|████▎     | 86/200 [2:39:06<3:05:33, 97.66s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                   [300]\ttraining's rmse: 0.833911\tvalid_1's rmse: 0.91812\n",
      "\n",
      " 43%|████▎     | 86/200 [2:40:13<3:05:33, 97.66s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                   Early stopping, best iteration is:\n",
      "[341]\ttraining's rmse: 0.828121\tvalid_1's rmse: 0.917078\n",
      "\n",
      " 43%|████▎     | 86/200 [2:40:39<3:05:33, 97.66s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                   0.9170780977601289\n",
      "\n",
      " 43%|████▎     | 86/200 [2:40:40<3:05:33, 97.66s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                   {'bagging_frequency': 0.6, 'boosting': 'gbdt', 'colsample_bytree': 0.8, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 50, 'objective': 'regression', 'reg_alpha': 0.1, 'reg_lambda': 0.5, 'subsample': 0.7}\n",
      "\n",
      " 43%|████▎     | 86/200 [2:40:40<3:05:33, 97.66s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 44%|████▎     | 87/200 [2:40:40<3:02:00, 96.64s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                   Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 44%|████▎     | 87/200 [2:40:40<3:02:00, 96.64s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                   [300]\ttraining's rmse: 0.761458\tvalid_1's rmse: 0.924991\n",
      "\n",
      " 44%|████▎     | 87/200 [2:42:32<3:02:00, 96.64s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                   Early stopping, best iteration is:\n",
      "[227]\ttraining's rmse: 0.783035\tvalid_1's rmse: 0.921989\n",
      "\n",
      " 44%|████▎     | 87/200 [2:42:41<3:02:00, 96.64s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                   0.9219889516636157\n",
      "\n",
      " 44%|████▎     | 87/200 [2:42:42<3:02:00, 96.64s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                   {'bagging_frequency': 0.7, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.8, 'learning_rate': 0.01, 'max_depth': 12, 'metric': 'rmse', 'min_child_samples': 30, 'num_leaves': 400, 'objective': 'regression', 'reg_alpha': 0.4, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "\n",
      " 44%|████▎     | 87/200 [2:42:42<3:02:00, 96.64s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 44%|████▍     | 88/200 [2:42:42<3:14:37, 104.26s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 44%|████▍     | 88/200 [2:42:43<3:14:37, 104.26s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    [300]\ttraining's rmse: 1.05276\tvalid_1's rmse: 1.03884\n",
      "\n",
      " 44%|████▍     | 88/200 [2:45:06<3:14:37, 104.26s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    [600]\ttraining's rmse: 0.962866\tvalid_1's rmse: 0.979729\n",
      "\n",
      " 44%|████▍     | 88/200 [2:47:22<3:14:37, 104.26s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    [900]\ttraining's rmse: 0.903204\tvalid_1's rmse: 0.945388\n",
      "\n",
      " 44%|████▍     | 88/200 [2:49:29<3:14:37, 104.26s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 0.888097\tvalid_1's rmse: 0.937621\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 88/200 [2:50:12<3:14:37, 104.26s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    0.937621480187813\n",
      "\n",
      " 44%|████▍     | 88/200 [2:50:20<3:14:37, 104.26s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.5, 'boosting': 'gbdt', 'colsample_bytree': 0.9, 'feature_fraction': 0.6, 'learning_rate': 0.001, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 500, 'objective': 'regression', 'reg_alpha': 0.7, 'reg_lambda': 0.4, 'subsample': 0.5}\n",
      "\n",
      " 44%|████▍     | 88/200 [2:50:20<3:14:37, 104.26s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 44%|████▍     | 89/200 [2:50:20<6:29:12, 210.38s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 44%|████▍     | 89/200 [2:50:20<6:29:12, 210.38s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Early stopping, best iteration is:\n",
      "[33]\ttraining's rmse: 0.83303\tvalid_1's rmse: 0.94269\n",
      "\n",
      " 44%|████▍     | 89/200 [2:50:56<6:29:12, 210.38s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    0.942690059979458\n",
      "\n",
      " 44%|████▍     | 89/200 [2:50:56<6:29:12, 210.38s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.9, 'boosting': 'gbdt', 'colsample_bytree': 0.8, 'feature_fraction': 0.9, 'learning_rate': 0.05, 'max_depth': 15, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 150, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.1, 'subsample': 0.3}\n",
      "\n",
      " 44%|████▍     | 89/200 [2:50:56<6:29:12, 210.38s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 45%|████▌     | 90/200 [2:50:56<4:49:41, 158.02s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 45%|████▌     | 90/200 [2:50:56<4:49:41, 158.02s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    [300]\ttraining's rmse: 0.871016\tvalid_1's rmse: 0.934346\n",
      "\n",
      " 45%|████▌     | 90/200 [2:51:53<4:49:41, 158.02s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    [600]\ttraining's rmse: 0.85011\tvalid_1's rmse: 0.927969\n",
      "\n",
      " 45%|████▌     | 90/200 [2:53:01<4:49:41, 158.02s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    [900]\ttraining's rmse: 0.83586\tvalid_1's rmse: 0.920319\n",
      "\n",
      " 45%|████▌     | 90/200 [2:54:02<4:49:41, 158.02s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 0.832496\tvalid_1's rmse: 0.919592\n",
      "\n",
      " 45%|████▌     | 90/200 [2:54:20<4:49:41, 158.02s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    0.9195915857007928\n",
      "\n",
      " 45%|████▌     | 90/200 [2:54:22<4:49:41, 158.02s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.9, 'boosting': 'gbdt', 'colsample_bytree': 1, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 5, 'metric': 'rmse', 'min_child_samples': 20, 'num_leaves': 400, 'objective': 'regression', 'reg_alpha': 0.6, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "\n",
      " 45%|████▌     | 90/200 [2:54:22<4:49:41, 158.02s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 46%|████▌     | 91/200 [2:54:22<5:13:17, 172.45s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 46%|████▌     | 91/200 [2:54:22<5:13:17, 172.45s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Early stopping, best iteration is:\n",
      "[11]\ttraining's rmse: 0.829817\tvalid_1's rmse: 0.917497\n",
      "\n",
      " 46%|████▌     | 91/200 [2:54:42<5:13:17, 172.45s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    0.9174970968363497\n",
      "\n",
      " 46%|████▌     | 91/200 [2:54:42<5:13:17, 172.45s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.9, 'boosting': 'gbdt', 'colsample_bytree': 0.8, 'feature_fraction': 0.7, 'learning_rate': 0.2, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 125, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.3, 'subsample': 0.7}\n",
      "\n",
      " 46%|████▌     | 91/200 [2:54:42<5:13:17, 172.45s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 46%|████▌     | 92/200 [2:54:42<3:48:16, 126.82s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 46%|████▌     | 92/200 [2:54:42<3:48:16, 126.82s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    [300]\ttraining's rmse: 0.904563\tvalid_1's rmse: 0.957061\n",
      "\n",
      " 46%|████▌     | 92/200 [2:55:24<3:48:16, 126.82s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    [600]\ttraining's rmse: 0.885186\tvalid_1's rmse: 0.944757\n",
      "\n",
      " 46%|████▌     | 92/200 [2:56:05<3:48:16, 126.82s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    [900]\ttraining's rmse: 0.874745\tvalid_1's rmse: 0.937731\n",
      "\n",
      " 46%|████▌     | 92/200 [2:56:51<3:48:16, 126.82s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 0.870763\tvalid_1's rmse: 0.934577\n",
      "\n",
      " 46%|████▌     | 92/200 [2:57:07<3:48:16, 126.82s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    0.9345768968356652\n",
      "\n",
      " 46%|████▌     | 92/200 [2:57:08<3:48:16, 126.82s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.8, 'boosting': 'gbdt', 'colsample_bytree': 0.8, 'feature_fraction': 1, 'learning_rate': 0.01, 'max_depth': 3, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 25, 'objective': 'regression', 'reg_alpha': 0.3, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "\n",
      " 46%|████▌     | 92/200 [2:57:08<3:48:16, 126.82s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 46%|████▋     | 93/200 [2:57:08<3:56:15, 132.48s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 46%|████▋     | 93/200 [2:57:08<3:56:15, 132.48s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    [300]\ttraining's rmse: 0.815892\tvalid_1's rmse: 0.932001\n",
      "\n",
      " 46%|████▋     | 93/200 [2:58:28<3:56:15, 132.48s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Early stopping, best iteration is:\n",
      "[409]\ttraining's rmse: 0.801066\tvalid_1's rmse: 0.930446\n",
      "\n",
      " 46%|████▋     | 93/200 [2:59:16<3:56:15, 132.48s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    0.9304464337338211\n",
      "\n",
      " 46%|████▋     | 93/200 [2:59:17<3:56:15, 132.48s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.6, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 12, 'metric': 'rmse', 'min_child_samples': 30, 'num_leaves': 75, 'objective': 'regression', 'reg_alpha': 0.4, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "\n",
      " 46%|████▋     | 93/200 [2:59:17<3:56:15, 132.48s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 47%|████▋     | 94/200 [2:59:17<3:52:26, 131.57s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 47%|████▋     | 94/200 [2:59:18<3:52:26, 131.57s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    [300]\ttraining's rmse: 0.813768\tvalid_1's rmse: 0.912055\n",
      "\n",
      " 47%|████▋     | 94/200 [3:00:53<3:52:26, 131.57s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Early stopping, best iteration is:\n",
      "[308]\ttraining's rmse: 0.812371\tvalid_1's rmse: 0.912015\n",
      "\n",
      " 47%|████▋     | 94/200 [3:01:21<3:52:26, 131.57s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    0.9120148164408565\n",
      "\n",
      " 47%|████▋     | 94/200 [3:01:21<3:52:26, 131.57s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.7, 'boosting': 'gbdt', 'colsample_bytree': 0.8, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 100, 'objective': 'regression', 'reg_alpha': 0.1, 'reg_lambda': 0.5, 'subsample': 0.5}\n",
      "\n",
      " 47%|████▋     | 94/200 [3:01:21<3:52:26, 131.57s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 48%|████▊     | 95/200 [3:01:21<3:46:16, 129.30s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 48%|████▊     | 95/200 [3:01:22<3:46:16, 129.30s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    [300]\ttraining's rmse: 1.06387\tvalid_1's rmse: 1.04505\n",
      "\n",
      " 48%|████▊     | 95/200 [3:03:02<3:46:16, 129.30s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    [600]\ttraining's rmse: 0.984456\tvalid_1's rmse: 0.991047\n",
      "\n",
      " 48%|████▊     | 95/200 [3:04:29<3:46:16, 129.30s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    [900]\ttraining's rmse: 0.933975\tvalid_1's rmse: 0.95969\n",
      "\n",
      " 48%|████▊     | 95/200 [3:06:08<3:46:16, 129.30s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 0.921594\tvalid_1's rmse: 0.952674\n",
      "\n",
      " 48%|████▊     | 95/200 [3:06:40<3:46:16, 129.30s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    0.9526739044014725\n",
      "\n",
      " 48%|████▊     | 95/200 [3:06:43<3:46:16, 129.30s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.9, 'boosting': 'gbdt', 'colsample_bytree': 0.9, 'feature_fraction': 0.8, 'learning_rate': 0.001, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 50, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.4, 'subsample': 0.9}\n",
      "\n",
      " 48%|████▊     | 95/200 [3:06:43<3:46:16, 129.30s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 48%|████▊     | 96/200 [3:06:43<5:23:57, 186.90s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 48%|████▊     | 96/200 [3:06:43<5:23:57, 186.90s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Early stopping, best iteration is:\n",
      "[38]\ttraining's rmse: 0.828613\tvalid_1's rmse: 0.911558\n",
      "\n",
      " 48%|████▊     | 96/200 [3:07:27<5:23:57, 186.90s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    0.9115578522989312\n",
      "\n",
      " 48%|████▊     | 96/200 [3:07:28<5:23:57, 186.90s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.5, 'boosting': 'gbdt', 'colsample_bytree': 0.8, 'feature_fraction': 0.6, 'learning_rate': 0.05, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 20, 'num_leaves': 225, 'objective': 'regression', 'reg_alpha': 0.7, 'reg_lambda': 0.1, 'subsample': 0.3}\n",
      "\n",
      " 48%|████▊     | 96/200 [3:07:28<5:23:57, 186.90s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 48%|████▊     | 97/200 [3:07:28<4:07:41, 144.28s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 48%|████▊     | 97/200 [3:07:28<4:07:41, 144.28s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    [300]\ttraining's rmse: 0.749474\tvalid_1's rmse: 0.939146\n",
      "\n",
      " 48%|████▊     | 97/200 [3:10:03<4:07:41, 144.28s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Early stopping, best iteration is:\n",
      "[272]\ttraining's rmse: 0.757126\tvalid_1's rmse: 0.93882\n",
      "\n",
      " 48%|████▊     | 97/200 [3:10:29<4:07:41, 144.28s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    0.9388195304679409\n",
      "\n",
      " 48%|████▊     | 97/200 [3:10:30<4:07:41, 144.28s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.9, 'boosting': 'gbdt', 'colsample_bytree': 1, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 15, 'metric': 'rmse', 'min_child_samples': 30, 'num_leaves': 400, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "\n",
      " 48%|████▊     | 97/200 [3:10:30<4:07:41, 144.28s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 49%|████▉     | 98/200 [3:10:30<4:24:46, 155.75s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 49%|████▉     | 98/200 [3:10:31<4:24:46, 155.75s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Early stopping, best iteration is:\n",
      "[13]\ttraining's rmse: 0.781733\tvalid_1's rmse: 0.90905\n",
      "\n",
      " 49%|████▉     | 98/200 [3:11:21<4:24:46, 155.75s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    0.909049628579443\n",
      "\n",
      " 49%|████▉     | 98/200 [3:11:21<4:24:46, 155.75s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.9, 'boosting': 'gbdt', 'colsample_bytree': 0.8, 'feature_fraction': 0.9, 'learning_rate': 0.2, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 500, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'subsample': 0.7}\n",
      "\n",
      " 49%|████▉     | 98/200 [3:11:21<4:24:46, 155.75s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 50%|████▉     | 99/200 [3:11:21<3:29:16, 124.33s/trial, best loss: 0.9039042903249872]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[ATraining until validation scores don't improve for 100 rounds                       \n",
      "\n",
      " 50%|████▉     | 99/200 [3:11:21<3:29:16, 124.33s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    [300]\ttraining's rmse: 0.869726\tvalid_1's rmse: 0.932598\n",
      "\n",
      " 50%|████▉     | 99/200 [3:12:11<3:29:16, 124.33s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    [600]\ttraining's rmse: 0.849504\tvalid_1's rmse: 0.926701\n",
      "\n",
      " 50%|████▉     | 99/200 [3:12:52<3:29:16, 124.33s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    [900]\ttraining's rmse: 0.835706\tvalid_1's rmse: 0.919592\n",
      "\n",
      " 50%|████▉     | 99/200 [3:13:34<3:29:16, 124.33s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 0.832025\tvalid_1's rmse: 0.917968\n",
      "\n",
      " 50%|████▉     | 99/200 [3:13:48<3:29:16, 124.33s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    0.9179680777378705\n",
      "\n",
      " 50%|████▉     | 99/200 [3:13:50<3:29:16, 124.33s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.8, 'feature_fraction': 0.7, 'learning_rate': 0.01, 'max_depth': 5, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 400, 'objective': 'regression', 'reg_alpha': 0.7, 'reg_lambda': 0.3, 'subsample': 0.9}\n",
      "\n",
      " 50%|████▉     | 99/200 [3:13:50<3:29:16, 124.33s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 50%|█████     | 100/200 [3:13:50<3:39:23, 131.63s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 50%|█████     | 100/200 [3:13:50<3:39:23, 131.63s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     Early stopping, best iteration is:\n",
      "[27]\ttraining's rmse: 0.824797\tvalid_1's rmse: 0.91188\n",
      "\n",
      " 50%|█████     | 100/200 [3:14:21<3:39:23, 131.63s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     0.911880042002071\n",
      "\n",
      " 50%|█████     | 100/200 [3:14:21<3:39:23, 131.63s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     {'bagging_frequency': 0.9, 'boosting': 'gbdt', 'colsample_bytree': 0.6, 'feature_fraction': 1, 'learning_rate': 0.1, 'max_depth': 8, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 150, 'objective': 'regression', 'reg_alpha': 0.3, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "\n",
      " 50%|█████     | 100/200 [3:14:21<3:39:23, 131.63s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 50%|█████     | 101/200 [3:14:21<2:47:41, 101.63s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 50%|█████     | 101/200 [3:14:22<2:47:41, 101.63s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     [300]\ttraining's rmse: 0.904619\tvalid_1's rmse: 0.956682\n",
      "\n",
      " 50%|█████     | 101/200 [3:14:58<2:47:41, 101.63s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     [600]\ttraining's rmse: 0.885613\tvalid_1's rmse: 0.94511\n",
      "\n",
      " 50%|█████     | 101/200 [3:15:34<2:47:41, 101.63s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     [900]\ttraining's rmse: 0.875041\tvalid_1's rmse: 0.937732\n",
      "\n",
      " 50%|█████     | 101/200 [3:16:10<2:47:41, 101.63s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 0.871209\tvalid_1's rmse: 0.934801\n",
      "\n",
      " 50%|█████     | 101/200 [3:16:22<2:47:41, 101.63s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     0.9348009783922893\n",
      "\n",
      " 50%|█████     | 101/200 [3:16:23<2:47:41, 101.63s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     {'bagging_frequency': 0.8, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 3, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 125, 'objective': 'regression', 'reg_alpha': 0.6, 'reg_lambda': 0.6, 'subsample': 0.8}\n",
      "\n",
      " 50%|█████     | 101/200 [3:16:23<2:47:41, 101.63s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 51%|█████     | 102/200 [3:16:23<2:55:59, 107.75s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 51%|█████     | 102/200 [3:16:24<2:55:59, 107.75s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     [300]\ttraining's rmse: 1.06893\tvalid_1's rmse: 1.04671\n",
      "\n",
      " 51%|█████     | 102/200 [3:17:26<2:55:59, 107.75s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     [600]\ttraining's rmse: 0.993682\tvalid_1's rmse: 0.994663\n",
      "\n",
      " 51%|█████     | 102/200 [3:18:29<2:55:59, 107.75s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     [900]\ttraining's rmse: 0.946376\tvalid_1's rmse: 0.964593\n",
      "\n",
      " 51%|█████     | 102/200 [3:19:30<2:55:59, 107.75s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 0.934807\tvalid_1's rmse: 0.957792\n",
      "\n",
      " 51%|█████     | 102/200 [3:19:50<2:55:59, 107.75s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     0.9577921578279637\n",
      "\n",
      " 51%|█████     | 102/200 [3:19:52<2:55:59, 107.75s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     {'bagging_frequency': 0.3, 'boosting': 'gbdt', 'colsample_bytree': 0.5, 'feature_fraction': 0.9, 'learning_rate': 0.001, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 20, 'num_leaves': 25, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'subsample': 1}\n",
      "\n",
      " 51%|█████     | 102/200 [3:19:52<2:55:59, 107.75s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 52%|█████▏    | 103/200 [3:19:52<3:43:02, 137.96s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 52%|█████▏    | 103/200 [3:19:52<3:43:02, 137.96s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     [300]\ttraining's rmse: 0.816338\tvalid_1's rmse: 0.931172\n",
      "\n",
      " 52%|█████▏    | 103/200 [3:21:00<3:43:02, 137.96s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     Early stopping, best iteration is:\n",
      "[375]\ttraining's rmse: 0.805513\tvalid_1's rmse: 0.930362\n",
      "\n",
      " 52%|█████▏    | 103/200 [3:21:34<3:43:02, 137.96s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     0.9303618266329741\n",
      "\n",
      " 52%|█████▏    | 103/200 [3:21:35<3:43:02, 137.96s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A{'bagging_frequency': 0.6, 'boosting': 'gbdt', 'colsample_bytree': 0.8, 'feature_fraction': 0.8, 'learning_rate': 0.01, 'max_depth': 12, 'metric': 'rmse', 'min_child_samples': 30, 'num_leaves': 75, 'objective': 'regression', 'reg_alpha': 0.2, 'reg_lambda': 0.4, 'subsample': 0.9}\n",
      "\n",
      " 52%|█████▏    | 103/200 [3:21:35<3:43:02, 137.96s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 52%|█████▏    | 104/200 [3:21:35<3:23:59, 127.50s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 52%|█████▏    | 104/200 [3:21:35<3:23:59, 127.50s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     Early stopping, best iteration is:\n",
      "[55]\ttraining's rmse: 0.821578\tvalid_1's rmse: 0.912766\n",
      "\n",
      " 52%|█████▏    | 104/200 [3:22:04<3:23:59, 127.50s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     0.9127664278145622\n",
      "\n",
      " 52%|█████▏    | 104/200 [3:22:04<3:23:59, 127.50s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     {'bagging_frequency': 0.7, 'boosting': 'gbdt', 'colsample_bytree': 0.9, 'feature_fraction': 0.6, 'learning_rate': 0.05, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 100, 'objective': 'regression', 'reg_alpha': 0.4, 'reg_lambda': 0.2, 'subsample': 0.3}\n",
      "\n",
      " 52%|█████▏    | 104/200 [3:22:04<3:23:59, 127.50s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 52%|█████▎    | 105/200 [3:22:04<2:35:12, 98.03s/trial, best loss: 0.9039042903249872] \u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 52%|█████▎    | 105/200 [3:22:05<2:35:12, 98.03s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    [300]\ttraining's rmse: 0.776408\tvalid_1's rmse: 0.905909\n",
      "\n",
      " 52%|█████▎    | 105/200 [3:24:02<2:35:12, 98.03s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Early stopping, best iteration is:\n",
      "[290]\ttraining's rmse: 0.778734\tvalid_1's rmse: 0.905408\n",
      "\n",
      " 52%|█████▎    | 105/200 [3:24:32<2:35:12, 98.03s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    0.9054084003134788\n",
      "\n",
      " 52%|█████▎    | 105/200 [3:24:34<2:35:12, 98.03s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.9, 'boosting': 'gbdt', 'colsample_bytree': 0.8, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 400, 'objective': 'regression', 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'subsample': 0.5}\n",
      "\n",
      " 52%|█████▎    | 105/200 [3:24:34<2:35:12, 98.03s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 53%|█████▎    | 106/200 [3:24:34<2:57:41, 113.42s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 53%|█████▎    | 106/200 [3:24:34<2:57:41, 113.42s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     Early stopping, best iteration is:\n",
      "[16]\ttraining's rmse: 0.83856\tvalid_1's rmse: 0.920158\n",
      "\n",
      " 53%|█████▎    | 106/200 [3:24:52<2:57:41, 113.42s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     0.9201582045791549\n",
      "\n",
      " 53%|█████▎    | 106/200 [3:24:52<2:57:41, 113.42s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     {'bagging_frequency': 0.9, 'boosting': 'gbdt', 'colsample_bytree': 1, 'feature_fraction': 0.9, 'learning_rate': 0.2, 'max_depth': 8, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 50, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "\n",
      " 53%|█████▎    | 106/200 [3:24:52<2:57:41, 113.42s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 54%|█████▎    | 107/200 [3:24:52<2:11:32, 84.86s/trial, best loss: 0.9039042903249872] \u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 54%|█████▎    | 107/200 [3:24:52<2:11:32, 84.86s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Early stopping, best iteration is:\n",
      "[169]\ttraining's rmse: 0.821073\tvalid_1's rmse: 0.931732\n",
      "\n",
      " 54%|█████▎    | 107/200 [3:26:07<2:11:32, 84.86s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    0.9317324415001819\n",
      "\n",
      " 54%|█████▎    | 107/200 [3:26:07<2:11:32, 84.86s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.5, 'boosting': 'gbdt', 'colsample_bytree': 0.8, 'feature_fraction': 0.7, 'learning_rate': 0.01, 'max_depth': 15, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 225, 'objective': 'regression', 'reg_alpha': 0.7, 'reg_lambda': 0.3, 'subsample': 0.7}\n",
      "\n",
      " 54%|█████▎    | 107/200 [3:26:07<2:11:32, 84.86s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 54%|█████▍    | 108/200 [3:26:07<2:05:53, 82.11s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 54%|█████▍    | 108/200 [3:26:08<2:05:53, 82.11s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Early stopping, best iteration is:\n",
      "[28]\ttraining's rmse: 0.793999\tvalid_1's rmse: 0.90834\n",
      "\n",
      " 54%|█████▍    | 108/200 [3:26:45<2:05:53, 82.11s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    0.9083404672526745\n",
      "\n",
      " 54%|█████▍    | 108/200 [3:26:45<2:05:53, 82.11s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.6, 'feature_fraction': 1, 'learning_rate': 0.1, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 20, 'num_leaves': 250, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "\n",
      " 54%|█████▍    | 108/200 [3:26:45<2:05:53, 82.11s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 55%|█████▍    | 109/200 [3:26:45<1:44:24, 68.84s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 55%|█████▍    | 109/200 [3:26:46<1:44:24, 68.84s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    [300]\ttraining's rmse: 0.871018\tvalid_1's rmse: 0.933947\n",
      "\n",
      " 55%|█████▍    | 109/200 [3:27:40<1:44:24, 68.84s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    [600]\ttraining's rmse: 0.850226\tvalid_1's rmse: 0.928763\n",
      "\n",
      " 55%|█████▍    | 109/200 [3:28:29<1:44:24, 68.84s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    [900]\ttraining's rmse: 0.835667\tvalid_1's rmse: 0.920718\n",
      "\n",
      " 55%|█████▍    | 109/200 [3:29:19<1:44:24, 68.84s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 0.832067\tvalid_1's rmse: 0.919539\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 109/200 [3:29:34<1:44:24, 68.84s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    0.9195391712774695\n",
      "\n",
      " 55%|█████▍    | 109/200 [3:29:37<1:44:24, 68.84s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.9, 'boosting': 'gbdt', 'colsample_bytree': 0.8, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 5, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 400, 'objective': 'regression', 'reg_alpha': 0.7, 'reg_lambda': 0.6, 'subsample': 0.8}\n",
      "\n",
      " 55%|█████▍    | 109/200 [3:29:37<1:44:24, 68.84s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 55%|█████▌    | 110/200 [3:29:37<2:29:18, 99.54s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 55%|█████▌    | 110/200 [3:29:37<2:29:18, 99.54s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    [300]\ttraining's rmse: 0.904421\tvalid_1's rmse: 0.956523\n",
      "\n",
      " 55%|█████▌    | 110/200 [3:30:14<2:29:18, 99.54s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    [600]\ttraining's rmse: 0.884722\tvalid_1's rmse: 0.943948\n",
      "\n",
      " 55%|█████▌    | 110/200 [3:30:51<2:29:18, 99.54s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    [900]\ttraining's rmse: 0.874756\tvalid_1's rmse: 0.937085\n",
      "\n",
      " 55%|█████▌    | 110/200 [3:31:29<2:29:18, 99.54s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 0.871524\tvalid_1's rmse: 0.934661\n",
      "\n",
      " 55%|█████▌    | 110/200 [3:31:42<2:29:18, 99.54s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    0.9346610960045776\n",
      "\n",
      " 55%|█████▌    | 110/200 [3:31:43<2:29:18, 99.54s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.3, 'boosting': 'gbdt', 'colsample_bytree': 0.5, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 3, 'metric': 'rmse', 'min_child_samples': 30, 'num_leaves': 500, 'objective': 'regression', 'reg_alpha': 0.2, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "\n",
      " 55%|█████▌    | 110/200 [3:31:43<2:29:18, 99.54s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 56%|█████▌    | 111/200 [3:31:43<2:39:36, 107.60s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 56%|█████▌    | 111/200 [3:31:44<2:39:36, 107.60s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     [300]\ttraining's rmse: 1.04851\tvalid_1's rmse: 1.03571\n",
      "\n",
      " 56%|█████▌    | 111/200 [3:33:58<2:39:36, 107.60s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     [600]\ttraining's rmse: 0.958131\tvalid_1's rmse: 0.976771\n",
      "\n",
      " 56%|█████▌    | 111/200 [3:36:10<2:39:36, 107.60s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     [900]\ttraining's rmse: 0.899758\tvalid_1's rmse: 0.943084\n",
      "\n",
      " 56%|█████▌    | 111/200 [3:38:16<2:39:36, 107.60s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 0.885197\tvalid_1's rmse: 0.935767\n",
      "\n",
      " 56%|█████▌    | 111/200 [3:38:58<2:39:36, 107.60s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     0.9357674532312578\n",
      "\n",
      " 56%|█████▌    | 111/200 [3:39:04<2:39:36, 107.60s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     {'bagging_frequency': 0.9, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.8, 'learning_rate': 0.001, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 400, 'objective': 'regression', 'reg_alpha': 0.6, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n",
      " 56%|█████▌    | 111/200 [3:39:04<2:39:36, 107.60s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 56%|█████▌    | 112/200 [3:39:04<5:04:34, 207.66s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 56%|█████▌    | 112/200 [3:39:04<5:04:34, 207.66s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     Early stopping, best iteration is:\n",
      "[65]\ttraining's rmse: 0.794017\tvalid_1's rmse: 0.927061\n",
      "\n",
      " 56%|█████▌    | 112/200 [3:39:44<5:04:34, 207.66s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     0.9270608040576506\n",
      "\n",
      " 56%|█████▌    | 112/200 [3:39:44<5:04:34, 207.66s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     {'bagging_frequency': 0.8, 'boosting': 'gbdt', 'colsample_bytree': 0.8, 'feature_fraction': 0.9, 'learning_rate': 0.05, 'max_depth': 12, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 125, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "\n",
      " 56%|█████▌    | 112/200 [3:39:44<5:04:34, 207.66s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 56%|█████▋    | 113/200 [3:39:44<3:48:08, 157.34s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 56%|█████▋    | 113/200 [3:39:44<3:48:08, 157.34s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     [300]\ttraining's rmse: 0.80491\tvalid_1's rmse: 0.911677\n",
      "\n",
      " 56%|█████▋    | 113/200 [3:41:02<3:48:08, 157.34s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     Early stopping, best iteration is:\n",
      "[336]\ttraining's rmse: 0.798687\tvalid_1's rmse: 0.911309\n",
      "\n",
      " 56%|█████▋    | 113/200 [3:41:31<3:48:08, 157.34s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     0.9113088571588562\n",
      "\n",
      " 56%|█████▋    | 113/200 [3:41:32<3:48:08, 157.34s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     {'bagging_frequency': 0.6, 'boosting': 'gbdt', 'colsample_bytree': 0.9, 'feature_fraction': 0.6, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 150, 'objective': 'regression', 'reg_alpha': 0.3, 'reg_lambda': 0.4, 'subsample': 0.5}\n",
      "\n",
      " 56%|█████▋    | 113/200 [3:41:32<3:48:08, 157.34s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 57%|█████▋    | 114/200 [3:41:32<3:24:09, 142.44s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 57%|█████▋    | 114/200 [3:41:32<3:24:09, 142.44s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     Early stopping, best iteration is:\n",
      "[81]\ttraining's rmse: 0.821386\tvalid_1's rmse: 0.914376\n",
      "\n",
      " 57%|█████▋    | 114/200 [3:41:58<3:24:09, 142.44s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A0.9143757816130276                                                                   \n",
      "\n",
      " 57%|█████▋    | 114/200 [3:41:58<3:24:09, 142.44s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     {'bagging_frequency': 0.9, 'boosting': 'gbdt', 'colsample_bytree': 0.8, 'feature_fraction': 0.9, 'learning_rate': 0.1, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 20, 'num_leaves': 25, 'objective': 'regression', 'reg_alpha': 0.4, 'reg_lambda': 0.1, 'subsample': 0.3}\n",
      "\n",
      " 57%|█████▋    | 114/200 [3:41:58<3:24:09, 142.44s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 57%|█████▊    | 115/200 [3:41:58<2:32:18, 107.52s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 57%|█████▊    | 115/200 [3:41:58<2:32:18, 107.52s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     Early stopping, best iteration is:\n",
      "[32]\ttraining's rmse: 0.78581\tvalid_1's rmse: 0.911348\n",
      "\n",
      " 57%|█████▊    | 115/200 [3:42:29<2:32:18, 107.52s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     0.9113475789825771\n",
      "\n",
      " 57%|█████▊    | 115/200 [3:42:29<2:32:18, 107.52s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     {'bagging_frequency': 0.7, 'boosting': 'gbdt', 'colsample_bytree': 0.6, 'feature_fraction': 0.7, 'learning_rate': 0.2, 'max_depth': 8, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 400, 'objective': 'regression', 'reg_alpha': 0.1, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "\n",
      " 57%|█████▊    | 115/200 [3:42:29<2:32:18, 107.52s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 58%|█████▊    | 116/200 [3:42:29<1:58:32, 84.68s/trial, best loss: 0.9039042903249872] \u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 58%|█████▊    | 116/200 [3:42:29<1:58:32, 84.68s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    [300]\ttraining's rmse: 0.812734\tvalid_1's rmse: 0.939147\n",
      "\n",
      " 58%|█████▊    | 116/200 [3:43:45<1:58:32, 84.68s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Early stopping, best iteration is:\n",
      "[427]\ttraining's rmse: 0.795677\tvalid_1's rmse: 0.936796\n",
      "\n",
      " 58%|█████▊    | 116/200 [3:44:32<1:58:32, 84.68s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    0.9367957824272176\n",
      "\n",
      " 58%|█████▊    | 116/200 [3:44:33<1:58:32, 84.68s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.9, 'boosting': 'gbdt', 'colsample_bytree': 0.8, 'feature_fraction': 1, 'learning_rate': 0.01, 'max_depth': 15, 'metric': 'rmse', 'min_child_samples': 30, 'num_leaves': 75, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.3, 'subsample': 0.7}\n",
      "\n",
      " 58%|█████▊    | 116/200 [3:44:33<1:58:32, 84.68s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 58%|█████▊    | 117/200 [3:44:33<2:13:18, 96.36s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 58%|█████▊    | 117/200 [3:44:33<2:13:18, 96.36s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    [300]\ttraining's rmse: 0.813497\tvalid_1's rmse: 0.91247\n",
      "\n",
      " 58%|█████▊    | 117/200 [3:45:55<2:13:18, 96.36s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Early stopping, best iteration is:\n",
      "[316]\ttraining's rmse: 0.81083\tvalid_1's rmse: 0.91209\n",
      "\n",
      " 58%|█████▊    | 117/200 [3:46:21<2:13:18, 96.36s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    0.9120895475022434\n",
      "\n",
      " 58%|█████▊    | 117/200 [3:46:22<2:13:18, 96.36s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.5, 'boosting': 'gbdt', 'colsample_bytree': 1, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 100, 'objective': 'regression', 'reg_alpha': 0.7, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "\n",
      " 58%|█████▊    | 117/200 [3:46:22<2:13:18, 96.36s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 59%|█████▉    | 118/200 [3:46:22<2:17:07, 100.34s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 59%|█████▉    | 118/200 [3:46:23<2:17:07, 100.34s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     [300]\ttraining's rmse: 0.871065\tvalid_1's rmse: 0.934215\n",
      "\n",
      " 59%|█████▉    | 118/200 [3:47:16<2:17:07, 100.34s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     [600]\ttraining's rmse: 0.850244\tvalid_1's rmse: 0.929056\n",
      "\n",
      " 59%|█████▉    | 118/200 [3:48:05<2:17:07, 100.34s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     [900]\ttraining's rmse: 0.83585\tvalid_1's rmse: 0.921444\n",
      "\n",
      " 59%|█████▉    | 118/200 [3:48:54<2:17:07, 100.34s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 0.832493\tvalid_1's rmse: 0.92038\n",
      "\n",
      " 59%|█████▉    | 118/200 [3:49:10<2:17:07, 100.34s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     0.9203800695176065\n",
      "\n",
      " 59%|█████▉    | 118/200 [3:49:12<2:17:07, 100.34s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.5, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 5, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 50, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.6, 'subsample': 0.8}\n",
      "\n",
      " 59%|█████▉    | 118/200 [3:49:12<2:17:07, 100.34s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 60%|█████▉    | 119/200 [3:49:12<2:43:37, 121.20s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 60%|█████▉    | 119/200 [3:49:12<2:43:37, 121.20s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     [300]\ttraining's rmse: 1.08988\tvalid_1's rmse: 1.06947\n",
      "\n",
      " 60%|█████▉    | 119/200 [3:49:46<2:43:37, 121.20s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     [600]\ttraining's rmse: 1.02643\tvalid_1's rmse: 1.02815\n",
      "\n",
      " 60%|█████▉    | 119/200 [3:50:21<2:43:37, 121.20s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     [900]\ttraining's rmse: 0.985262\tvalid_1's rmse: 1.00219\n",
      "\n",
      " 60%|█████▉    | 119/200 [3:50:55<2:43:37, 121.20s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 0.974972\tvalid_1's rmse: 0.995871\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 119/200 [3:51:07<2:43:37, 121.20s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     0.9958714368844136\n",
      "\n",
      " 60%|█████▉    | 119/200 [3:51:08<2:43:37, 121.20s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     {'bagging_frequency': 0.9, 'boosting': 'gbdt', 'colsample_bytree': 0.8, 'feature_fraction': 0.8, 'learning_rate': 0.001, 'max_depth': 3, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 250, 'objective': 'regression', 'reg_alpha': 0.7, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n",
      " 60%|█████▉    | 119/200 [3:51:08<2:43:37, 121.20s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 60%|██████    | 120/200 [3:51:08<2:39:25, 119.56s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 60%|██████    | 120/200 [3:51:08<2:39:25, 119.56s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     Early stopping, best iteration is:\n",
      "[59]\ttraining's rmse: 0.792605\tvalid_1's rmse: 0.905908\n",
      "\n",
      " 60%|██████    | 120/200 [3:51:56<2:39:25, 119.56s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     0.9059084764579182\n",
      "\n",
      " 60%|██████    | 120/200 [3:51:57<2:39:25, 119.56s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     {'bagging_frequency': 0.3, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.9, 'learning_rate': 0.05, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 20, 'num_leaves': 225, 'objective': 'regression', 'reg_alpha': 0.2, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "\n",
      " 60%|██████    | 120/200 [3:51:57<2:39:25, 119.56s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 60%|██████    | 121/200 [3:51:57<2:09:25, 98.30s/trial, best loss: 0.9039042903249872] \u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 60%|██████    | 121/200 [3:51:57<2:09:25, 98.30s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    [300]\ttraining's rmse: 0.782469\tvalid_1's rmse: 0.910138\n",
      "\n",
      " 60%|██████    | 121/200 [3:53:37<2:09:25, 98.30s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Early stopping, best iteration is:\n",
      "[249]\ttraining's rmse: 0.796027\tvalid_1's rmse: 0.909207\n",
      "\n",
      " 60%|██████    | 121/200 [3:53:50<2:09:25, 98.30s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    0.909207421252794\n",
      "\n",
      " 60%|██████    | 121/200 [3:53:52<2:09:25, 98.30s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.9, 'boosting': 'gbdt', 'colsample_bytree': 0.8, 'feature_fraction': 0.6, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 350, 'objective': 'regression', 'reg_alpha': 0.6, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "\n",
      " 60%|██████    | 121/200 [3:53:52<2:09:25, 98.30s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 61%|██████    | 122/200 [3:53:52<2:14:17, 103.30s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 61%|██████    | 122/200 [3:53:52<2:14:17, 103.30s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     Early stopping, best iteration is:\n",
      "[25]\ttraining's rmse: 0.773254\tvalid_1's rmse: 0.928667\n",
      "\n",
      " 61%|██████    | 122/200 [3:54:33<2:14:17, 103.30s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     0.928666520077532\n",
      "\n",
      " 61%|██████    | 122/200 [3:54:34<2:14:17, 103.30s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     {'bagging_frequency': 0.8, 'boosting': 'gbdt', 'colsample_bytree': 0.9, 'feature_fraction': 0.9, 'learning_rate': 0.1, 'max_depth': 12, 'metric': 'rmse', 'min_child_samples': 30, 'num_leaves': 400, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.4, 'subsample': 0.5}\n",
      "\n",
      " 61%|██████    | 122/200 [3:54:34<2:14:17, 103.30s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 62%|██████▏   | 123/200 [3:54:34<1:48:56, 84.89s/trial, best loss: 0.9039042903249872] \u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 62%|██████▏   | 123/200 [3:54:34<1:48:56, 84.89s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    [300]\ttraining's rmse: 0.776058\tvalid_1's rmse: 0.9081\n",
      "\n",
      " 62%|██████▏   | 123/200 [3:56:28<1:48:56, 84.89s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Early stopping, best iteration is:\n",
      "[257]\ttraining's rmse: 0.786956\tvalid_1's rmse: 0.907239\n",
      "\n",
      " 62%|██████▏   | 123/200 [3:56:47<1:48:56, 84.89s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    0.9072392487488135\n",
      "\n",
      " 62%|██████▏   | 123/200 [3:56:48<1:48:56, 84.89s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.9, 'boosting': 'gbdt', 'colsample_bytree': 0.8, 'feature_fraction': 0.7, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 40, 'num_leaves': 500, 'objective': 'regression', 'reg_alpha': 0.3, 'reg_lambda': 0.1, 'subsample': 0.3}\n",
      "\n",
      " 62%|██████▏   | 123/200 [3:56:48<1:48:56, 84.89s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 62%|██████▏   | 124/200 [3:56:48<2:06:31, 99.89s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 62%|██████▏   | 124/200 [3:56:49<2:06:31, 99.89s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Early stopping, best iteration is:\n",
      "[13]\ttraining's rmse: 0.821795\tvalid_1's rmse: 0.909026\n",
      "\n",
      " 62%|██████▏   | 124/200 [3:57:17<2:06:31, 99.89s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    0.9090264514002769\n",
      "\n",
      " 62%|██████▏   | 124/200 [3:57:17<2:06:31, 99.89s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.6, 'boosting': 'gbdt', 'colsample_bytree': 0.8, 'feature_fraction': 1, 'learning_rate': 0.2, 'max_depth': 8, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 400, 'objective': 'regression', 'reg_alpha': 0.4, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "\n",
      " 62%|██████▏   | 124/200 [3:57:17<2:06:31, 99.89s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 62%|██████▎   | 125/200 [3:57:17<1:38:14, 78.59s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 62%|██████▎   | 125/200 [3:57:18<1:38:14, 78.59s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    [300]\ttraining's rmse: 0.752697\tvalid_1's rmse: 0.940126\n",
      "\n",
      " 62%|██████▎   | 125/200 [3:59:05<1:38:14, 78.59s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Early stopping, best iteration is:\n",
      "[285]\ttraining's rmse: 0.756702\tvalid_1's rmse: 0.93966\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 125/200 [3:59:29<1:38:14, 78.59s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    0.9396597001275642\n",
      "\n",
      " 62%|██████▎   | 125/200 [3:59:31<1:38:14, 78.59s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.7, 'boosting': 'gbdt', 'colsample_bytree': 0.6, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 15, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 350, 'objective': 'regression', 'reg_alpha': 0.1, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "\n",
      " 62%|██████▎   | 125/200 [3:59:31<1:38:14, 78.59s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 63%|██████▎   | 126/200 [3:59:31<1:57:11, 95.02s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 63%|██████▎   | 126/200 [3:59:31<1:57:11, 95.02s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    [300]\ttraining's rmse: 0.802413\tvalid_1's rmse: 0.909017\n",
      "\n",
      " 63%|██████▎   | 126/200 [4:00:59<1:57:11, 95.02s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Early stopping, best iteration is:\n",
      "[302]\ttraining's rmse: 0.802009\tvalid_1's rmse: 0.90896\n",
      "\n",
      " 63%|██████▎   | 126/200 [4:01:24<1:57:11, 95.02s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    0.9089601563960816\n",
      "\n",
      " 63%|██████▎   | 126/200 [4:01:25<1:57:11, 95.02s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.9, 'boosting': 'gbdt', 'colsample_bytree': 1, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 150, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.3, 'subsample': 0.7}\n",
      "\n",
      " 63%|██████▎   | 126/200 [4:01:25<1:57:11, 95.02s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 64%|██████▎   | 127/200 [4:01:25<2:02:44, 100.88s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 64%|██████▎   | 127/200 [4:01:25<2:02:44, 100.88s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     [300]\ttraining's rmse: 1.07421\tvalid_1's rmse: 1.05669\n",
      "\n",
      " 64%|██████▎   | 127/200 [4:02:18<2:02:44, 100.88s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     [600]\ttraining's rmse: 1.00155\tvalid_1's rmse: 1.00836\n",
      "\n",
      " 64%|██████▎   | 127/200 [4:03:10<2:02:44, 100.88s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     [900]\ttraining's rmse: 0.955651\tvalid_1's rmse: 0.979132\n",
      "\n",
      " 64%|██████▎   | 127/200 [4:04:02<2:02:44, 100.88s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 0.944385\tvalid_1's rmse: 0.972189\n",
      "\n",
      " 64%|██████▎   | 127/200 [4:04:19<2:02:44, 100.88s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     0.9721893984292647\n",
      "\n",
      " 64%|██████▎   | 127/200 [4:04:21<2:02:44, 100.88s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     {'bagging_frequency': 0.5, 'boosting': 'gbdt', 'colsample_bytree': 0.8, 'feature_fraction': 0.8, 'learning_rate': 0.001, 'max_depth': 5, 'metric': 'rmse', 'min_child_samples': 20, 'num_leaves': 400, 'objective': 'regression', 'reg_alpha': 0.7, 'reg_lambda': 0.6, 'subsample': 0.8}\n",
      "\n",
      " 64%|██████▎   | 127/200 [4:04:21<2:02:44, 100.88s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 64%|██████▍   | 128/200 [4:04:21<2:27:52, 123.23s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 64%|██████▍   | 128/200 [4:04:21<2:27:52, 123.23s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     [300]\ttraining's rmse: 0.904421\tvalid_1's rmse: 0.956523\n",
      "\n",
      " 64%|██████▍   | 128/200 [4:04:56<2:27:52, 123.23s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     [600]\ttraining's rmse: 0.884986\tvalid_1's rmse: 0.944228\n",
      "\n",
      " 64%|██████▍   | 128/200 [4:05:32<2:27:52, 123.23s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     [900]\ttraining's rmse: 0.874875\tvalid_1's rmse: 0.937153\n",
      "\n",
      " 64%|██████▍   | 128/200 [4:06:11<2:27:52, 123.23s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 0.870976\tvalid_1's rmse: 0.934187\n",
      "\n",
      " 64%|██████▍   | 128/200 [4:06:25<2:27:52, 123.23s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     0.9341873673986649\n",
      "\n",
      " 64%|██████▍   | 128/200 [4:06:26<2:27:52, 123.23s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     {'bagging_frequency': 0.9, 'boosting': 'gbdt', 'colsample_bytree': 0.5, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 3, 'metric': 'rmse', 'min_child_samples': 40, 'num_leaves': 125, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'subsample': 1}\n",
      "\n",
      " 64%|██████▍   | 128/200 [4:06:26<2:27:52, 123.23s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 64%|██████▍   | 129/200 [4:06:26<2:26:45, 124.01s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 64%|██████▍   | 129/200 [4:06:27<2:26:45, 124.01s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     Early stopping, best iteration is:\n",
      "[68]\ttraining's rmse: 0.818099\tvalid_1's rmse: 0.91701\n",
      "\n",
      " 64%|██████▍   | 129/200 [4:06:56<2:26:45, 124.01s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     0.9170099798665534\n",
      "\n",
      " 64%|██████▍   | 129/200 [4:06:56<2:26:45, 124.01s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.6, 'learning_rate': 0.05, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 75, 'objective': 'regression', 'reg_alpha': 0.7, 'reg_lambda': 0.2, 'subsample': 0.9}\n",
      "\n",
      " 64%|██████▍   | 129/200 [4:06:56<2:26:45, 124.01s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 65%|██████▌   | 130/200 [4:06:56<1:51:39, 95.70s/trial, best loss: 0.9039042903249872] \u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 65%|██████▌   | 130/200 [4:06:56<1:51:39, 95.70s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Early stopping, best iteration is:\n",
      "[76]\ttraining's rmse: 0.823097\tvalid_1's rmse: 0.917851\n",
      "\n",
      " 65%|██████▌   | 130/200 [4:07:21<1:51:39, 95.70s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    0.9178512387127203\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 130/200 [4:07:21<1:51:39, 95.70s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.3, 'boosting': 'gbdt', 'colsample_bytree': 0.8, 'feature_fraction': 0.9, 'learning_rate': 0.1, 'max_depth': 12, 'metric': 'rmse', 'min_child_samples': 30, 'num_leaves': 25, 'objective': 'regression', 'reg_alpha': 0.6, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "\n",
      " 65%|██████▌   | 130/200 [4:07:21<1:51:39, 95.70s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 66%|██████▌   | 131/200 [4:07:21<1:25:42, 74.53s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 66%|██████▌   | 131/200 [4:07:22<1:25:42, 74.53s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    [300]\ttraining's rmse: 0.779946\tvalid_1's rmse: 0.91137\n",
      "\n",
      " 66%|██████▌   | 131/200 [4:08:59<1:25:42, 74.53s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Early stopping, best iteration is:\n",
      "[261]\ttraining's rmse: 0.789526\tvalid_1's rmse: 0.910366\n",
      "\n",
      " 66%|██████▌   | 131/200 [4:09:16<1:25:42, 74.53s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    0.9103655242678785\n",
      "\n",
      " 66%|██████▌   | 131/200 [4:09:17<1:25:42, 74.53s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.9, 'boosting': 'gbdt', 'colsample_bytree': 0.9, 'feature_fraction': 0.7, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 350, 'objective': 'regression', 'reg_alpha': 0.2, 'reg_lambda': 0.4, 'subsample': 0.5}\n",
      "\n",
      " 66%|██████▌   | 131/200 [4:09:17<1:25:42, 74.53s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 66%|██████▌   | 132/200 [4:09:17<1:38:37, 87.02s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 66%|██████▌   | 132/200 [4:09:18<1:38:37, 87.02s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Early stopping, best iteration is:\n",
      "[9]\ttraining's rmse: 0.844932\tvalid_1's rmse: 0.91881\n",
      "\n",
      " 66%|██████▌   | 132/200 [4:09:37<1:38:37, 87.02s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    0.9188096528233525\n",
      "\n",
      " 66%|██████▌   | 132/200 [4:09:37<1:38:37, 87.02s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.8, 'boosting': 'gbdt', 'colsample_bytree': 0.8, 'feature_fraction': 0.9, 'learning_rate': 0.2, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 100, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "\n",
      " 66%|██████▌   | 132/200 [4:09:37<1:38:37, 87.02s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 66%|██████▋   | 133/200 [4:09:37<1:14:38, 66.85s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 66%|██████▋   | 133/200 [4:09:38<1:14:38, 66.85s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    [300]\ttraining's rmse: 0.812772\tvalid_1's rmse: 0.909162\n",
      "\n",
      " 66%|██████▋   | 133/200 [4:11:16<1:14:38, 66.85s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Early stopping, best iteration is:\n",
      "[286]\ttraining's rmse: 0.815444\tvalid_1's rmse: 0.909103\n",
      "\n",
      " 66%|██████▋   | 133/200 [4:11:39<1:14:38, 66.85s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    0.9091032017813505\n",
      "\n",
      " 66%|██████▋   | 133/200 [4:11:40<1:14:38, 66.85s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.6, 'boosting': 'gbdt', 'colsample_bytree': 0.8, 'feature_fraction': 1, 'learning_rate': 0.01, 'max_depth': 8, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 250, 'objective': 'regression', 'reg_alpha': 0.3, 'reg_lambda': 0.1, 'subsample': 0.3}\n",
      "\n",
      " 66%|██████▋   | 133/200 [4:11:40<1:14:38, 66.85s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 67%|██████▋   | 134/200 [4:11:40<1:32:06, 83.74s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 67%|██████▋   | 134/200 [4:11:41<1:32:06, 83.74s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    [300]\ttraining's rmse: 0.829397\tvalid_1's rmse: 0.92923\n",
      "\n",
      " 67%|██████▋   | 134/200 [4:12:45<1:32:06, 83.74s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Early stopping, best iteration is:\n",
      "[426]\ttraining's rmse: 0.814159\tvalid_1's rmse: 0.927887\n",
      "\n",
      " 67%|██████▋   | 134/200 [4:13:25<1:32:06, 83.74s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    0.9278868980229346\n",
      "\n",
      " 67%|██████▋   | 134/200 [4:13:26<1:32:06, 83.74s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.9, 'boosting': 'gbdt', 'colsample_bytree': 1, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 15, 'metric': 'rmse', 'min_child_samples': 40, 'num_leaves': 50, 'objective': 'regression', 'reg_alpha': 0.1, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "\n",
      " 67%|██████▋   | 134/200 [4:13:26<1:32:06, 83.74s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 68%|██████▊   | 135/200 [4:13:26<1:37:59, 90.45s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 68%|██████▊   | 135/200 [4:13:27<1:37:59, 90.45s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    [300]\ttraining's rmse: 1.05236\tvalid_1's rmse: 1.03773\n",
      "\n",
      " 68%|██████▊   | 135/200 [4:15:17<1:37:59, 90.45s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    [600]\ttraining's rmse: 0.964854\tvalid_1's rmse: 0.979697\n",
      "\n",
      " 68%|██████▊   | 135/200 [4:17:04<1:37:59, 90.45s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    [900]\ttraining's rmse: 0.908548\tvalid_1's rmse: 0.946374\n",
      "\n",
      " 68%|██████▊   | 135/200 [4:18:49<1:37:59, 90.45s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 0.894572\tvalid_1's rmse: 0.939119\n",
      "\n",
      " 68%|██████▊   | 135/200 [4:19:22<1:37:59, 90.45s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    0.9391188552564007\n",
      "\n",
      " 68%|██████▊   | 135/200 [4:19:26<1:37:59, 90.45s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.7, 'boosting': 'gbdt', 'colsample_bytree': 0.6, 'feature_fraction': 0.8, 'learning_rate': 0.001, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 20, 'num_leaves': 225, 'objective': 'regression', 'reg_alpha': 0.4, 'reg_lambda': 0.3, 'subsample': 0.7}\n",
      "\n",
      " 68%|██████▊   | 135/200 [4:19:26<1:37:59, 90.45s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 68%|██████▊   | 136/200 [4:19:26<3:02:45, 171.33s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 68%|██████▊   | 136/200 [4:19:27<3:02:45, 171.33s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     [300]\ttraining's rmse: 0.871065\tvalid_1's rmse: 0.934215\n",
      "\n",
      " 68%|██████▊   | 136/200 [4:20:19<3:02:45, 171.33s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     [600]\ttraining's rmse: 0.850244\tvalid_1's rmse: 0.929056\n",
      "\n",
      " 68%|██████▊   | 136/200 [4:21:07<3:02:45, 171.33s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     [900]\ttraining's rmse: 0.83585\tvalid_1's rmse: 0.921444\n",
      "\n",
      " 68%|██████▊   | 136/200 [4:21:54<3:02:45, 171.33s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 0.832493\tvalid_1's rmse: 0.92038\n",
      "\n",
      " 68%|██████▊   | 136/200 [4:22:09<3:02:45, 171.33s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     0.9203800695176065\n",
      "\n",
      " 68%|██████▊   | 136/200 [4:22:11<3:02:45, 171.33s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     {'bagging_frequency': 0.9, 'boosting': 'gbdt', 'colsample_bytree': 0.8, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 5, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 400, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.6, 'subsample': 0.8}\n",
      "\n",
      " 68%|██████▊   | 136/200 [4:22:11<3:02:45, 171.33s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 68%|██████▊   | 137/200 [4:22:11<2:57:51, 169.39s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 68%|██████▊   | 137/200 [4:22:12<2:57:51, 169.39s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     [300]\ttraining's rmse: 0.864245\tvalid_1's rmse: 0.932122\n",
      "\n",
      " 68%|██████▊   | 137/200 [4:22:41<2:57:51, 169.39s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     [600]\ttraining's rmse: 0.848949\tvalid_1's rmse: 0.926918\n",
      "\n",
      " 68%|██████▊   | 137/200 [4:23:09<2:57:51, 169.39s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     Early stopping, best iteration is:\n",
      "[731]\ttraining's rmse: 0.844689\tvalid_1's rmse: 0.925832\n",
      "\n",
      " 68%|██████▊   | 137/200 [4:23:32<2:57:51, 169.39s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     0.9258324097325814\n",
      "\n",
      " 68%|██████▊   | 137/200 [4:23:33<2:57:51, 169.39s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     {'bagging_frequency': 0.5, 'boosting': 'gbdt', 'colsample_bytree': 0.5, 'feature_fraction': 0.6, 'learning_rate': 0.05, 'max_depth': 3, 'metric': 'rmse', 'min_child_samples': 30, 'num_leaves': 350, 'objective': 'regression', 'reg_alpha': 0.7, 'reg_lambda': 0.5, 'subsample': 0.9}\n",
      "\n",
      " 68%|██████▊   | 137/200 [4:23:33<2:57:51, 169.39s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 69%|██████▉   | 138/200 [4:23:33<2:27:58, 143.20s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 69%|██████▉   | 138/200 [4:23:34<2:27:58, 143.20s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     [300]\ttraining's rmse: 0.771915\tvalid_1's rmse: 0.904647\n",
      "\n",
      " 69%|██████▉   | 138/200 [4:25:47<2:27:58, 143.20s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     Early stopping, best iteration is:\n",
      "[278]\ttraining's rmse: 0.77695\tvalid_1's rmse: 0.903437\n",
      "\n",
      " 69%|██████▉   | 138/200 [4:26:17<2:27:58, 143.20s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     0.9034371353029375\n",
      "\n",
      " 69%|██████▉   | 138/200 [4:26:19<2:27:58, 143.20s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      "\u001b[A                                                                                     {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 500, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n",
      " 69%|██████▉   | 138/200 [4:26:19<2:27:58, 143.20s/trial, best loss: 0.9039042903249872]\u001b[A\n",
      " 70%|██████▉   | 139/200 [4:26:19<2:32:22, 149.87s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 70%|██████▉   | 139/200 [4:26:19<2:32:22, 149.87s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     Early stopping, best iteration is:\n",
      "[21]\ttraining's rmse: 0.797547\tvalid_1's rmse: 0.908594\n",
      "\n",
      " 70%|██████▉   | 139/200 [4:27:04<2:32:22, 149.87s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     0.9085940654159216\n",
      "\n",
      " 70%|██████▉   | 139/200 [4:27:04<2:32:22, 149.87s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.9, 'learning_rate': 0.1, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 500, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n",
      " 70%|██████▉   | 139/200 [4:27:04<2:32:22, 149.87s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      " 70%|███████   | 140/200 [4:27:04<1:58:26, 118.44s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 70%|███████   | 140/200 [4:27:05<1:58:26, 118.44s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     Early stopping, best iteration is:\n",
      "[13]\ttraining's rmse: 0.769152\tvalid_1's rmse: 0.91074\n",
      "\n",
      " 70%|███████   | 140/200 [4:27:38<1:58:26, 118.44s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     0.910740274402145\n",
      "\n",
      " 70%|███████   | 140/200 [4:27:38<1:58:26, 118.44s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.7, 'learning_rate': 0.2, 'max_depth': 12, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 500, 'objective': 'regression', 'reg_alpha': 0.7, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n",
      " 70%|███████   | 140/200 [4:27:38<1:58:26, 118.44s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      " 70%|███████   | 141/200 [4:27:38<1:31:32, 93.09s/trial, best loss: 0.9034371353029375] \u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 141/200 [4:27:39<1:31:32, 93.09s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                    [300]\ttraining's rmse: 0.777244\tvalid_1's rmse: 0.905664\n",
      "\n",
      " 70%|███████   | 141/200 [4:29:57<1:31:32, 93.09s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                    Early stopping, best iteration is:\n",
      "[276]\ttraining's rmse: 0.782683\tvalid_1's rmse: 0.905331\n",
      "\n",
      " 70%|███████   | 141/200 [4:30:27<1:31:32, 93.09s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                    0.9053314124857386\n",
      "\n",
      " 70%|███████   | 141/200 [4:30:29<1:31:32, 93.09s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 1, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 40, 'num_leaves': 500, 'objective': 'regression', 'reg_alpha': 0.2, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n",
      " 70%|███████   | 141/200 [4:30:29<1:31:32, 93.09s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      " 71%|███████   | 142/200 [4:30:29<1:52:34, 116.45s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 71%|███████   | 142/200 [4:30:29<1:52:34, 116.45s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     [300]\ttraining's rmse: 0.812408\tvalid_1's rmse: 0.908776\n",
      "\n",
      " 71%|███████   | 142/200 [4:32:11<1:52:34, 116.45s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     Early stopping, best iteration is:\n",
      "[300]\ttraining's rmse: 0.812408\tvalid_1's rmse: 0.908776\n",
      "\n",
      " 71%|███████   | 142/200 [4:32:39<1:52:34, 116.45s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     0.9087764947487333\n",
      "\n",
      " 71%|███████   | 142/200 [4:32:40<1:52:34, 116.45s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 8, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 500, 'objective': 'regression', 'reg_alpha': 0.6, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n",
      " 71%|███████   | 142/200 [4:32:40<1:52:34, 116.45s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      " 72%|███████▏  | 143/200 [4:32:40<1:54:51, 120.90s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 72%|███████▏  | 143/200 [4:32:41<1:54:51, 120.90s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     [300]\ttraining's rmse: 1.04586\tvalid_1's rmse: 1.03459\n",
      "\n",
      " 72%|███████▏  | 143/200 [4:35:10<1:54:51, 120.90s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     [600]\ttraining's rmse: 0.954558\tvalid_1's rmse: 0.975743\n",
      "\n",
      " 72%|███████▏  | 143/200 [4:37:38<1:54:51, 120.90s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     [900]\ttraining's rmse: 0.896081\tvalid_1's rmse: 0.941985\n",
      "\n",
      " 72%|███████▏  | 143/200 [4:40:00<1:54:51, 120.90s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 0.881561\tvalid_1's rmse: 0.934485\n",
      "\n",
      " 72%|███████▏  | 143/200 [4:40:47<1:54:51, 120.90s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     0.9344850160408844\n",
      "\n",
      " 72%|███████▏  | 143/200 [4:40:54<1:54:51, 120.90s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.9, 'learning_rate': 0.001, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 20, 'num_leaves': 500, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n",
      " 72%|███████▏  | 143/200 [4:40:54<1:54:51, 120.90s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      " 72%|███████▏  | 144/200 [4:40:54<3:37:14, 232.76s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 72%|███████▏  | 144/200 [4:40:55<3:37:14, 232.76s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     Early stopping, best iteration is:\n",
      "[173]\ttraining's rmse: 0.789453\tvalid_1's rmse: 0.933538\n",
      "\n",
      " 72%|███████▏  | 144/200 [4:42:42<3:37:14, 232.76s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     0.933538405557044\n",
      "\n",
      " 72%|███████▏  | 144/200 [4:42:43<3:37:14, 232.76s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.8, 'learning_rate': 0.01, 'max_depth': 15, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 500, 'objective': 'regression', 'reg_alpha': 0.4, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n",
      " 72%|███████▏  | 144/200 [4:42:43<3:37:14, 232.76s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      " 72%|███████▎  | 145/200 [4:42:43<2:59:26, 195.76s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 72%|███████▎  | 145/200 [4:42:44<2:59:26, 195.76s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     [300]\ttraining's rmse: 0.777036\tvalid_1's rmse: 0.904039\n",
      "\n",
      " 72%|███████▎  | 145/200 [4:44:47<2:59:26, 195.76s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     Early stopping, best iteration is:\n",
      "[272]\ttraining's rmse: 0.783553\tvalid_1's rmse: 0.903734\n",
      "\n",
      " 72%|███████▎  | 145/200 [4:45:12<2:59:26, 195.76s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     0.9037338937277921\n",
      "\n",
      " 72%|███████▎  | 145/200 [4:45:14<2:59:26, 195.76s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 400, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n",
      " 72%|███████▎  | 145/200 [4:45:14<2:59:26, 195.76s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      " 73%|███████▎  | 146/200 [4:45:14<2:43:53, 182.10s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 73%|███████▎  | 146/200 [4:45:14<2:43:53, 182.10s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     [300]\ttraining's rmse: 0.777036\tvalid_1's rmse: 0.904039\n",
      "\n",
      " 73%|███████▎  | 146/200 [4:47:17<2:43:53, 182.10s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     Early stopping, best iteration is:\n",
      "[272]\ttraining's rmse: 0.783553\tvalid_1's rmse: 0.903734\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 146/200 [4:47:43<2:43:53, 182.10s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     0.9037338937277921\n",
      "\n",
      " 73%|███████▎  | 146/200 [4:47:44<2:43:53, 182.10s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 400, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n",
      " 73%|███████▎  | 146/200 [4:47:44<2:43:53, 182.10s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      " 74%|███████▎  | 147/200 [4:47:44<2:32:29, 172.63s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 74%|███████▎  | 147/200 [4:47:45<2:32:29, 172.63s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     [300]\ttraining's rmse: 0.777036\tvalid_1's rmse: 0.904039\n",
      "\n",
      " 74%|███████▎  | 147/200 [4:49:48<2:32:29, 172.63s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     Early stopping, best iteration is:\n",
      "[272]\ttraining's rmse: 0.783553\tvalid_1's rmse: 0.903734\n",
      "\n",
      " 74%|███████▎  | 147/200 [4:50:13<2:32:29, 172.63s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     0.903733893727792\n",
      "\n",
      " 74%|███████▎  | 147/200 [4:50:15<2:32:29, 172.63s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 400, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n",
      " 74%|███████▎  | 147/200 [4:50:15<2:32:29, 172.63s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      " 74%|███████▍  | 148/200 [4:50:15<2:23:53, 166.02s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 74%|███████▍  | 148/200 [4:50:15<2:23:53, 166.02s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     [300]\ttraining's rmse: 0.777036\tvalid_1's rmse: 0.904039\n",
      "\n",
      " 74%|███████▍  | 148/200 [4:52:22<2:23:53, 166.02s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     Early stopping, best iteration is:\n",
      "[272]\ttraining's rmse: 0.783553\tvalid_1's rmse: 0.903734\n",
      "\n",
      " 74%|███████▍  | 148/200 [4:52:47<2:23:53, 166.02s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     0.9037338937277921\n",
      "\n",
      " 74%|███████▍  | 148/200 [4:52:48<2:23:53, 166.02s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 400, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n",
      " 74%|███████▍  | 148/200 [4:52:48<2:23:53, 166.02s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      " 74%|███████▍  | 149/200 [4:52:48<2:17:58, 162.32s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 74%|███████▍  | 149/200 [4:52:49<2:17:58, 162.32s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     [300]\ttraining's rmse: 0.777036\tvalid_1's rmse: 0.904039\n",
      "\n",
      " 74%|███████▍  | 149/200 [4:55:01<2:17:58, 162.32s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     Early stopping, best iteration is:\n",
      "[272]\ttraining's rmse: 0.783553\tvalid_1's rmse: 0.903734\n",
      "\n",
      " 74%|███████▍  | 149/200 [4:55:27<2:17:58, 162.32s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     0.9037338937277921\n",
      "\n",
      " 74%|███████▍  | 149/200 [4:55:28<2:17:58, 162.32s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 400, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n",
      " 74%|███████▍  | 149/200 [4:55:28<2:17:58, 162.32s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      " 75%|███████▌  | 150/200 [4:55:28<2:14:36, 161.53s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 75%|███████▌  | 150/200 [4:55:29<2:14:36, 161.53s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     [300]\ttraining's rmse: 0.777036\tvalid_1's rmse: 0.904039\n",
      "\n",
      " 75%|███████▌  | 150/200 [4:57:41<2:14:36, 161.53s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     Early stopping, best iteration is:\n",
      "[272]\ttraining's rmse: 0.783553\tvalid_1's rmse: 0.903734\n",
      "\n",
      " 75%|███████▌  | 150/200 [4:58:06<2:14:36, 161.53s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     0.9037338937277921\n",
      "\n",
      " 75%|███████▌  | 150/200 [4:58:08<2:14:36, 161.53s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 400, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n",
      " 75%|███████▌  | 150/200 [4:58:08<2:14:36, 161.53s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      " 76%|███████▌  | 151/200 [4:58:08<2:11:24, 160.92s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 76%|███████▌  | 151/200 [4:58:08<2:11:24, 160.92s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     [300]\ttraining's rmse: 0.777036\tvalid_1's rmse: 0.904039\n",
      "\n",
      " 76%|███████▌  | 151/200 [5:00:12<2:11:24, 160.92s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     Early stopping, best iteration is:\n",
      "[272]\ttraining's rmse: 0.783553\tvalid_1's rmse: 0.903734\n",
      "\n",
      " 76%|███████▌  | 151/200 [5:00:38<2:11:24, 160.92s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     0.903733893727792\n",
      "\n",
      " 76%|███████▌  | 151/200 [5:00:39<2:11:24, 160.92s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 400, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n",
      " 76%|███████▌  | 151/200 [5:00:39<2:11:24, 160.92s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      " 76%|███████▌  | 152/200 [5:00:39<2:06:28, 158.10s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 76%|███████▌  | 152/200 [5:00:40<2:06:28, 158.10s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     [300]\ttraining's rmse: 0.777036\tvalid_1's rmse: 0.904039\n",
      "\n",
      " 76%|███████▌  | 152/200 [5:02:48<2:06:28, 158.10s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     Early stopping, best iteration is:\n",
      "[272]\ttraining's rmse: 0.783553\tvalid_1's rmse: 0.903734\n",
      "\n",
      " 76%|███████▌  | 152/200 [5:03:14<2:06:28, 158.10s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     0.903733893727792\n",
      "\n",
      " 76%|███████▌  | 152/200 [5:03:15<2:06:28, 158.10s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 400, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n",
      " 76%|███████▌  | 152/200 [5:03:15<2:06:28, 158.10s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      " 76%|███████▋  | 153/200 [5:03:15<2:03:21, 157.48s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 76%|███████▋  | 153/200 [5:03:16<2:03:21, 157.48s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     [300]\ttraining's rmse: 0.771915\tvalid_1's rmse: 0.904647\n",
      "\n",
      " 76%|███████▋  | 153/200 [5:05:35<2:03:21, 157.48s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     Early stopping, best iteration is:\n",
      "[278]\ttraining's rmse: 0.77695\tvalid_1's rmse: 0.903437\n",
      "\n",
      " 76%|███████▋  | 153/200 [5:06:10<2:03:21, 157.48s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     0.9034371353029375\n",
      "\n",
      " 76%|███████▋  | 153/200 [5:06:12<2:03:21, 157.48s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 500, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n",
      " 76%|███████▋  | 153/200 [5:06:12<2:03:21, 157.48s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      " 77%|███████▋  | 154/200 [5:06:12<2:05:09, 163.25s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 77%|███████▋  | 154/200 [5:06:13<2:05:09, 163.25s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     [300]\ttraining's rmse: 0.771915\tvalid_1's rmse: 0.904647\n",
      "\n",
      " 77%|███████▋  | 154/200 [5:08:31<2:05:09, 163.25s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     Early stopping, best iteration is:\n",
      "[278]\ttraining's rmse: 0.77695\tvalid_1's rmse: 0.903437\n",
      "\n",
      " 77%|███████▋  | 154/200 [5:09:00<2:05:09, 163.25s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     0.9034371353029375\n",
      "\n",
      " 77%|███████▋  | 154/200 [5:09:02<2:05:09, 163.25s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 500, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n",
      " 77%|███████▋  | 154/200 [5:09:02<2:05:09, 163.25s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      " 78%|███████▊  | 155/200 [5:09:02<2:03:56, 165.25s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 78%|███████▊  | 155/200 [5:09:02<2:03:56, 165.25s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     [300]\ttraining's rmse: 0.771915\tvalid_1's rmse: 0.904647\n",
      "\n",
      " 78%|███████▊  | 155/200 [5:11:09<2:03:56, 165.25s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     Early stopping, best iteration is:\n",
      "[278]\ttraining's rmse: 0.77695\tvalid_1's rmse: 0.903437\n",
      "\n",
      " 78%|███████▊  | 155/200 [5:11:38<2:03:56, 165.25s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     0.9034371353029375\n",
      "\n",
      " 78%|███████▊  | 155/200 [5:11:39<2:03:56, 165.25s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 500, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n",
      " 78%|███████▊  | 155/200 [5:11:39<2:03:56, 165.25s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      " 78%|███████▊  | 156/200 [5:11:39<1:59:31, 162.98s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 78%|███████▊  | 156/200 [5:11:40<1:59:31, 162.98s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     [300]\ttraining's rmse: 0.771915\tvalid_1's rmse: 0.904647\n",
      "\n",
      " 78%|███████▊  | 156/200 [5:13:47<1:59:31, 162.98s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     Early stopping, best iteration is:\n",
      "[278]\ttraining's rmse: 0.77695\tvalid_1's rmse: 0.903437\n",
      "\n",
      " 78%|███████▊  | 156/200 [5:14:16<1:59:31, 162.98s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     0.9034371353029375\n",
      "\n",
      " 78%|███████▊  | 156/200 [5:14:18<1:59:31, 162.98s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 500, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n",
      " 78%|███████▊  | 156/200 [5:14:18<1:59:31, 162.98s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      " 78%|███████▊  | 157/200 [5:14:18<1:55:45, 161.51s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 78%|███████▊  | 157/200 [5:14:18<1:55:45, 161.51s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     [300]\ttraining's rmse: 0.819453\tvalid_1's rmse: 0.919464\n",
      "\n",
      " 78%|███████▊  | 157/200 [5:15:05<1:55:45, 161.51s/trial, best loss: 0.9034371353029375]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[AEarly stopping, best iteration is:                                                   \n",
      "[295]\ttraining's rmse: 0.819992\tvalid_1's rmse: 0.91911\n",
      "\n",
      " 78%|███████▊  | 157/200 [5:15:19<1:55:45, 161.51s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     0.9191095479021328\n",
      "\n",
      " 78%|███████▊  | 157/200 [5:15:20<1:55:45, 161.51s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.9, 'learning_rate': 0.05, 'max_depth': 5, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 500, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n",
      " 78%|███████▊  | 157/200 [5:15:20<1:55:45, 161.51s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      " 79%|███████▉  | 158/200 [5:15:20<1:32:10, 131.69s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 79%|███████▉  | 158/200 [5:15:20<1:32:10, 131.69s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     [300]\ttraining's rmse: 0.904757\tvalid_1's rmse: 0.955876\n",
      "\n",
      " 79%|███████▉  | 158/200 [5:15:49<1:32:10, 131.69s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     [600]\ttraining's rmse: 0.886443\tvalid_1's rmse: 0.945547\n",
      "\n",
      " 79%|███████▉  | 158/200 [5:16:19<1:32:10, 131.69s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     [900]\ttraining's rmse: 0.876968\tvalid_1's rmse: 0.940129\n",
      "\n",
      " 79%|███████▉  | 158/200 [5:16:48<1:32:10, 131.69s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 0.874777\tvalid_1's rmse: 0.938574\n",
      "\n",
      " 79%|███████▉  | 158/200 [5:16:58<1:32:10, 131.69s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     0.938574436325182\n",
      "\n",
      " 79%|███████▉  | 158/200 [5:16:59<1:32:10, 131.69s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.6, 'learning_rate': 0.01, 'max_depth': 3, 'metric': 'rmse', 'min_child_samples': 30, 'num_leaves': 500, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n",
      " 79%|███████▉  | 158/200 [5:16:59<1:32:10, 131.69s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      " 80%|███████▉  | 159/200 [5:16:59<1:23:21, 122.00s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 80%|███████▉  | 159/200 [5:17:00<1:23:21, 122.00s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     Early stopping, best iteration is:\n",
      "[21]\ttraining's rmse: 0.797547\tvalid_1's rmse: 0.908594\n",
      "\n",
      " 80%|███████▉  | 159/200 [5:17:43<1:23:21, 122.00s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     0.9085940654159216\n",
      "\n",
      " 80%|███████▉  | 159/200 [5:17:43<1:23:21, 122.00s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.9, 'learning_rate': 0.1, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 500, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n",
      " 80%|███████▉  | 159/200 [5:17:43<1:23:21, 122.00s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      " 80%|████████  | 160/200 [5:17:43<1:05:41, 98.53s/trial, best loss: 0.9034371353029375] \u001b[A\n",
      "\u001b[A                                                                                    Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 80%|████████  | 160/200 [5:17:43<1:05:41, 98.53s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                    [300]\ttraining's rmse: 0.771424\tvalid_1's rmse: 0.908778\n",
      "\n",
      " 80%|████████  | 160/200 [5:19:34<1:05:41, 98.53s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                    Early stopping, best iteration is:\n",
      "[257]\ttraining's rmse: 0.782763\tvalid_1's rmse: 0.907033\n",
      "\n",
      " 80%|████████  | 160/200 [5:19:54<1:05:41, 98.53s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                    0.9070326698567724\n",
      "\n",
      " 80%|████████  | 160/200 [5:19:55<1:05:41, 98.53s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                    {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.7, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 500, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n",
      " 80%|████████  | 160/200 [5:19:55<1:05:41, 98.53s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      " 80%|████████  | 161/200 [5:19:55<1:10:35, 108.60s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 80%|████████  | 161/200 [5:19:55<1:10:35, 108.60s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     Early stopping, best iteration is:\n",
      "[11]\ttraining's rmse: 0.793064\tvalid_1's rmse: 0.910713\n",
      "\n",
      " 80%|████████  | 161/200 [5:20:32<1:10:35, 108.60s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     0.9107126346078661\n",
      "\n",
      " 80%|████████  | 161/200 [5:20:32<1:10:35, 108.60s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.9, 'learning_rate': 0.2, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 500, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n",
      " 80%|████████  | 161/200 [5:20:32<1:10:35, 108.60s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      " 81%|████████  | 162/200 [5:20:32<55:14, 87.22s/trial, best loss: 0.9034371353029375]   \u001b[A\n",
      "\u001b[A                                                                                  Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 81%|████████  | 162/200 [5:20:33<55:14, 87.22s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                  [300]\ttraining's rmse: 0.756292\tvalid_1's rmse: 0.928329\n",
      "\n",
      " 81%|████████  | 162/200 [5:22:49<55:14, 87.22s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                  Early stopping, best iteration is:\n",
      "[239]\ttraining's rmse: 0.774243\tvalid_1's rmse: 0.927504\n",
      "\n",
      " 81%|████████  | 162/200 [5:23:04<55:14, 87.22s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                  0.9275037682648777\n",
      "\n",
      " 81%|████████  | 162/200 [5:23:05<55:14, 87.22s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                  {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 1, 'learning_rate': 0.01, 'max_depth': 12, 'metric': 'rmse', 'min_child_samples': 40, 'num_leaves': 500, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 162/200 [5:23:05<55:14, 87.22s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      " 82%|████████▏ | 163/200 [5:23:05<1:05:59, 107.03s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 82%|████████▏ | 163/200 [5:23:06<1:05:59, 107.03s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     [300]\ttraining's rmse: 0.812264\tvalid_1's rmse: 0.908377\n",
      "\n",
      " 82%|████████▏ | 163/200 [5:24:47<1:05:59, 107.03s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     Early stopping, best iteration is:\n",
      "[302]\ttraining's rmse: 0.811886\tvalid_1's rmse: 0.908316\n",
      "\n",
      " 82%|████████▏ | 163/200 [5:25:16<1:05:59, 107.03s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     0.9083155407214053\n",
      "\n",
      " 82%|████████▏ | 163/200 [5:25:17<1:05:59, 107.03s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 8, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 500, 'objective': 'regression', 'reg_alpha': 0.3, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n",
      " 82%|████████▏ | 163/200 [5:25:17<1:05:59, 107.03s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      " 82%|████████▏ | 164/200 [5:25:17<1:08:41, 114.49s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 82%|████████▏ | 164/200 [5:25:18<1:08:41, 114.49s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     [300]\ttraining's rmse: 1.04586\tvalid_1's rmse: 1.03459\n",
      "\n",
      " 82%|████████▏ | 164/200 [5:27:48<1:08:41, 114.49s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     [600]\ttraining's rmse: 0.954558\tvalid_1's rmse: 0.975743\n",
      "\n",
      " 82%|████████▏ | 164/200 [5:30:24<1:08:41, 114.49s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     [900]\ttraining's rmse: 0.896081\tvalid_1's rmse: 0.941985\n",
      "\n",
      " 82%|████████▏ | 164/200 [5:32:54<1:08:41, 114.49s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 0.881561\tvalid_1's rmse: 0.934485\n",
      "\n",
      " 82%|████████▏ | 164/200 [5:33:43<1:08:41, 114.49s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     0.9344850160408844\n",
      "\n",
      " 82%|████████▏ | 164/200 [5:33:50<1:08:41, 114.49s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.9, 'learning_rate': 0.001, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 20, 'num_leaves': 500, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n",
      " 82%|████████▏ | 164/200 [5:33:50<1:08:41, 114.49s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      " 82%|████████▎ | 165/200 [5:33:50<2:16:23, 233.80s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 82%|████████▎ | 165/200 [5:33:50<2:16:23, 233.80s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     Early stopping, best iteration is:\n",
      "[165]\ttraining's rmse: 0.794571\tvalid_1's rmse: 0.937298\n",
      "\n",
      " 82%|████████▎ | 165/200 [5:35:43<2:16:23, 233.80s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     0.9372980533084615\n",
      "\n",
      " 82%|████████▎ | 165/200 [5:35:44<2:16:23, 233.80s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.8, 'learning_rate': 0.01, 'max_depth': 15, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 500, 'objective': 'regression', 'reg_alpha': 0.1, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n",
      " 82%|████████▎ | 165/200 [5:35:44<2:16:23, 233.80s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      " 83%|████████▎ | 166/200 [5:35:44<1:52:14, 198.08s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 83%|████████▎ | 166/200 [5:35:45<1:52:14, 198.08s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     Early stopping, best iteration is:\n",
      "[45]\ttraining's rmse: 0.795227\tvalid_1's rmse: 0.903714\n",
      "\n",
      " 83%|████████▎ | 166/200 [5:36:43<1:52:14, 198.08s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     0.9037137080177916\n",
      "\n",
      " 83%|████████▎ | 166/200 [5:36:43<1:52:14, 198.08s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.9, 'learning_rate': 0.05, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 30, 'num_leaves': 500, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n",
      " 83%|████████▎ | 166/200 [5:36:43<1:52:14, 198.08s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      " 84%|████████▎ | 167/200 [5:36:43<1:26:00, 156.37s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 84%|████████▎ | 167/200 [5:36:44<1:26:00, 156.37s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     [300]\ttraining's rmse: 0.869233\tvalid_1's rmse: 0.930816\n",
      "\n",
      " 84%|████████▎ | 167/200 [5:37:31<1:26:00, 156.37s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     [600]\ttraining's rmse: 0.849279\tvalid_1's rmse: 0.924883\n",
      "\n",
      " 84%|████████▎ | 167/200 [5:38:15<1:26:00, 156.37s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     [900]\ttraining's rmse: 0.835858\tvalid_1's rmse: 0.918934\n",
      "\n",
      " 84%|████████▎ | 167/200 [5:38:56<1:26:00, 156.37s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 0.832418\tvalid_1's rmse: 0.917355\n",
      "\n",
      " 84%|████████▎ | 167/200 [5:39:10<1:26:00, 156.37s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     0.9173549306539427\n",
      "\n",
      " 84%|████████▎ | 167/200 [5:39:12<1:26:00, 156.37s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.6, 'learning_rate': 0.01, 'max_depth': 5, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 500, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n",
      " 84%|████████▎ | 167/200 [5:39:12<1:26:00, 156.37s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      " 84%|████████▍ | 168/200 [5:39:12<1:22:12, 154.13s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 84%|████████▍ | 168/200 [5:39:12<1:22:12, 154.13s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     [300]\ttraining's rmse: 0.847031\tvalid_1's rmse: 0.924335\n",
      "\n",
      " 84%|████████▍ | 168/200 [5:39:51<1:22:12, 154.13s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     [600]\ttraining's rmse: 0.831786\tvalid_1's rmse: 0.92259\n",
      "\n",
      " 84%|████████▍ | 168/200 [5:40:31<1:22:12, 154.13s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     Early stopping, best iteration is:\n",
      "[621]\ttraining's rmse: 0.830525\tvalid_1's rmse: 0.92223\n",
      "\n",
      " 84%|████████▍ | 168/200 [5:40:48<1:22:12, 154.13s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     0.9222298261163078\n",
      "\n",
      " 84%|████████▍ | 168/200 [5:40:49<1:22:12, 154.13s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.9, 'learning_rate': 0.1, 'max_depth': 3, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 500, 'objective': 'regression', 'reg_alpha': 0.2, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n",
      " 84%|████████▍ | 168/200 [5:40:49<1:22:12, 154.13s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      " 84%|████████▍ | 169/200 [5:40:49<1:10:40, 136.78s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 84%|████████▍ | 169/200 [5:40:50<1:10:40, 136.78s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     [300]\ttraining's rmse: 0.775925\tvalid_1's rmse: 0.907332\n",
      "\n",
      " 84%|████████▍ | 169/200 [5:43:01<1:10:40, 136.78s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     Early stopping, best iteration is:\n",
      "[260]\ttraining's rmse: 0.785958\tvalid_1's rmse: 0.906214\n",
      "\n",
      " 84%|████████▍ | 169/200 [5:43:21<1:10:40, 136.78s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     0.9062138820417652\n",
      "\n",
      " 84%|████████▍ | 169/200 [5:43:23<1:10:40, 136.78s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.7, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 40, 'num_leaves': 500, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n",
      " 84%|████████▍ | 169/200 [5:43:23<1:10:40, 136.78s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      " 85%|████████▌ | 170/200 [5:43:23<1:11:00, 142.03s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 85%|████████▌ | 170/200 [5:43:23<1:11:00, 142.03s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     Early stopping, best iteration is:\n",
      "[13]\ttraining's rmse: 0.759968\tvalid_1's rmse: 0.937921\n",
      "\n",
      " 85%|████████▌ | 170/200 [5:44:00<1:11:00, 142.03s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     0.9379206948817074\n",
      "\n",
      " 85%|████████▌ | 170/200 [5:44:00<1:11:00, 142.03s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                     {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 1, 'learning_rate': 0.2, 'max_depth': 12, 'metric': 'rmse', 'min_child_samples': 10, 'num_leaves': 500, 'objective': 'regression', 'reg_alpha': 0.3, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n",
      " 85%|████████▌ | 170/200 [5:44:00<1:11:00, 142.03s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      " 86%|████████▌ | 171/200 [5:44:00<53:29, 110.66s/trial, best loss: 0.9034371353029375]  \u001b[A\n",
      "\u001b[A                                                                                   Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 86%|████████▌ | 171/200 [5:44:01<53:29, 110.66s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                   [300]\ttraining's rmse: 0.77353\tvalid_1's rmse: 0.903285\n",
      "\n",
      " 86%|████████▌ | 171/200 [5:46:10<53:29, 110.66s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                   Early stopping, best iteration is:\n",
      "[278]\ttraining's rmse: 0.77851\tvalid_1's rmse: 0.902661\n",
      "\n",
      " 86%|████████▌ | 171/200 [5:46:39<53:29, 110.66s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                   0.9026607656143669\n",
      "\n",
      " 86%|████████▌ | 171/200 [5:46:41<53:29, 110.66s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      "\u001b[A                                                                                   {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 20, 'num_leaves': 500, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n",
      " 86%|████████▌ | 171/200 [5:46:41<53:29, 110.66s/trial, best loss: 0.9034371353029375]\u001b[A\n",
      " 86%|████████▌ | 172/200 [5:46:41<58:35, 125.57s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 86%|████████▌ | 172/200 [5:46:41<58:35, 125.57s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   [300]\ttraining's rmse: 0.802755\tvalid_1's rmse: 0.908524\n",
      "\n",
      " 86%|████████▌ | 172/200 [5:48:09<58:35, 125.57s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   Early stopping, best iteration is:\n",
      "[314]\ttraining's rmse: 0.800227\tvalid_1's rmse: 0.908459\n",
      "\n",
      " 86%|████████▌ | 172/200 [5:48:37<58:35, 125.57s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   0.9084590410249588\n",
      "\n",
      " 86%|████████▌ | 172/200 [5:48:38<58:35, 125.57s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   {'bagging_frequency': 0.3, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 20, 'num_leaves': 150, 'objective': 'regression', 'reg_alpha': 0.6, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n",
      " 86%|████████▌ | 172/200 [5:48:38<58:35, 125.57s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      " 86%|████████▋ | 173/200 [5:48:38<55:25, 123.15s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 86%|████████▋ | 173/200 [5:48:39<55:25, 123.15s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   [300]\ttraining's rmse: 1.05951\tvalid_1's rmse: 1.04198\n",
      "\n",
      " 86%|████████▋ | 173/200 [5:50:11<55:25, 123.15s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   [600]\ttraining's rmse: 0.977333\tvalid_1's rmse: 0.985407\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 86%|████████▋ | 173/200 [5:51:41<55:25, 123.15s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   [900]\ttraining's rmse: 0.924936\tvalid_1's rmse: 0.952141\n",
      "\n",
      " 86%|████████▋ | 173/200 [5:53:08<55:25, 123.15s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 0.912037\tvalid_1's rmse: 0.944502\n",
      "\n",
      " 86%|████████▋ | 173/200 [5:53:36<55:25, 123.15s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   0.9445019784902343\n",
      "\n",
      " 86%|████████▋ | 173/200 [5:53:39<55:25, 123.15s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.8, 'learning_rate': 0.001, 'max_depth': 8, 'metric': 'rmse', 'min_child_samples': 20, 'num_leaves': 125, 'objective': 'regression', 'reg_alpha': 0.4, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n",
      " 86%|████████▋ | 173/200 [5:53:39<55:25, 123.15s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      " 87%|████████▋ | 174/200 [5:53:39<1:16:25, 176.37s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                     Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 87%|████████▋ | 174/200 [5:53:39<1:16:25, 176.37s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                     [300]\ttraining's rmse: 0.738941\tvalid_1's rmse: 0.936604\n",
      "\n",
      " 87%|████████▋ | 174/200 [5:55:39<1:16:25, 176.37s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                     Early stopping, best iteration is:\n",
      "[305]\ttraining's rmse: 0.737654\tvalid_1's rmse: 0.936457\n",
      "\n",
      " 87%|████████▋ | 174/200 [5:56:17<1:16:25, 176.37s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                     0.9364565032782977\n",
      "\n",
      " 87%|████████▋ | 174/200 [5:56:18<1:16:25, 176.37s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                     {'bagging_frequency': 0.8, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 15, 'metric': 'rmse', 'min_child_samples': 20, 'num_leaves': 500, 'objective': 'regression', 'reg_alpha': 0.1, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n",
      " 87%|████████▋ | 174/200 [5:56:18<1:16:25, 176.37s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      " 88%|████████▊ | 175/200 [5:56:18<1:11:22, 171.31s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                     Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 88%|████████▊ | 175/200 [5:56:19<1:11:22, 171.31s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                     Early stopping, best iteration is:\n",
      "[81]\ttraining's rmse: 0.81094\tvalid_1's rmse: 0.912145\n",
      "\n",
      " 88%|████████▊ | 175/200 [5:56:50<1:11:22, 171.31s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                     0.9121452546801647\n",
      "\n",
      " 88%|████████▊ | 175/200 [5:56:50<1:11:22, 171.31s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                     {'bagging_frequency': 0.7, 'boosting': 'gbdt', 'colsample_bytree': 0.9, 'feature_fraction': 0.6, 'learning_rate': 0.05, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 20, 'num_leaves': 75, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.4, 'subsample': 0.5}\n",
      "\n",
      " 88%|████████▊ | 175/200 [5:56:50<1:11:22, 171.31s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      " 88%|████████▊ | 176/200 [5:56:50<51:45, 129.39s/trial, best loss: 0.9026607656143669]  \u001b[A\n",
      "\u001b[A                                                                                   Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 88%|████████▊ | 176/200 [5:56:50<51:45, 129.39s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   [300]\ttraining's rmse: 0.801137\tvalid_1's rmse: 0.923261\n",
      "\n",
      " 88%|████████▊ | 176/200 [5:57:34<51:45, 129.39s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   Early stopping, best iteration is:\n",
      "[225]\ttraining's rmse: 0.809647\tvalid_1's rmse: 0.919427\n",
      "\n",
      " 88%|████████▊ | 176/200 [5:57:37<51:45, 129.39s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   0.9194265113323143\n",
      "\n",
      " 88%|████████▊ | 176/200 [5:57:38<51:45, 129.39s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   {'bagging_frequency': 0.6, 'boosting': 'gbdt', 'colsample_bytree': 0.6, 'feature_fraction': 0.9, 'learning_rate': 0.1, 'max_depth': 5, 'metric': 'rmse', 'min_child_samples': 20, 'num_leaves': 25, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.1, 'subsample': 0.3}\n",
      "\n",
      " 88%|████████▊ | 176/200 [5:57:38<51:45, 129.39s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      " 88%|████████▊ | 177/200 [5:57:38<40:14, 104.99s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 88%|████████▊ | 177/200 [5:57:38<40:14, 104.99s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   [300]\ttraining's rmse: 0.813715\tvalid_1's rmse: 0.912844\n",
      "\n",
      " 88%|████████▊ | 177/200 [5:58:57<40:14, 104.99s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   Early stopping, best iteration is:\n",
      "[316]\ttraining's rmse: 0.810963\tvalid_1's rmse: 0.912231\n",
      "\n",
      " 88%|████████▊ | 177/200 [5:59:23<40:14, 104.99s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   0.912231414418913\n",
      "\n",
      " 88%|████████▊ | 177/200 [5:59:24<40:14, 104.99s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 20, 'num_leaves': 100, 'objective': 'regression', 'reg_alpha': 0.2, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n",
      " 88%|████████▊ | 177/200 [5:59:24<40:14, 104.99s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      " 89%|████████▉ | 178/200 [5:59:24<38:38, 105.40s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 89%|████████▉ | 178/200 [5:59:24<38:38, 105.40s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   [300]\ttraining's rmse: 0.833301\tvalid_1's rmse: 0.928728\n",
      "\n",
      " 89%|████████▉ | 178/200 [5:59:56<38:38, 105.40s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   Early stopping, best iteration is:\n",
      "[248]\ttraining's rmse: 0.836811\tvalid_1's rmse: 0.924599\n",
      "\n",
      " 89%|████████▉ | 178/200 [6:00:01<38:38, 105.40s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   0.9245988205241726\n",
      "\n",
      " 89%|████████▉ | 178/200 [6:00:01<38:38, 105.40s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   {'bagging_frequency': 0.5, 'boosting': 'gbdt', 'colsample_bytree': 1, 'feature_fraction': 0.7, 'learning_rate': 0.2, 'max_depth': 3, 'metric': 'rmse', 'min_child_samples': 20, 'num_leaves': 500, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.3, 'subsample': 0.7}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 178/200 [6:00:01<38:38, 105.40s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      " 90%|████████▉ | 179/200 [6:00:01<29:40, 84.80s/trial, best loss: 0.9026607656143669] \u001b[A\n",
      "\u001b[A                                                                                  Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 90%|████████▉ | 179/200 [6:00:01<29:40, 84.80s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                  [300]\ttraining's rmse: 0.789024\tvalid_1's rmse: 0.907364\n",
      "\n",
      " 90%|████████▉ | 179/200 [6:01:52<29:40, 84.80s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                  Early stopping, best iteration is:\n",
      "[249]\ttraining's rmse: 0.802169\tvalid_1's rmse: 0.90615\n",
      "\n",
      " 90%|████████▉ | 179/200 [6:02:08<29:40, 84.80s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                  0.9061495806666892\n",
      "\n",
      " 90%|████████▉ | 179/200 [6:02:09<29:40, 84.80s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                  {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.5, 'feature_fraction': 1, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 20, 'num_leaves': 250, 'objective': 'regression', 'reg_alpha': 0.6, 'reg_lambda': 0.2, 'subsample': 0.8}\n",
      "\n",
      " 90%|████████▉ | 179/200 [6:02:09<29:40, 84.80s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      " 90%|█████████ | 180/200 [6:02:09<32:32, 97.64s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                  Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 90%|█████████ | 180/200 [6:02:09<32:32, 97.64s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                  [300]\ttraining's rmse: 0.833684\tvalid_1's rmse: 0.919249\n",
      "\n",
      " 90%|█████████ | 180/200 [6:03:19<32:32, 97.64s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                  Early stopping, best iteration is:\n",
      "[337]\ttraining's rmse: 0.828482\tvalid_1's rmse: 0.918483\n",
      "\n",
      " 90%|█████████ | 180/200 [6:03:46<32:32, 97.64s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                  0.9184827366284939\n",
      "\n",
      " 90%|█████████ | 180/200 [6:03:46<32:32, 97.64s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                  {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 20, 'num_leaves': 50, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.6, 'subsample': 1}\n",
      "\n",
      " 90%|█████████ | 180/200 [6:03:46<32:32, 97.64s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      " 90%|█████████ | 181/200 [6:03:46<30:55, 97.68s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                  Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 90%|█████████ | 181/200 [6:03:47<30:55, 97.68s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                  [300]\ttraining's rmse: 0.778669\tvalid_1's rmse: 0.923464\n",
      "\n",
      " 90%|█████████ | 181/200 [6:05:27<30:55, 97.68s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                  Early stopping, best iteration is:\n",
      "[313]\ttraining's rmse: 0.775943\tvalid_1's rmse: 0.923321\n",
      "\n",
      " 90%|█████████ | 181/200 [6:06:00<30:55, 97.68s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                  0.9233214227475839\n",
      "\n",
      " 90%|█████████ | 181/200 [6:06:01<30:55, 97.68s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                  {'bagging_frequency': 0.3, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 12, 'metric': 'rmse', 'min_child_samples': 20, 'num_leaves': 225, 'objective': 'regression', 'reg_alpha': 0.3, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n",
      " 90%|█████████ | 181/200 [6:06:01<30:55, 97.68s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      " 91%|█████████ | 182/200 [6:06:01<32:38, 108.80s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 91%|█████████ | 182/200 [6:06:02<32:38, 108.80s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   [300]\ttraining's rmse: 1.04745\tvalid_1's rmse: 1.03513\n",
      "\n",
      " 91%|█████████ | 182/200 [6:08:26<32:38, 108.80s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   [600]\ttraining's rmse: 0.956303\tvalid_1's rmse: 0.975809\n",
      "\n",
      " 91%|█████████ | 182/200 [6:10:48<32:38, 108.80s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   [900]\ttraining's rmse: 0.89739\tvalid_1's rmse: 0.941924\n",
      "\n",
      " 91%|█████████ | 182/200 [6:13:05<32:38, 108.80s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 0.882688\tvalid_1's rmse: 0.934519\n",
      "\n",
      " 91%|█████████ | 182/200 [6:13:50<32:38, 108.80s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   0.9345192753930208\n",
      "\n",
      " 91%|█████████ | 182/200 [6:13:57<32:38, 108.80s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.8, 'learning_rate': 0.001, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 20, 'num_leaves': 500, 'objective': 'regression', 'reg_alpha': 0.4, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n",
      " 91%|█████████ | 182/200 [6:13:57<32:38, 108.80s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      " 92%|█████████▏| 183/200 [6:13:57<1:02:00, 218.88s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                     Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 92%|█████████▏| 183/200 [6:13:57<1:02:00, 218.88s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                     [300]\ttraining's rmse: 0.813827\tvalid_1's rmse: 0.907826\n",
      "\n",
      " 92%|█████████▏| 183/200 [6:15:39<1:02:00, 218.88s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                     Early stopping, best iteration is:\n",
      "[325]\ttraining's rmse: 0.809679\tvalid_1's rmse: 0.907579\n",
      "\n",
      " 92%|█████████▏| 183/200 [6:16:14<1:02:00, 218.88s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                     0.9075789362387914\n",
      "\n",
      " 92%|█████████▏| 183/200 [6:16:15<1:02:00, 218.88s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                     {'bagging_frequency': 0.8, 'boosting': 'gbdt', 'colsample_bytree': 0.9, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 8, 'metric': 'rmse', 'min_child_samples': 20, 'num_leaves': 500, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.4, 'subsample': 0.5}\n",
      "\n",
      " 92%|█████████▏| 183/200 [6:16:15<1:02:00, 218.88s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      " 92%|█████████▏| 184/200 [6:16:15<51:56, 194.77s/trial, best loss: 0.9026607656143669]  \u001b[A\n",
      "\u001b[A                                                                                   Training until validation scores don't improve for 100 rounds\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 184/200 [6:16:16<51:56, 194.77s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   Early stopping, best iteration is:\n",
      "[34]\ttraining's rmse: 0.837077\tvalid_1's rmse: 0.930838\n",
      "\n",
      " 92%|█████████▏| 184/200 [6:16:45<51:56, 194.77s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   0.9308378486449318\n",
      "\n",
      " 92%|█████████▏| 184/200 [6:16:46<51:56, 194.77s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   {'bagging_frequency': 0.6, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.6, 'learning_rate': 0.05, 'max_depth': 15, 'metric': 'rmse', 'min_child_samples': 20, 'num_leaves': 150, 'objective': 'regression', 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'subsample': 1}\n",
      "\n",
      " 92%|█████████▏| 184/200 [6:16:46<51:56, 194.77s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      " 92%|█████████▎| 185/200 [6:16:46<36:21, 145.41s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 92%|█████████▎| 185/200 [6:16:46<36:21, 145.41s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   [300]\ttraining's rmse: 0.774704\tvalid_1's rmse: 0.906049\n",
      "\n",
      " 92%|█████████▎| 185/200 [6:19:02<36:21, 145.41s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   Early stopping, best iteration is:\n",
      "[243]\ttraining's rmse: 0.789761\tvalid_1's rmse: 0.903582\n",
      "\n",
      " 92%|█████████▎| 185/200 [6:19:20<36:21, 145.41s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   0.9035819963821956\n",
      "\n",
      " 92%|█████████▎| 185/200 [6:19:21<36:21, 145.41s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.6, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 30, 'num_leaves': 500, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.2, 'subsample': 0.3}\n",
      "\n",
      " 92%|█████████▎| 185/200 [6:19:21<36:21, 145.41s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      " 93%|█████████▎| 186/200 [6:19:21<34:38, 148.46s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 93%|█████████▎| 186/200 [6:19:21<34:38, 148.46s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   Early stopping, best iteration is:\n",
      "[186]\ttraining's rmse: 0.813935\tvalid_1's rmse: 0.918299\n",
      "\n",
      " 93%|█████████▎| 186/200 [6:20:08<34:38, 148.46s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   0.9182988746961444\n",
      "\n",
      " 93%|█████████▎| 186/200 [6:20:09<34:38, 148.46s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   {'bagging_frequency': 0.7, 'boosting': 'gbdt', 'colsample_bytree': 1, 'feature_fraction': 0.9, 'learning_rate': 0.1, 'max_depth': 5, 'metric': 'rmse', 'min_child_samples': 40, 'num_leaves': 125, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.3, 'subsample': 0.7}\n",
      "\n",
      " 93%|█████████▎| 186/200 [6:20:09<34:38, 148.46s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      " 94%|█████████▎| 187/200 [6:20:09<25:36, 118.16s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 94%|█████████▎| 187/200 [6:20:09<25:36, 118.16s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   Early stopping, best iteration is:\n",
      "[21]\ttraining's rmse: 0.810107\tvalid_1's rmse: 0.920582\n",
      "\n",
      " 94%|█████████▎| 187/200 [6:20:27<25:36, 118.16s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   0.9205824317064983\n",
      "\n",
      " 94%|█████████▎| 187/200 [6:20:27<25:36, 118.16s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.5, 'feature_fraction': 0.7, 'learning_rate': 0.2, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 20, 'num_leaves': 75, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.2, 'subsample': 0.8}\n",
      "\n",
      " 94%|█████████▎| 187/200 [6:20:27<25:36, 118.16s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      " 94%|█████████▍| 188/200 [6:20:27<17:39, 88.31s/trial, best loss: 0.9026607656143669] \u001b[A\n",
      "\u001b[A                                                                                  Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 94%|█████████▍| 188/200 [6:20:27<17:39, 88.31s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                  [300]\ttraining's rmse: 0.855592\tvalid_1's rmse: 0.92003\n",
      "\n",
      " 94%|█████████▍| 188/200 [6:21:26<17:39, 88.31s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                  [600]\ttraining's rmse: 0.832177\tvalid_1's rmse: 0.917475\n",
      "\n",
      " 94%|█████████▍| 188/200 [6:22:14<17:39, 88.31s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                  [900]\ttraining's rmse: 0.817024\tvalid_1's rmse: 0.916006\n",
      "\n",
      " 94%|█████████▍| 188/200 [6:22:57<17:39, 88.31s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                  Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 0.81369\tvalid_1's rmse: 0.915836\n",
      "\n",
      " 94%|█████████▍| 188/200 [6:23:11<17:39, 88.31s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                  0.9158357190192892\n",
      "\n",
      " 94%|█████████▍| 188/200 [6:23:13<17:39, 88.31s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                  {'bagging_frequency': 0.5, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 1, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 30, 'num_leaves': 25, 'objective': 'regression', 'reg_alpha': 0.2, 'reg_lambda': 0.6, 'subsample': 1}\n",
      "\n",
      " 94%|█████████▍| 188/200 [6:23:13<17:39, 88.31s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      " 94%|█████████▍| 189/200 [6:23:13<20:26, 111.49s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 94%|█████████▍| 189/200 [6:23:13<20:26, 111.49s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   [300]\ttraining's rmse: 0.904604\tvalid_1's rmse: 0.956775\n",
      "\n",
      " 94%|█████████▍| 189/200 [6:23:50<20:26, 111.49s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   [600]\ttraining's rmse: 0.885306\tvalid_1's rmse: 0.944687\n",
      "\n",
      " 94%|█████████▍| 189/200 [6:24:27<20:26, 111.49s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   [900]\ttraining's rmse: 0.87439\tvalid_1's rmse: 0.936851\n",
      "\n",
      " 94%|█████████▍| 189/200 [6:25:04<20:26, 111.49s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 0.871011\tvalid_1's rmse: 0.934337\n",
      "\n",
      " 94%|█████████▍| 189/200 [6:25:16<20:26, 111.49s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   0.9343371078983561\n",
      "\n",
      " 94%|█████████▍| 189/200 [6:25:17<20:26, 111.49s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 3, 'metric': 'rmse', 'min_child_samples': 20, 'num_leaves': 500, 'objective': 'regression', 'reg_alpha': 0.6, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n",
      " 94%|█████████▍| 189/200 [6:25:17<20:26, 111.49s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      " 95%|█████████▌| 190/200 [6:25:17<19:12, 115.27s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 95%|█████████▌| 190/200 [6:25:17<19:12, 115.27s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   [300]\ttraining's rmse: 0.806874\tvalid_1's rmse: 0.93241\n",
      "\n",
      " 95%|█████████▌| 190/200 [6:26:34<19:12, 115.27s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   Early stopping, best iteration is:\n",
      "[326]\ttraining's rmse: 0.802221\tvalid_1's rmse: 0.932137\n",
      "\n",
      " 95%|█████████▌| 190/200 [6:27:02<19:12, 115.27s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   0.9321368261338012\n",
      "\n",
      " 95%|█████████▌| 190/200 [6:27:03<19:12, 115.27s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   {'bagging_frequency': 0.3, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 12, 'metric': 'rmse', 'min_child_samples': 40, 'num_leaves': 100, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n",
      " 95%|█████████▌| 190/200 [6:27:03<19:12, 115.27s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      " 96%|█████████▌| 191/200 [6:27:03<16:53, 112.56s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 96%|█████████▌| 191/200 [6:27:04<16:53, 112.56s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   [300]\ttraining's rmse: 1.05166\tvalid_1's rmse: 1.03732\n",
      "\n",
      " 96%|█████████▌| 191/200 [6:29:08<16:53, 112.56s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   [600]\ttraining's rmse: 0.963655\tvalid_1's rmse: 0.979005\n",
      "\n",
      " 96%|█████████▌| 191/200 [6:31:04<16:53, 112.56s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   [900]\ttraining's rmse: 0.907002\tvalid_1's rmse: 0.945522\n",
      "\n",
      " 96%|█████████▌| 191/200 [6:32:57<16:53, 112.56s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 0.892917\tvalid_1's rmse: 0.938377\n",
      "\n",
      " 96%|█████████▌| 191/200 [6:33:33<16:53, 112.56s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   0.938376617838463\n",
      "\n",
      " 96%|█████████▌| 191/200 [6:33:38<16:53, 112.56s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.9, 'feature_fraction': 0.8, 'learning_rate': 0.001, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 20, 'num_leaves': 250, 'objective': 'regression', 'reg_alpha': 0.3, 'reg_lambda': 0.4, 'subsample': 0.5}\n",
      "\n",
      " 96%|█████████▌| 191/200 [6:33:38<16:53, 112.56s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      " 96%|█████████▌| 192/200 [6:33:38<26:16, 197.11s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 96%|█████████▌| 192/200 [6:33:38<26:16, 197.11s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   [300]\ttraining's rmse: 0.833685\tvalid_1's rmse: 0.919805\n",
      "\n",
      " 96%|█████████▌| 192/200 [6:34:46<26:16, 197.11s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   Early stopping, best iteration is:\n",
      "[350]\ttraining's rmse: 0.826773\tvalid_1's rmse: 0.91846\n",
      "\n",
      " 96%|█████████▌| 192/200 [6:35:14<26:16, 197.11s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   0.918460136329789\n",
      "\n",
      " 96%|█████████▌| 192/200 [6:35:15<26:16, 197.11s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   {'bagging_frequency': 0.4, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 20, 'num_leaves': 50, 'objective': 'regression', 'reg_alpha': 0.4, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n",
      " 96%|█████████▌| 192/200 [6:35:15<26:16, 197.11s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      " 96%|█████████▋| 193/200 [6:35:15<19:30, 167.28s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 96%|█████████▋| 193/200 [6:35:16<19:30, 167.28s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   Early stopping, best iteration is:\n",
      "[75]\ttraining's rmse: 0.805777\tvalid_1's rmse: 0.909058\n",
      "\n",
      " 96%|█████████▋| 193/200 [6:35:59<19:30, 167.28s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   0.9090575526507565\n",
      "\n",
      " 96%|█████████▋| 193/200 [6:35:59<19:30, 167.28s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   {'bagging_frequency': 0.8, 'boosting': 'gbdt', 'colsample_bytree': 0.6, 'feature_fraction': 0.6, 'learning_rate': 0.05, 'max_depth': 8, 'metric': 'rmse', 'min_child_samples': 30, 'num_leaves': 225, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.1, 'subsample': 0.3}\n",
      "\n",
      " 96%|█████████▋| 193/200 [6:35:59<19:30, 167.28s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      " 97%|█████████▋| 194/200 [6:35:59<13:01, 130.24s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 97%|█████████▋| 194/200 [6:36:00<13:01, 130.24s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   [300]\ttraining's rmse: 0.776766\tvalid_1's rmse: 0.904854\n",
      "\n",
      " 97%|█████████▋| 194/200 [6:38:14<13:01, 130.24s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   Early stopping, best iteration is:\n",
      "[233]\ttraining's rmse: 0.794425\tvalid_1's rmse: 0.90324\n",
      "\n",
      " 97%|█████████▋| 194/200 [6:38:27<13:01, 130.24s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   0.9032402359279743\n",
      "\n",
      " 97%|█████████▋| 194/200 [6:38:28<13:01, 130.24s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   {'bagging_frequency': 0.7, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 40, 'num_leaves': 500, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 194/200 [6:38:28<13:01, 130.24s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      " 98%|█████████▊| 195/200 [6:38:28<11:19, 135.90s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 98%|█████████▊| 195/200 [6:38:29<11:19, 135.90s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   Early stopping, best iteration is:\n",
      "[21]\ttraining's rmse: 0.770583\tvalid_1's rmse: 0.933527\n",
      "\n",
      " 98%|█████████▊| 195/200 [6:39:12<11:19, 135.90s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   0.9335269292370982\n",
      "\n",
      " 98%|█████████▊| 195/200 [6:39:12<11:19, 135.90s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   {'bagging_frequency': 0.7, 'boosting': 'gbdt', 'colsample_bytree': 1, 'feature_fraction': 0.9, 'learning_rate': 0.1, 'max_depth': 15, 'metric': 'rmse', 'min_child_samples': 40, 'num_leaves': 500, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.3, 'subsample': 0.7}\n",
      "\n",
      " 98%|█████████▊| 195/200 [6:39:12<11:19, 135.90s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      " 98%|█████████▊| 196/200 [6:39:12<07:12, 108.19s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 98%|█████████▊| 196/200 [6:39:12<07:12, 108.19s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   [300]\ttraining's rmse: 0.775974\tvalid_1's rmse: 0.907799\n",
      "\n",
      " 98%|█████████▊| 196/200 [6:41:09<07:12, 108.19s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   Early stopping, best iteration is:\n",
      "[251]\ttraining's rmse: 0.788642\tvalid_1's rmse: 0.906732\n",
      "\n",
      " 98%|█████████▊| 196/200 [6:41:27<07:12, 108.19s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   0.9067315518744361\n",
      "\n",
      " 98%|█████████▊| 196/200 [6:41:29<07:12, 108.19s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   {'bagging_frequency': 0.7, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.7, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 40, 'num_leaves': 500, 'objective': 'regression', 'reg_alpha': 0.1, 'reg_lambda': 0.6, 'subsample': 0.8}\n",
      "\n",
      " 98%|█████████▊| 196/200 [6:41:29<07:12, 108.19s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      " 98%|█████████▊| 197/200 [6:41:29<05:50, 116.85s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 98%|█████████▊| 197/200 [6:41:29<05:50, 116.85s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   [300]\ttraining's rmse: 0.833444\tvalid_1's rmse: 0.924806\n",
      "\n",
      " 98%|█████████▊| 197/200 [6:42:05<05:50, 116.85s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   Early stopping, best iteration is:\n",
      "[220]\ttraining's rmse: 0.840988\tvalid_1's rmse: 0.923056\n",
      "\n",
      " 98%|█████████▊| 197/200 [6:42:07<05:50, 116.85s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   0.9230555945768091\n",
      "\n",
      " 98%|█████████▊| 197/200 [6:42:07<05:50, 116.85s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   {'bagging_frequency': 0.7, 'boosting': 'gbdt', 'colsample_bytree': 0.5, 'feature_fraction': 0.9, 'learning_rate': 0.2, 'max_depth': 3, 'metric': 'rmse', 'min_child_samples': 40, 'num_leaves': 150, 'objective': 'regression', 'reg_alpha': 0.2, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n",
      " 98%|█████████▊| 197/200 [6:42:07<05:50, 116.85s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      " 99%|█████████▉| 198/200 [6:42:07<03:06, 93.42s/trial, best loss: 0.9026607656143669] \u001b[A\n",
      "\u001b[A                                                                                  Training until validation scores don't improve for 100 rounds\n",
      "\n",
      " 99%|█████████▉| 198/200 [6:42:08<03:06, 93.42s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                  [300]\ttraining's rmse: 0.872429\tvalid_1's rmse: 0.935901\n",
      "\n",
      " 99%|█████████▉| 198/200 [6:43:02<03:06, 93.42s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                  [600]\ttraining's rmse: 0.851235\tvalid_1's rmse: 0.929779\n",
      "\n",
      " 99%|█████████▉| 198/200 [6:43:53<03:06, 93.42s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                  [900]\ttraining's rmse: 0.836495\tvalid_1's rmse: 0.921335\n",
      "\n",
      " 99%|█████████▉| 198/200 [6:44:42<03:06, 93.42s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                  Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 0.833299\tvalid_1's rmse: 0.92035\n",
      "\n",
      " 99%|█████████▉| 198/200 [6:44:58<03:06, 93.42s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                  0.9203501221066012\n",
      "\n",
      " 99%|█████████▉| 198/200 [6:45:00<03:06, 93.42s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                  {'bagging_frequency': 0.7, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 1, 'learning_rate': 0.01, 'max_depth': 5, 'metric': 'rmse', 'min_child_samples': 40, 'num_leaves': 500, 'objective': 'regression', 'reg_alpha': 0.5, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n",
      " 99%|█████████▉| 198/200 [6:45:00<03:06, 93.42s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "100%|█████████▉| 199/200 [6:45:00<01:57, 117.27s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   Training until validation scores don't improve for 100 rounds\n",
      "\n",
      "100%|█████████▉| 199/200 [6:45:01<01:57, 117.27s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   [300]\ttraining's rmse: 0.80792\tvalid_1's rmse: 0.91063\n",
      "\n",
      "100%|█████████▉| 199/200 [6:46:27<01:57, 117.27s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   Early stopping, best iteration is:\n",
      "[302]\ttraining's rmse: 0.807549\tvalid_1's rmse: 0.910554\n",
      "\n",
      "100%|█████████▉| 199/200 [6:46:52<01:57, 117.27s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   0.9105537746127988\n",
      "\n",
      "100%|█████████▉| 199/200 [6:46:53<01:57, 117.27s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "\u001b[A                                                                                   {'bagging_frequency': 0.7, 'boosting': 'gbdt', 'colsample_bytree': 0.7, 'feature_fraction': 0.9, 'learning_rate': 0.01, 'max_depth': 10, 'metric': 'rmse', 'min_child_samples': 40, 'num_leaves': 125, 'objective': 'regression', 'reg_alpha': 0.6, 'reg_lambda': 0.2, 'subsample': 1}\n",
      "\n",
      "100%|█████████▉| 199/200 [6:46:53<01:57, 117.27s/trial, best loss: 0.9026607656143669]\u001b[A\n",
      "100%|██████████| 200/200 [6:46:53<00:00, 122.07s/trial, best loss: 0.9026607656143669]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "# Trail\n",
    "trials = Trials()\n",
    "\n",
    "# Set algoritm parameters\n",
    "algo = partial(tpe.suggest, \n",
    "               n_startup_jobs=-1)\n",
    "\n",
    "# Seting the number of Evals\n",
    "MAX_EVALS= 200\n",
    "\n",
    "# Fit Tree Parzen Estimator\n",
    "best_vals = fmin(fn=evaluate_metric, space=hyper_space, verbose=1,\n",
    "                 algo=algo, max_evals=MAX_EVALS, trials=trials)\n",
    "\n",
    "# Print best parameters\n",
    "best_params = space_eval(hyper_space, best_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 500 rounds\n",
      "[300]\ttraining's rmse: 0.77353\tvalid_1's rmse: 0.903285\n",
      "[600]\ttraining's rmse: 0.739502\tvalid_1's rmse: 0.914145\n",
      "Early stopping, best iteration is:\n",
      "[278]\ttraining's rmse: 0.77851\tvalid_1's rmse: 0.902661\n"
     ]
    }
   ],
   "source": [
    "model_lgb = lightgbm.train(best_params, lgtrain, 1000, \n",
    "                      valid_sets=[lgtrain, lgval], early_stopping_rounds=500, \n",
    "                      verbose_eval=300)\n",
    "\n",
    "lgb_pred = model_lgb.predict(X_test).clip(0, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Booster' object has no attribute 'evals_result_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-467ca8194771>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# retrieve performance metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_lgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevals_result_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'validation_0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rmse'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mx_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Booster' object has no attribute 'evals_result_'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# retrieve performance metrics\n",
    "results = model_lgb.evals_result_\n",
    "epochs = len(results['validation_0']['rmse'])\n",
    "x_axis = range(0, epochs)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_axis, results['validation_0']['rmse'], label='Train')\n",
    "ax.plot(x_axis, results['validation_1']['rmse'], label='Test')\n",
    "ax.legend()\n",
    "plt.ylabel('rmse')\n",
    "plt.title('XGBoost rmse')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "booster must be dict or LGBMModel.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-caca98131784>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlightgbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_lgb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/lightgbm/plotting.py\u001b[0m in \u001b[0;36mplot_metric\u001b[0;34m(booster, metric, dataset_names, ax, xlim, ylim, title, xlabel, ylabel, figsize, dpi, grid)\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0meval_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbooster\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'booster must be dict or LGBMModel.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0mnum_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: booster must be dict or LGBMModel."
     ]
    }
   ],
   "source": [
    "lightgbm.plot_metric(model_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "_uuid": "8ff5a80a22d046c5ca1cb27e938c757b607551d2"
   },
   "outputs": [],
   "source": [
    "Y_pred = model_lgb.predict(X_valid).clip(0, 20)\n",
    "Y_test = model_lgb.predict(X_test).clip(0, 20)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"ID\": test.index, \n",
    "    \"item_cnt_month\": Y_test\n",
    "})\n",
    "submission.to_csv('xgb_submission.csv', index=False)\n",
    "\n",
    "# save predictions for an ensemble\n",
    "pickle.dump(Y_pred, open('xgb_train.pickle', 'wb'))\n",
    "pickle.dump(Y_test, open('xgb_test.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "c8adc7c93323eb77baeceb2e8db17390b5c4deb3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f7704ee0a20>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x1008 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_features(model, (10,14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
